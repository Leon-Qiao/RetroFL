{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712e2fa-92bb-499b-801e-950dfcda6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 13:30:59.015899: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-14 13:30:59.487461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b51fc08-787d-42bb-bf42-65eacb5ce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a8dd19-5960-44cc-90c6-e2c42c86b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=512, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=512, activation=tf.nn.leaky_relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=256, activation=tf.nn.leaky_relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=8)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        output = self.dense5(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f65bda-5f20-4df4-b12e-e0fdf99c5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valiAll(index_epoch):\n",
    "    y_v_p = model(X_v)\n",
    "    va_mse = tf.reduce_mean(tf.square(y_v_p - y_v))\n",
    "    va_rmse = tf.sqrt(va_mse)\n",
    "    va_mae = tf.reduce_mean(tf.abs(y_v_p - y_v))\n",
    "    va_r2 = 1 - tf.reduce_sum(tf.square(y_v_p - y_v)) / tf.reduce_sum(tf.square(y_v - tf.reduce_mean(y_v)))\n",
    "    print(\"mse:{} rmse:{} mae:{} r2:{}\".format(va_mse, va_rmse, va_mae, va_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b543212c-3492-45eb-ad62-77930b2d4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"Test.csv\", encoding='utf-8').sample(frac=1).reset_index(drop=True)\n",
    "X_v = test_dataset.loc[:,'freq':'L4'].to_numpy(dtype = np.float32)\n",
    "y_v = test_dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eef8846-dcb9-4aca-bdb0-cfddcec2ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 13:31:01.057172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9604 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2023-09-14 13:31:01.057694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9621 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "dataset = pd.read_csv('./26Train.csv', encoding='utf-8').sample(frac=1).reset_index(drop=True)\n",
    "X = dataset.loc[:,'freq':'L4'].to_numpy(dtype = np.float32)\n",
    "y = dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=X.shape[0])\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624eb49b-e8c8-4d45-bc8b-b020e9398908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 13:31:01.769868: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55614834c680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-14 13:31:01.769890: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-14 13:31:01.769895: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-14 13:31:01.773453: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-14 13:31:01.884464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-14 13:31:01.991846: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f8a2fe4ce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f8a2fe4ce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch:0\n",
      "train mse:0.10196889191865921 rmse:0.31932568550109863 mae:0.25864964723587036 r2:0.15557944774627686\n",
      "mse:0.10659817606210709 rmse:0.3264937400817871 mae:0.2658608853816986 r2:0.11759287118911743\n",
      "epoch:1\n",
      "train mse:0.08677519857883453 rmse:0.2945762872695923 mae:0.23791292309761047 r2:0.27549540996551514\n",
      "mse:0.08649691939353943 rmse:0.29410359263420105 mae:0.23689745366573334 r2:0.2839886546134949\n",
      "epoch:2\n",
      "train mse:0.07779916375875473 rmse:0.27892500162124634 mae:0.22172410786151886 r2:0.34799057245254517\n",
      "mse:0.08065716922283173 rmse:0.28400206565856934 mae:0.22924017906188965 r2:0.3323294520378113\n",
      "epoch:3\n",
      "train mse:0.07245587557554245 rmse:0.26917627453804016 mae:0.21755973994731903 r2:0.40046072006225586\n",
      "mse:0.0795021802186966 rmse:0.2819612920284271 mae:0.2271060347557068 r2:0.3418903350830078\n",
      "epoch:4\n",
      "train mse:0.06953577697277069 rmse:0.2636963725090027 mae:0.21423955261707306 r2:0.4296085834503174\n",
      "mse:0.0727391242980957 rmse:0.26970192790031433 mae:0.21646088361740112 r2:0.397874116897583\n",
      "epoch:5\n",
      "train mse:0.0699048638343811 rmse:0.26439526677131653 mae:0.21176429092884064 r2:0.4178459048271179\n",
      "mse:0.07448820024728775 rmse:0.2729252576828003 mae:0.21790947020053864 r2:0.3833954334259033\n",
      "epoch:6\n",
      "train mse:0.06716993451118469 rmse:0.25917163491249084 mae:0.20193016529083252 r2:0.4455910921096802\n",
      "mse:0.06979217380285263 rmse:0.26418209075927734 mae:0.21118341386318207 r2:0.4222686290740967\n",
      "epoch:7\n",
      "train mse:0.06502367556095123 rmse:0.2549974024295807 mae:0.20387540757656097 r2:0.4633992314338684\n",
      "mse:0.07228364050388336 rmse:0.2688561677932739 mae:0.21379216015338898 r2:0.4016445279121399\n",
      "epoch:8\n",
      "train mse:0.056721318513154984 rmse:0.23816236853599548 mae:0.18910983204841614 r2:0.5326765775680542\n",
      "mse:0.06585932523012161 rmse:0.25663071870803833 mae:0.2060498297214508 r2:0.4548242688179016\n",
      "epoch:9\n",
      "train mse:0.05981328338384628 rmse:0.24456754326820374 mae:0.19535952806472778 r2:0.5067842602729797\n",
      "mse:0.06844014674425125 rmse:0.26161065697669983 mae:0.2082049697637558 r2:0.433460533618927\n",
      "epoch:10\n",
      "train mse:0.05617062747478485 rmse:0.23700343072414398 mae:0.1855189949274063 r2:0.5372182130813599\n",
      "mse:0.06294543296098709 rmse:0.25088927149772644 mae:0.19943682849407196 r2:0.47894513607025146\n",
      "epoch:11\n",
      "train mse:0.04691426828503609 rmse:0.21659702062606812 mae:0.17138710618019104 r2:0.6110092401504517\n",
      "mse:0.06367035955190659 rmse:0.25232985615730286 mae:0.2009076178073883 r2:0.47294431924819946\n",
      "epoch:12\n",
      "train mse:0.046163421124219894 rmse:0.21485674381256104 mae:0.17033380270004272 r2:0.6181299686431885\n",
      "mse:0.058421239256858826 rmse:0.24170485138893127 mae:0.19239680469036102 r2:0.5163959264755249\n",
      "epoch:13\n",
      "train mse:0.04472208023071289 rmse:0.21147595345973969 mae:0.164952352643013 r2:0.6253715753555298\n",
      "mse:0.05776461958885193 rmse:0.24034270644187927 mae:0.19153404235839844 r2:0.5218312740325928\n",
      "epoch:14\n",
      "train mse:0.040954411029815674 rmse:0.2023719698190689 mae:0.15795733034610748 r2:0.6620557308197021\n",
      "mse:0.06391172856092453 rmse:0.2528076767921448 mae:0.19965706765651703 r2:0.4709462523460388\n",
      "epoch:15\n",
      "train mse:0.04138128086924553 rmse:0.2034238874912262 mae:0.1613839566707611 r2:0.6566835641860962\n",
      "mse:0.055368244647979736 rmse:0.2353045791387558 mae:0.18568888306617737 r2:0.541668176651001\n",
      "epoch:16\n",
      "train mse:0.03859519585967064 rmse:0.19645659625530243 mae:0.15622711181640625 r2:0.6812364459037781\n",
      "mse:0.05678636208176613 rmse:0.23829889297485352 mae:0.1871567666530609 r2:0.5299291610717773\n",
      "epoch:17\n",
      "train mse:0.033252399414777756 rmse:0.18235240876674652 mae:0.1422853171825409 r2:0.726036787033081\n",
      "mse:0.060060612857341766 rmse:0.24507266283035278 mae:0.19335095584392548 r2:0.5028253197669983\n",
      "epoch:18\n",
      "train mse:0.032829537987709045 rmse:0.18118922412395477 mae:0.13992798328399658 r2:0.7285407781600952\n",
      "mse:0.05824899673461914 rmse:0.2413482815027237 mae:0.18882536888122559 r2:0.5178216695785522\n",
      "epoch:19\n",
      "train mse:0.031323473900556564 rmse:0.17698438465595245 mae:0.1394946426153183 r2:0.7408274412155151\n",
      "mse:0.0658964216709137 rmse:0.2567029893398285 mae:0.19937503337860107 r2:0.4545171856880188\n",
      "epoch:20\n",
      "train mse:0.031715717166662216 rmse:0.17808906733989716 mae:0.13592015206813812 r2:0.733184814453125\n",
      "mse:0.057401299476623535 rmse:0.23958568274974823 mae:0.1865641474723816 r2:0.5248388051986694\n",
      "epoch:21\n",
      "train mse:0.021874915808439255 rmse:0.147901713848114 mae:0.11331474035978317 r2:0.8193358182907104\n",
      "mse:0.06108023226261139 rmse:0.24714414775371552 mae:0.19050472974777222 r2:0.4943850040435791\n",
      "epoch:22\n",
      "train mse:0.026774117723107338 rmse:0.1636279821395874 mae:0.12671087682247162 r2:0.7759830951690674\n",
      "mse:0.060162775218486786 rmse:0.24528101086616516 mae:0.18952646851539612 r2:0.501979649066925\n",
      "epoch:23\n",
      "train mse:0.024339595809578896 rmse:0.15601152181625366 mae:0.12000306695699692 r2:0.7942250370979309\n",
      "mse:0.06055217236280441 rmse:0.24607351422309875 mae:0.1892847716808319 r2:0.4987562894821167\n",
      "epoch:24\n",
      "train mse:0.02154001221060753 rmse:0.14676515758037567 mae:0.11492754518985748 r2:0.8179962038993835\n",
      "mse:0.06138068810105324 rmse:0.24775126576423645 mae:0.19121213257312775 r2:0.4918978810310364\n",
      "epoch:25\n",
      "train mse:0.025257805362343788 rmse:0.1589270383119583 mae:0.12122544646263123 r2:0.7902386784553528\n",
      "mse:0.05909627303481102 rmse:0.2430972456932068 mae:0.18647198379039764 r2:0.510807991027832\n",
      "epoch:26\n",
      "train mse:0.02451327070593834 rmse:0.15656714141368866 mae:0.1229657307267189 r2:0.7964962720870972\n",
      "mse:0.0674891322851181 rmse:0.2597866952419281 mae:0.19989372789859772 r2:0.44133293628692627\n",
      "epoch:27\n",
      "train mse:0.023005714640021324 rmse:0.15167634189128876 mae:0.11643411219120026 r2:0.8094704151153564\n",
      "mse:0.058578431606292725 rmse:0.2420298159122467 mae:0.1863991916179657 r2:0.5150946378707886\n",
      "epoch:28\n",
      "train mse:0.024747014045715332 rmse:0.1573118418455124 mae:0.11857355386018753 r2:0.7922877073287964\n",
      "mse:0.06528989970684052 rmse:0.2555188834667206 mae:0.19453707337379456 r2:0.45953792333602905\n",
      "epoch:29\n",
      "train mse:0.03249732777476311 rmse:0.18027015030384064 mae:0.13350915908813477 r2:0.7282061576843262\n",
      "mse:0.05711257457733154 rmse:0.23898237943649292 mae:0.18273136019706726 r2:0.527228832244873\n",
      "epoch:30\n",
      "train mse:0.022009996697306633 rmse:0.14835765957832336 mae:0.11541476845741272 r2:0.8168388605117798\n",
      "mse:0.06124001741409302 rmse:0.24746720492839813 mae:0.18850621581077576 r2:0.4930623769760132\n",
      "epoch:31\n",
      "train mse:0.02918705716729164 rmse:0.17084220051765442 mae:0.12935912609100342 r2:0.7565409541130066\n",
      "mse:0.061798080801963806 rmse:0.24859219789505005 mae:0.19184838235378265 r2:0.4884427785873413\n",
      "epoch:32\n",
      "train mse:0.025562461465597153 rmse:0.15988264977931976 mae:0.12295814603567123 r2:0.7889986038208008\n",
      "mse:0.07315348088741302 rmse:0.2704690098762512 mae:0.20486634969711304 r2:0.3944441080093384\n",
      "epoch:33\n",
      "train mse:0.022481542080640793 rmse:0.1499384641647339 mae:0.11528487503528595 r2:0.81374192237854\n",
      "mse:0.07298880815505981 rmse:0.2701644003391266 mae:0.20614713430404663 r2:0.3958072066307068\n",
      "epoch:34\n",
      "train mse:0.02002039924263954 rmse:0.14149345457553864 mae:0.11125560849905014 r2:0.8349906206130981\n",
      "mse:0.057975415140390396 rmse:0.24078084528446198 mae:0.18291829526424408 r2:0.5200863480567932\n",
      "epoch:35\n",
      "train mse:0.020814277231693268 rmse:0.14427153766155243 mae:0.1088520810008049 r2:0.8271350264549255\n",
      "mse:0.05556558817625046 rmse:0.23572354018688202 mae:0.17874816060066223 r2:0.5400345921516418\n",
      "epoch:36\n",
      "train mse:0.017733663320541382 rmse:0.1331678032875061 mae:0.10286810249090195 r2:0.8532623052597046\n",
      "mse:0.0608476847410202 rmse:0.24667322635650635 mae:0.18684260547161102 r2:0.4963100552558899\n",
      "epoch:37\n",
      "train mse:0.017576245591044426 rmse:0.13257543742656708 mae:0.1031980961561203 r2:0.8551201820373535\n",
      "mse:0.05594862252473831 rmse:0.23653461039066315 mae:0.1788550615310669 r2:0.5368639230728149\n",
      "epoch:38\n",
      "train mse:0.017253825441002846 rmse:0.13135382533073425 mae:0.09747564047574997 r2:0.8566098213195801\n",
      "mse:0.06301333755254745 rmse:0.2510245740413666 mae:0.1893940418958664 r2:0.47838300466537476\n",
      "epoch:39\n",
      "train mse:0.01673859730362892 rmse:0.12937772274017334 mae:0.10150724649429321 r2:0.8613184690475464\n",
      "mse:0.06540680676698685 rmse:0.25574755668640137 mae:0.19369764626026154 r2:0.4585701823234558\n",
      "epoch:40\n",
      "train mse:0.02121644653379917 rmse:0.1456586718559265 mae:0.10639134049415588 r2:0.8235189318656921\n",
      "mse:0.06843483448028564 rmse:0.261600524187088 mae:0.19821184873580933 r2:0.43350452184677124\n",
      "epoch:41\n",
      "train mse:0.017299821600317955 rmse:0.1315287947654724 mae:0.0991058498620987 r2:0.857344388961792\n",
      "mse:0.0617263987660408 rmse:0.248447984457016 mae:0.18672429025173187 r2:0.4890361428260803\n",
      "epoch:42\n",
      "train mse:0.014973467215895653 rmse:0.12236612290143967 mae:0.0951806902885437 r2:0.8763676881790161\n",
      "mse:0.06850723922252655 rmse:0.2617388665676117 mae:0.19688200950622559 r2:0.4329051375389099\n",
      "epoch:43\n",
      "train mse:0.021666869521141052 rmse:0.1471966952085495 mae:0.11146803945302963 r2:0.8200433850288391\n",
      "mse:0.06934637576341629 rmse:0.26333698630332947 mae:0.19850227236747742 r2:0.42595887184143066\n",
      "epoch:44\n",
      "train mse:0.016547301784157753 rmse:0.12863631546497345 mae:0.09443660080432892 r2:0.8628190755844116\n",
      "mse:0.06149519979953766 rmse:0.24798226356506348 mae:0.18459098041057587 r2:0.49094998836517334\n",
      "epoch:45\n",
      "train mse:0.018427439033985138 rmse:0.13574770092964172 mae:0.10290783643722534 r2:0.8473526239395142\n",
      "mse:0.056907206773757935 rmse:0.23855231702327728 mae:0.17847026884555817 r2:0.5289288759231567\n",
      "epoch:46\n",
      "train mse:0.016731174662709236 rmse:0.12934903800487518 mae:0.10187255591154099 r2:0.861364483833313\n",
      "mse:0.06799490004777908 rmse:0.26075831055641174 mae:0.195032998919487 r2:0.43714624643325806\n",
      "epoch:47\n",
      "train mse:0.014292855747044086 rmse:0.11955273151397705 mae:0.09197288751602173 r2:0.8810598850250244\n",
      "mse:0.05517382174730301 rmse:0.23489108681678772 mae:0.17501530051231384 r2:0.5432776212692261\n",
      "epoch:48\n",
      "train mse:0.013114760629832745 rmse:0.11451969295740128 mae:0.08901198953390121 r2:0.890561580657959\n",
      "mse:0.061674777418375015 rmse:0.2483440637588501 mae:0.1860060691833496 r2:0.4894634485244751\n",
      "epoch:49\n",
      "train mse:0.01858922466635704 rmse:0.13634230196475983 mae:0.10231509804725647 r2:0.8447151780128479\n",
      "mse:0.06060975417494774 rmse:0.24619048833847046 mae:0.18374770879745483 r2:0.4982795715332031\n",
      "epoch:50\n",
      "train mse:0.015339060686528683 rmse:0.12385096400976181 mae:0.09324391931295395 r2:0.8725326061248779\n",
      "mse:0.062030624598264694 rmse:0.24905948340892792 mae:0.18638354539871216 r2:0.4865177869796753\n",
      "epoch:51\n",
      "train mse:0.017431585118174553 rmse:0.132028728723526 mae:0.1020314022898674 r2:0.8520110845565796\n",
      "mse:0.061552997678518295 rmse:0.24809876084327698 mae:0.18574228882789612 r2:0.4904715418815613\n",
      "epoch:52\n",
      "train mse:0.017135992646217346 rmse:0.13090451061725616 mae:0.10183579474687576 r2:0.8578988909721375\n",
      "mse:0.06300769746303558 rmse:0.2510133385658264 mae:0.187509685754776 r2:0.47842973470687866\n",
      "epoch:53\n",
      "train mse:0.019962742924690247 rmse:0.14128956198692322 mae:0.10759516805410385 r2:0.8345668911933899\n",
      "mse:0.05588765814900398 rmse:0.23640570044517517 mae:0.17514269053936005 r2:0.5373685359954834\n",
      "epoch:54\n",
      "train mse:0.012812362983822823 rmse:0.11319170892238617 mae:0.08740400522947311 r2:0.8937826156616211\n",
      "mse:0.06548064202070236 rmse:0.2558918595314026 mae:0.1893799751996994 r2:0.4579589366912842\n",
      "epoch:55\n",
      "train mse:0.016214337199926376 rmse:0.12733551859855652 mae:0.09717302024364471 r2:0.8660528063774109\n",
      "mse:0.057833895087242126 rmse:0.24048678576946259 mae:0.17924368381500244 r2:0.5212578177452087\n",
      "epoch:56\n",
      "train mse:0.016266033053398132 rmse:0.12753835320472717 mae:0.09699706733226776 r2:0.8616043329238892\n",
      "mse:0.061633314937353134 rmse:0.24826057255268097 mae:0.18378403782844543 r2:0.4898067116737366\n",
      "epoch:57\n",
      "train mse:0.016895124688744545 rmse:0.12998124957084656 mae:0.10138905793428421 r2:0.8608744144439697\n",
      "mse:0.061244819313287735 rmse:0.24747690558433533 mae:0.18378005921840668 r2:0.493022620677948\n",
      "epoch:58\n",
      "train mse:0.014729837886989117 rmse:0.12136654555797577 mae:0.09043008089065552 r2:0.877377986907959\n",
      "mse:0.0620560422539711 rmse:0.24911050498485565 mae:0.18487416207790375 r2:0.48630738258361816\n",
      "epoch:59\n",
      "train mse:0.012966849841177464 rmse:0.11387207359075546 mae:0.08813595026731491 r2:0.892859160900116\n",
      "mse:0.05775688961148262 rmse:0.2403266280889511 mae:0.17812617123126984 r2:0.5218952894210815\n",
      "epoch:60\n",
      "train mse:0.014567066915333271 rmse:0.120694100856781 mae:0.09154408425092697 r2:0.8799704909324646\n",
      "mse:0.06480655819177628 rmse:0.2545713186264038 mae:0.18809300661087036 r2:0.4635389447212219\n",
      "epoch:61\n",
      "train mse:0.01487686112523079 rmse:0.12197073549032211 mae:0.08931394666433334 r2:0.8750640749931335\n",
      "mse:0.06474471092224121 rmse:0.2544498145580292 mae:0.18773050606250763 r2:0.46405094861984253\n",
      "epoch:62\n",
      "train mse:0.01181189063936472 rmse:0.10868252068758011 mae:0.08324874937534332 r2:0.9029297828674316\n",
      "mse:0.06535442918539047 rmse:0.25564512610435486 mae:0.18823997676372528 r2:0.459003746509552\n",
      "epoch:63\n",
      "train mse:0.012808894738554955 rmse:0.1131763905286789 mae:0.08738812059164047 r2:0.8943186402320862\n",
      "mse:0.06270624697208405 rmse:0.25041213631629944 mae:0.18551363050937653 r2:0.4809250831604004\n",
      "epoch:64\n",
      "train mse:0.013880057260394096 rmse:0.11781365424394608 mae:0.0920439139008522 r2:0.8827518224716187\n",
      "mse:0.06210508942604065 rmse:0.24920892715454102 mae:0.18468067049980164 r2:0.485901415348053\n",
      "epoch:65\n",
      "train mse:0.01021350547671318 rmse:0.10106188803911209 mae:0.0755448266863823 r2:0.9160090684890747\n",
      "mse:0.06263960152864456 rmse:0.250279039144516 mae:0.18452921509742737 r2:0.4814767837524414\n",
      "epoch:66\n",
      "train mse:0.011399537324905396 rmse:0.10676861554384232 mae:0.08445567637681961 r2:0.9050323367118835\n",
      "mse:0.06601477414369583 rmse:0.25693339109420776 mae:0.18943853676319122 r2:0.4535374641418457\n",
      "epoch:67\n",
      "train mse:0.012564128264784813 rmse:0.11208982020616531 mae:0.08689738065004349 r2:0.8948960900306702\n",
      "mse:0.067970871925354 rmse:0.26071223616600037 mae:0.19236771762371063 r2:0.43734514713287354\n",
      "epoch:68\n",
      "train mse:0.009742127731442451 rmse:0.09870221465826035 mae:0.07784751802682877 r2:0.9190896153450012\n",
      "mse:0.06178572028875351 rmse:0.24856732785701752 mae:0.18280625343322754 r2:0.48854511976242065\n",
      "epoch:69\n",
      "train mse:0.011226256377995014 rmse:0.10595402866601944 mae:0.07899576425552368 r2:0.9074164628982544\n",
      "mse:0.06524046510457993 rmse:0.2554221451282501 mae:0.18831467628479004 r2:0.4599471092224121\n",
      "epoch:70\n",
      "train mse:0.009929714724421501 rmse:0.09964795410633087 mae:0.07729534059762955 r2:0.9172186255455017\n",
      "mse:0.059369344264268875 rmse:0.24365824460983276 mae:0.1785483956336975 r2:0.5085475444793701\n",
      "epoch:71\n",
      "train mse:0.013444162905216217 rmse:0.11594896763563156 mae:0.08762315660715103 r2:0.8886986970901489\n",
      "mse:0.0609917938709259 rmse:0.2469651699066162 mae:0.18281087279319763 r2:0.4951171278953552\n",
      "epoch:72\n",
      "train mse:0.0106364656239748 rmse:0.10313323885202408 mae:0.07879564166069031 r2:0.9116630554199219\n",
      "mse:0.05955200269818306 rmse:0.24403278529644012 mae:0.17900939285755157 r2:0.5070355534553528\n",
      "epoch:73\n",
      "train mse:0.008869816549122334 rmse:0.09417970478534698 mae:0.07144767791032791 r2:0.9256910681724548\n",
      "mse:0.06994009017944336 rmse:0.264461874961853 mae:0.19305264949798584 r2:0.42104417085647583\n",
      "epoch:74\n",
      "train mse:0.01071037258952856 rmse:0.1034909337759018 mae:0.0759478434920311 r2:0.9111202955245972\n",
      "mse:0.06815829873085022 rmse:0.26107144355773926 mae:0.19197027385234833 r2:0.4357936382293701\n",
      "epoch:75\n",
      "train mse:0.010189197957515717 rmse:0.10094155371189117 mae:0.07733200490474701 r2:0.91522216796875\n",
      "mse:0.07597842067480087 rmse:0.2756418287754059 mae:0.20468541979789734 r2:0.3710595965385437\n",
      "epoch:76\n",
      "train mse:0.010183443315327168 rmse:0.10091304779052734 mae:0.07484336942434311 r2:0.9156000018119812\n",
      "mse:0.06304696202278137 rmse:0.25109153985977173 mae:0.18356703221797943 r2:0.4781046509742737\n",
      "epoch:77\n",
      "train mse:0.009495368227362633 rmse:0.09744417667388916 mae:0.07733755558729172 r2:0.9205315113067627\n",
      "mse:0.0590122751891613 rmse:0.24292442202568054 mae:0.17838598787784576 r2:0.5115033388137817\n",
      "epoch:78\n",
      "train mse:0.011804014444351196 rmse:0.10864628106355667 mae:0.08231440931558609 r2:0.901522159576416\n",
      "mse:0.06517015397548676 rmse:0.25528445839881897 mae:0.18724223971366882 r2:0.4605291485786438\n",
      "epoch:79\n",
      "train mse:0.008203576318919659 rmse:0.09057359397411346 mae:0.07139681279659271 r2:0.9322651624679565\n",
      "mse:0.06634868681430817 rmse:0.2575823962688446 mae:0.1892767995595932 r2:0.4507734179496765\n",
      "epoch:80\n",
      "train mse:0.01419541984796524 rmse:0.11914452910423279 mae:0.09048908203840256 r2:0.8818341493606567\n",
      "mse:0.0716983750462532 rmse:0.26776552200317383 mae:0.19663505256175995 r2:0.4064892530441284\n",
      "epoch:81\n",
      "train mse:0.011508462950587273 rmse:0.10727750509977341 mae:0.08296147733926773 r2:0.9039364457130432\n",
      "mse:0.06381849199533463 rmse:0.25262320041656494 mae:0.18505343794822693 r2:0.4717180132865906\n",
      "epoch:82\n",
      "train mse:0.010758878663182259 rmse:0.10372501611709595 mae:0.07973857969045639 r2:0.9102740287780762\n",
      "mse:0.06187652423977852 rmse:0.24874992668628693 mae:0.1818648874759674 r2:0.4877933859825134\n",
      "epoch:83\n",
      "train mse:0.008017969317734241 rmse:0.08954311162233353 mae:0.06958670914173126 r2:0.931706428527832\n",
      "mse:0.07298987358808517 rmse:0.2701663672924042 mae:0.1980573982000351 r2:0.3957984447479248\n",
      "epoch:84\n",
      "train mse:0.008304915390908718 rmse:0.0911313071846962 mae:0.06954483687877655 r2:0.931763231754303\n",
      "mse:0.06204851716756821 rmse:0.2490953952074051 mae:0.1806868612766266 r2:0.48636966943740845\n",
      "epoch:85\n",
      "train mse:0.007778961211442947 rmse:0.08819841593503952 mae:0.06702455878257751 r2:0.9358118772506714\n",
      "mse:0.07236048579216003 rmse:0.26899904012680054 mae:0.19652140140533447 r2:0.4010084271430969\n",
      "epoch:86\n",
      "train mse:0.009618471376597881 rmse:0.098073810338974 mae:0.07455502450466156 r2:0.9190070629119873\n",
      "mse:0.07520803064107895 rmse:0.2742408215999603 mae:0.20097751915454865 r2:0.3774368166923523\n",
      "epoch:87\n",
      "train mse:0.009168123826384544 rmse:0.09575031697750092 mae:0.07285378873348236 r2:0.923835813999176\n",
      "mse:0.06265516579151154 rmse:0.25031012296676636 mae:0.182268887758255 r2:0.481347918510437\n",
      "epoch:88\n",
      "train mse:0.008622675202786922 rmse:0.09285835921764374 mae:0.07378827035427094 r2:0.9287173748016357\n",
      "mse:0.06405828148126602 rmse:0.25309738516807556 mae:0.18415415287017822 r2:0.4697331190109253\n",
      "epoch:89\n",
      "train mse:0.010349063202738762 rmse:0.1017303466796875 mae:0.07605673372745514 r2:0.913680374622345\n",
      "mse:0.06666430830955505 rmse:0.25819432735443115 mae:0.18819575011730194 r2:0.44816070795059204\n",
      "epoch:90\n",
      "train mse:0.010496538132429123 rmse:0.10245261341333389 mae:0.07658897340297699 r2:0.9124349355697632\n",
      "mse:0.06630702316761017 rmse:0.2575015127658844 mae:0.1871216595172882 r2:0.4511182904243469\n",
      "epoch:91\n",
      "train mse:0.010097882710397243 rmse:0.10048822313547134 mae:0.07257585972547531 r2:0.9150249361991882\n",
      "mse:0.06625652313232422 rmse:0.2574034333229065 mae:0.18653102219104767 r2:0.45153629779815674\n",
      "epoch:92\n",
      "train mse:0.008998396806418896 rmse:0.09485988318920135 mae:0.072027787566185 r2:0.9253156185150146\n",
      "mse:0.06434370577335358 rmse:0.2536606192588806 mae:0.1843339502811432 r2:0.4673703908920288\n",
      "epoch:93\n",
      "train mse:0.009051604196429253 rmse:0.09513991326093674 mae:0.07415930181741714 r2:0.9237948656082153\n",
      "mse:0.06769327074289322 rmse:0.2601793110370636 mae:0.18902458250522614 r2:0.43964308500289917\n",
      "epoch:94\n",
      "train mse:0.014870886690914631 rmse:0.12194624543190002 mae:0.09142178297042847 r2:0.8765197992324829\n",
      "mse:0.06341477483510971 rmse:0.25182288885116577 mae:0.18399730324745178 r2:0.47505998611450195\n",
      "epoch:95\n",
      "train mse:0.00941210612654686 rmse:0.09701600670814514 mae:0.0728258416056633 r2:0.9212422966957092\n",
      "mse:0.07414202392101288 rmse:0.27229031920433044 mae:0.19914551079273224 r2:0.38626110553741455\n",
      "epoch:96\n",
      "train mse:0.008095778524875641 rmse:0.08997654169797897 mae:0.06819477677345276 r2:0.933100700378418\n",
      "mse:0.06252327561378479 rmse:0.2500465512275696 mae:0.18103109300136566 r2:0.48243969678878784\n",
      "epoch:97\n",
      "train mse:0.006825780496001244 rmse:0.08261828124523163 mae:0.06313490867614746 r2:0.943161129951477\n",
      "mse:0.06294665485620499 rmse:0.25089171528816223 mae:0.18164785206317902 r2:0.47893500328063965\n",
      "epoch:98\n",
      "train mse:0.009611709974706173 rmse:0.09803932905197144 mae:0.07551883161067963 r2:0.9205815196037292\n",
      "mse:0.06513190269470215 rmse:0.2552095353603363 mae:0.1846187263727188 r2:0.4608457684516907\n",
      "epoch:99\n",
      "train mse:0.00670486968010664 rmse:0.0818832665681839 mae:0.06254447996616364 r2:0.9446756839752197\n",
      "mse:0.06074133887887001 rmse:0.24645757675170898 mae:0.1781911849975586 r2:0.49719035625457764\n",
      "epoch:100\n",
      "train mse:0.008730483241379261 rmse:0.09343705326318741 mae:0.07095999270677567 r2:0.9280486702919006\n",
      "mse:0.06709279865026474 rmse:0.2590227723121643 mae:0.18767033517360687 r2:0.4446136951446533\n",
      "epoch:101\n",
      "train mse:0.007500889245420694 rmse:0.0866076722741127 mae:0.06729315221309662 r2:0.9368119835853577\n",
      "mse:0.07429001480340958 rmse:0.2725619375705719 mae:0.19983482360839844 r2:0.38503605127334595\n",
      "epoch:102\n",
      "train mse:0.008003074675798416 rmse:0.08945990353822708 mae:0.0693422257900238 r2:0.9338169693946838\n",
      "mse:0.06409310549497604 rmse:0.25316616892814636 mae:0.18210214376449585 r2:0.46944481134414673\n",
      "epoch:103\n",
      "train mse:0.00753356097266078 rmse:0.08679609000682831 mae:0.06617194414138794 r2:0.9373430013656616\n",
      "mse:0.0708293467760086 rmse:0.26613783836364746 mae:0.19317223131656647 r2:0.4136829972267151\n",
      "epoch:104\n",
      "train mse:0.008926337584853172 rmse:0.09447929263114929 mae:0.07171247899532318 r2:0.9264189004898071\n",
      "mse:0.06267664581537247 rmse:0.25035303831100464 mae:0.18073947727680206 r2:0.48117005825042725\n",
      "epoch:105\n",
      "train mse:0.009484549053013325 rmse:0.09738864749670029 mae:0.07602597028017044 r2:0.9214504957199097\n",
      "mse:0.06815549731254578 rmse:0.2610660791397095 mae:0.1898728907108307 r2:0.43581682443618774\n",
      "epoch:106\n",
      "train mse:0.006837646011263132 rmse:0.08269006013870239 mae:0.06457877159118652 r2:0.9432827830314636\n",
      "mse:0.07021921873092651 rmse:0.26498907804489136 mae:0.1922324150800705 r2:0.4187335968017578\n",
      "epoch:107\n",
      "train mse:0.008065683767199516 rmse:0.08980914950370789 mae:0.06866011768579483 r2:0.9329978823661804\n",
      "mse:0.06347748637199402 rmse:0.2519473731517792 mae:0.1817556619644165 r2:0.4745408892631531\n",
      "epoch:108\n",
      "train mse:0.0065453932620584965 rmse:0.08090360462665558 mae:0.062137849628925323 r2:0.9455732703208923\n",
      "mse:0.0764278769493103 rmse:0.27645590901374817 mae:0.2023811787366867 r2:0.3673390746116638\n",
      "epoch:109\n",
      "train mse:0.007134849671274424 rmse:0.08446803689002991 mae:0.0649687722325325 r2:0.9403881430625916\n",
      "mse:0.06999042630195618 rmse:0.26455703377723694 mae:0.19155143201351166 r2:0.42062753438949585\n",
      "epoch:110\n",
      "train mse:0.007268244866281748 rmse:0.0852539986371994 mae:0.06606099009513855 r2:0.9398497939109802\n",
      "mse:0.06985855102539062 rmse:0.2643076777458191 mae:0.1919938027858734 r2:0.42171913385391235\n",
      "epoch:111\n",
      "train mse:0.008623912930488586 rmse:0.09286502748727798 mae:0.07257171720266342 r2:0.9271619915962219\n",
      "mse:0.06856167316436768 rmse:0.26184284687042236 mae:0.1895817667245865 r2:0.43245458602905273\n",
      "epoch:112\n",
      "train mse:0.006912850309163332 rmse:0.08314354717731476 mae:0.06455882638692856 r2:0.9425798058509827\n",
      "mse:0.06786079704761505 rmse:0.26050105690956116 mae:0.18777316808700562 r2:0.43825632333755493\n",
      "epoch:113\n",
      "train mse:0.010865265503525734 rmse:0.10423658043146133 mae:0.07613112032413483 r2:0.9098795056343079\n",
      "mse:0.06838051974773407 rmse:0.2614966928958893 mae:0.18882709741592407 r2:0.433954119682312\n",
      "epoch:114\n",
      "train mse:0.005928240716457367 rmse:0.07699506729841232 mae:0.05901772156357765 r2:0.9506678581237793\n",
      "mse:0.06363720446825027 rmse:0.252264142036438 mae:0.18111369013786316 r2:0.4732186794281006\n",
      "epoch:115\n",
      "train mse:0.007405584678053856 rmse:0.0860557034611702 mae:0.06710560619831085 r2:0.9386489987373352\n",
      "mse:0.07097441703081131 rmse:0.266410231590271 mae:0.19405485689640045 r2:0.4124821424484253\n",
      "epoch:116\n",
      "train mse:0.007152870297431946 rmse:0.0845746397972107 mae:0.06465471535921097 r2:0.9400205612182617\n",
      "mse:0.06455074995756149 rmse:0.25406837463378906 mae:0.18282677233219147 r2:0.4656565189361572\n",
      "epoch:117\n",
      "train mse:0.006840853486210108 rmse:0.08270944654941559 mae:0.0638352483510971 r2:0.943606972694397\n",
      "mse:0.06712165474891663 rmse:0.2590784728527069 mae:0.18687841296195984 r2:0.44437485933303833\n",
      "epoch:118\n",
      "train mse:0.007529115304350853 rmse:0.08677047491073608 mae:0.06640590727329254 r2:0.9380179047584534\n",
      "mse:0.062229085713624954 rmse:0.24945758283138275 mae:0.17920079827308655 r2:0.484874963760376\n",
      "epoch:119\n",
      "train mse:0.008160899393260479 rmse:0.09033769369125366 mae:0.06543823331594467 r2:0.9318705201148987\n",
      "mse:0.07147646695375443 rmse:0.26735082268714905 mae:0.19441759586334229 r2:0.4083262085914612\n",
      "epoch:120\n",
      "train mse:0.006494049448519945 rmse:0.08058566600084305 mae:0.059756942093372345 r2:0.945915162563324\n",
      "mse:0.06690021604299545 rmse:0.2586507499217987 mae:0.18614178895950317 r2:0.4462079405784607\n",
      "epoch:121\n",
      "train mse:0.006611301563680172 rmse:0.08130990713834763 mae:0.06272119283676147 r2:0.9453600645065308\n",
      "mse:0.06912276893854141 rmse:0.2629120945930481 mae:0.1904345601797104 r2:0.4278098940849304\n",
      "epoch:122\n",
      "train mse:0.006444930098950863 rmse:0.08028031885623932 mae:0.06173822283744812 r2:0.9466798901557922\n",
      "mse:0.06858652830123901 rmse:0.2618902921676636 mae:0.18881814181804657 r2:0.4322488307952881\n",
      "epoch:123\n",
      "train mse:0.0071049160324037075 rmse:0.08429066091775894 mae:0.06432835012674332 r2:0.94100421667099\n",
      "mse:0.07145510613918304 rmse:0.26731085777282715 mae:0.19463051855564117 r2:0.40850305557250977\n",
      "epoch:124\n",
      "train mse:0.006972549017518759 rmse:0.08350178599357605 mae:0.0620991550385952 r2:0.9417281746864319\n",
      "mse:0.07008013129234314 rmse:0.26472651958465576 mae:0.19134601950645447 r2:0.41988492012023926\n",
      "epoch:125\n",
      "train mse:0.006016084924340248 rmse:0.07756342738866806 mae:0.0603211373090744 r2:0.9502285122871399\n",
      "mse:0.0674184262752533 rmse:0.2596505880355835 mae:0.18673884868621826 r2:0.44191819429397583\n",
      "epoch:126\n",
      "train mse:0.0064348336309194565 rmse:0.08021741360425949 mae:0.0613841712474823 r2:0.9467184543609619\n",
      "mse:0.07167258113622665 rmse:0.2677173614501953 mae:0.1934944987297058 r2:0.40670281648635864\n",
      "epoch:127\n",
      "train mse:0.0070593287236988544 rmse:0.08401980996131897 mae:0.06481916457414627 r2:0.9412765502929688\n",
      "mse:0.06580643355846405 rmse:0.2565276324748993 mae:0.18476366996765137 r2:0.45526206493377686\n",
      "epoch:128\n",
      "train mse:0.006974866613745689 rmse:0.08351566642522812 mae:0.06215066462755203 r2:0.9421446323394775\n",
      "mse:0.06465689092874527 rmse:0.25427716970443726 mae:0.1828458458185196 r2:0.4647778868675232\n",
      "epoch:129\n",
      "train mse:0.006943374872207642 rmse:0.08332691341638565 mae:0.06349547207355499 r2:0.9423894882202148\n",
      "mse:0.07709988206624985 rmse:0.27766865491867065 mae:0.20304836332798004 r2:0.36177629232406616\n",
      "epoch:130\n",
      "train mse:0.006232422776520252 rmse:0.07894569635391235 mae:0.06187403202056885 r2:0.9476718306541443\n",
      "mse:0.06391873955726624 rmse:0.25282156467437744 mae:0.18130311369895935 r2:0.4708881974220276\n",
      "epoch:131\n",
      "train mse:0.006593390367925167 rmse:0.08119969069957733 mae:0.06104239821434021 r2:0.9451013803482056\n",
      "mse:0.06284362822771072 rmse:0.25068631768226624 mae:0.18016476929187775 r2:0.4797878861427307\n",
      "epoch:132\n",
      "train mse:0.007649843115359545 rmse:0.08746337890625 mae:0.0667450800538063 r2:0.9357301592826843\n",
      "mse:0.06786196678876877 rmse:0.26050329208374023 mae:0.18733935058116913 r2:0.4382466673851013\n",
      "epoch:133\n",
      "train mse:0.00568667147308588 rmse:0.07541002333164215 mae:0.058083903044462204 r2:0.9523347020149231\n",
      "mse:0.07189899682998657 rmse:0.26813986897468567 mae:0.19280575215816498 r2:0.4048285484313965\n",
      "epoch:134\n",
      "train mse:0.007068488746881485 rmse:0.08407430350780487 mae:0.06228376552462578 r2:0.940679132938385\n",
      "mse:0.07266984134912491 rmse:0.269573450088501 mae:0.19369031488895416 r2:0.3984476327896118\n",
      "epoch:135\n",
      "train mse:0.00854470580816269 rmse:0.09243757277727127 mae:0.0712646022439003 r2:0.9292813539505005\n",
      "mse:0.061450522392988205 rmse:0.24789215624332428 mae:0.1785542517900467 r2:0.49131983518600464\n",
      "epoch:136\n",
      "train mse:0.005938059650361538 rmse:0.077058807015419 mae:0.059517938643693924 r2:0.9510720372200012\n",
      "mse:0.06556929647922516 rmse:0.2560650110244751 mae:0.1839800328016281 r2:0.45722508430480957\n",
      "epoch:137\n",
      "train mse:0.008132217451930046 rmse:0.09017881006002426 mae:0.06842318922281265 r2:0.9327260255813599\n",
      "mse:0.06557569652795792 rmse:0.25607749819755554 mae:0.18436327576637268 r2:0.45717209577560425\n",
      "epoch:138\n",
      "train mse:0.005166721995919943 rmse:0.07187991589307785 mae:0.05650469288229942 r2:0.9574523568153381\n",
      "mse:0.0640513151884079 rmse:0.25308361649513245 mae:0.18110807240009308 r2:0.4697907567024231\n",
      "epoch:139\n",
      "train mse:0.006216814741492271 rmse:0.07884678244590759 mae:0.06051996722817421 r2:0.947603166103363\n",
      "mse:0.06521719694137573 rmse:0.25537657737731934 mae:0.18317165970802307 r2:0.46013975143432617\n",
      "epoch:140\n",
      "train mse:0.005914745386689901 rmse:0.07690738141536713 mae:0.05954931303858757 r2:0.9502535462379456\n",
      "mse:0.06999491900205612 rmse:0.26456552743911743 mae:0.18983247876167297 r2:0.42059028148651123\n",
      "epoch:141\n",
      "train mse:0.005554478615522385 rmse:0.07452837377786636 mae:0.058905333280563354 r2:0.9538088440895081\n",
      "mse:0.06420907378196716 rmse:0.25339508056640625 mae:0.18186548352241516 r2:0.46848487854003906\n",
      "epoch:142\n",
      "train mse:0.005051143001765013 rmse:0.07107139378786087 mae:0.054720599204301834 r2:0.9583934545516968\n",
      "mse:0.06623832136392593 rmse:0.2573680579662323 mae:0.1848682165145874 r2:0.4516869783401489\n",
      "epoch:143\n",
      "train mse:0.0059465267695486546 rmse:0.07711372524499893 mae:0.059482160955667496 r2:0.9509720206260681\n",
      "mse:0.07245422154664993 rmse:0.26917320489883423 mae:0.1952330470085144 r2:0.4002324938774109\n",
      "epoch:144\n",
      "train mse:0.005031947046518326 rmse:0.07093621790409088 mae:0.05407946929335594 r2:0.9579637050628662\n",
      "mse:0.06603675335645676 rmse:0.2569761574268341 mae:0.18470722436904907 r2:0.45335549116134644\n",
      "epoch:145\n",
      "train mse:0.006869795732200146 rmse:0.08288422971963882 mae:0.06265794485807419 r2:0.9431614279747009\n",
      "mse:0.06502611935138702 rmse:0.2550022006034851 mae:0.1834951490163803 r2:0.46172142028808594\n",
      "epoch:146\n",
      "train mse:0.006929602473974228 rmse:0.08324423432350159 mae:0.06127740815281868 r2:0.9420946836471558\n",
      "mse:0.07053116708993912 rmse:0.2655770480632782 mae:0.19063341617584229 r2:0.4161512851715088\n",
      "epoch:147\n",
      "train mse:0.006359358783811331 rmse:0.0797455832362175 mae:0.05908467248082161 r2:0.9470469951629639\n",
      "mse:0.06712526828050613 rmse:0.25908544659614563 mae:0.18532641232013702 r2:0.4443449378013611\n",
      "epoch:148\n",
      "train mse:0.006338495761156082 rmse:0.07961466908454895 mae:0.06144825741648674 r2:0.9478558301925659\n",
      "mse:0.06215069070458412 rmse:0.24930040538311005 mae:0.17958657443523407 r2:0.48552393913269043\n",
      "epoch:149\n",
      "train mse:0.00534006068482995 rmse:0.07307571917772293 mae:0.05702207610011101 r2:0.955606997013092\n",
      "mse:0.06730542331933975 rmse:0.2594328820705414 mae:0.18570537865161896 r2:0.4428536891937256\n"
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    for X, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            tr_mse = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        tr_rmse = tf.sqrt(tr_mse)\n",
    "        tr_mae = tf.reduce_mean(tf.abs(y_pred - y))\n",
    "        tr_r2 = 1 - tf.reduce_sum(tf.square(y_pred - y)) / tf.reduce_sum(tf.square(y - tf.reduce_mean(y)))\n",
    "        grads = tape.gradient(tr_mse, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    print(\"epoch:{}\".format(i))\n",
    "    print(\"train mse:{} rmse:{} mae:{} r2:{}\".format(tr_mse, tr_rmse, tr_mae, tr_r2))\n",
    "    valiAll(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65290d0d-0a5b-4b36-847f-4faac8fb96d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecac09-38d3-4cd1-af0e-47314b7f11bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
