{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712e2fa-92bb-499b-801e-950dfcda6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 02:25:25.319508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-14 02:25:25.815024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b51fc08-787d-42bb-bf42-65eacb5ce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a8dd19-5960-44cc-90c6-e2c42c86b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=8)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        output = self.dense5(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f65bda-5f20-4df4-b12e-e0fdf99c5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valiAll(index_epoch):\n",
    "    y_v_p = model(X_v)\n",
    "    va_mse = tf.reduce_mean(tf.square(y_v_p - y_v))\n",
    "    va_rmse = tf.sqrt(va_mse)\n",
    "    va_mae = tf.reduce_mean(tf.abs(y_v_p - y_v))\n",
    "    va_r2 = 1 - tf.reduce_sum(tf.square(y_v_p - y_v)) / tf.reduce_sum(tf.square(y_v - tf.reduce_mean(y_v)))\n",
    "    print(\"mse:{} rmse:{} mae:{} r2:{}\".format(va_mse, va_rmse, va_mae, va_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b543212c-3492-45eb-ad62-77930b2d4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"testset5.csv\", encoding='utf-8').sample(frac=1).reset_index(drop=True)\n",
    "X_v = test_dataset.loc[:,'freq':'L2'].to_numpy(dtype = np.float32)\n",
    "y_v = test_dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eef8846-dcb9-4aca-bdb0-cfddcec2ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 02:25:29.270239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9604 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2023-09-14 02:25:29.270758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9621 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./trainset95.csv', encoding='utf-8').sample(frac=1).reset_index(drop=True)\n",
    "dataset = dataset[dataset['freq'] == 1.0]\n",
    "X = dataset.loc[:,'freq':'L2'].to_numpy(dtype = np.float32)\n",
    "y = dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=23000)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1327167a-9b84-45dd-a8c1-edf35b22e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624eb49b-e8c8-4d45-bc8b-b020e9398908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 02:25:30.183563: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d11e15caa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-14 02:25:30.183584: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-14 02:25:30.183588: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-14 02:25:30.186625: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-14 02:25:30.288421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-14 02:25:30.392237: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f038a49d280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f038a49d280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch:0\n",
      "train mse:0.033133216202259064 rmse:0.18202531337738037 mae:0.13518191874027252 r2:0.7290502190589905\n",
      "mse:0.03210780397057533 rmse:0.17918650805950165 mae:0.1325996220111847 r2:0.7370811104774475\n",
      "epoch:1\n",
      "train mse:0.024867530912160873 rmse:0.15769442915916443 mae:0.11552955955266953 r2:0.7963812351226807\n",
      "mse:0.02182110957801342 rmse:0.14771969616413116 mae:0.10443442314863205 r2:0.8213150501251221\n",
      "epoch:2\n",
      "train mse:0.016756433993577957 rmse:0.12944664061069489 mae:0.09099897742271423 r2:0.8629574775695801\n",
      "mse:0.016346093267202377 rmse:0.12785184383392334 mae:0.09104352444410324 r2:0.8661478757858276\n",
      "epoch:3\n",
      "train mse:0.013312586583197117 rmse:0.1153801828622818 mae:0.08235827833414078 r2:0.8910212516784668\n",
      "mse:0.012853110209107399 rmse:0.11337155848741531 mae:0.07841794937849045 r2:0.8947506546974182\n",
      "epoch:4\n",
      "train mse:0.011255982331931591 rmse:0.10609421133995056 mae:0.0750293880701065 r2:0.9079592823982239\n",
      "mse:0.01098242774605751 rmse:0.10479708015918732 mae:0.07336802035570145 r2:0.9100689888000488\n",
      "epoch:5\n",
      "train mse:0.010890565812587738 rmse:0.10435787588357925 mae:0.07317160069942474 r2:0.9107640981674194\n",
      "mse:0.011126312427222729 rmse:0.1054813340306282 mae:0.07437437027692795 r2:0.9088907241821289\n",
      "epoch:6\n",
      "train mse:0.008722689934074879 rmse:0.09339534491300583 mae:0.06640774011611938 r2:0.9285775423049927\n",
      "mse:0.008857409469783306 rmse:0.09411381185054779 mae:0.0642770528793335 r2:0.9274699687957764\n",
      "epoch:7\n",
      "train mse:0.007096378598362207 rmse:0.08424000442028046 mae:0.057892799377441406 r2:0.9418415427207947\n",
      "mse:0.008195649832487106 rmse:0.09052982926368713 mae:0.06181994080543518 r2:0.9328888654708862\n",
      "epoch:8\n",
      "train mse:0.007506169844418764 rmse:0.08663815259933472 mae:0.05980489030480385 r2:0.9386731386184692\n",
      "mse:0.011042819358408451 rmse:0.10508482158184052 mae:0.0744980201125145 r2:0.9095744490623474\n",
      "epoch:9\n",
      "train mse:0.006671777926385403 rmse:0.08168095350265503 mae:0.05781177058815956 r2:0.9453705549240112\n",
      "mse:0.010224716737866402 rmse:0.101117342710495 mae:0.07201801240444183 r2:0.9162735939025879\n",
      "epoch:10\n",
      "train mse:0.009320213459432125 rmse:0.09654124826192856 mae:0.06989745795726776 r2:0.9237303733825684\n",
      "mse:0.008783279918134212 rmse:0.09371915459632874 mae:0.0643940418958664 r2:0.928076982498169\n",
      "epoch:11\n",
      "train mse:0.007323058787733316 rmse:0.08557487279176712 mae:0.059307895600795746 r2:0.9401223659515381\n",
      "mse:0.006610291078686714 rmse:0.08130369335412979 mae:0.054902248084545135 r2:0.9458707571029663\n",
      "epoch:12\n",
      "train mse:0.006507631856948137 rmse:0.08066989481449127 mae:0.05493972823023796 r2:0.9465480446815491\n",
      "mse:0.007244185544550419 rmse:0.08511278033256531 mae:0.0586472824215889 r2:0.9406800270080566\n",
      "epoch:13\n",
      "train mse:0.006270702928304672 rmse:0.079187773168087 mae:0.05356442555785179 r2:0.9484685063362122\n",
      "mse:0.005991041194647551 rmse:0.07740181684494019 mae:0.05294395238161087 r2:0.9509415626525879\n",
      "epoch:14\n",
      "train mse:0.0058856201358139515 rmse:0.0767177939414978 mae:0.05354498699307442 r2:0.9517908692359924\n",
      "mse:0.005934701766818762 rmse:0.077037014067173 mae:0.053001806139945984 r2:0.9514029026031494\n",
      "epoch:15\n",
      "train mse:0.006106749176979065 rmse:0.07814569026231766 mae:0.05275283753871918 r2:0.9498733878135681\n",
      "mse:0.007247353903949261 rmse:0.08513139188289642 mae:0.05913084000349045 r2:0.9406540989875793\n",
      "epoch:16\n",
      "train mse:0.006399268750101328 rmse:0.07999543100595474 mae:0.0558306910097599 r2:0.9476290941238403\n",
      "mse:0.006234263069927692 rmse:0.07895734906196594 mae:0.05499346926808357 r2:0.948949933052063\n",
      "epoch:17\n",
      "train mse:0.0051293703727424145 rmse:0.07161962240934372 mae:0.04753575474023819 r2:0.9580287933349609\n",
      "mse:0.006083791609853506 rmse:0.07799866050481796 mae:0.05352305993437767 r2:0.9501820802688599\n",
      "epoch:18\n",
      "train mse:0.004787868820130825 rmse:0.06919442862272263 mae:0.04557441920042038 r2:0.960720419883728\n",
      "mse:0.004874683916568756 rmse:0.06981893628835678 mae:0.0465412512421608 r2:0.9600830078125\n",
      "epoch:19\n",
      "train mse:0.005714836996048689 rmse:0.07559654116630554 mae:0.05256830155849457 r2:0.953174352645874\n",
      "mse:0.0049077956937253475 rmse:0.07005565613508224 mae:0.04768642410635948 r2:0.9598118662834167\n",
      "epoch:20\n",
      "train mse:0.004805012606084347 rmse:0.06931819766759872 mae:0.04723184183239937 r2:0.9606146812438965\n",
      "mse:0.005222943611443043 rmse:0.07226993888616562 mae:0.05033000186085701 r2:0.9572312235832214\n",
      "epoch:21\n",
      "train mse:0.005131899379193783 rmse:0.07163727283477783 mae:0.048787690699100494 r2:0.9579780101776123\n",
      "mse:0.006593348924070597 rmse:0.08119943737983704 mae:0.057599253952503204 r2:0.9460095167160034\n",
      "epoch:22\n",
      "train mse:0.004082862287759781 rmse:0.0638972818851471 mae:0.04423703998327255 r2:0.9665409326553345\n",
      "mse:0.0040191542357206345 rmse:0.06339680403470993 mae:0.042902931571006775 r2:0.9670886397361755\n",
      "epoch:23\n",
      "train mse:0.004862899426370859 rmse:0.06973449140787125 mae:0.047839295119047165 r2:0.9601197242736816\n",
      "mse:0.004021988250315189 rmse:0.0634191483259201 mae:0.04301995784044266 r2:0.9670654535293579\n",
      "epoch:24\n",
      "train mse:0.003905209247022867 rmse:0.06249167397618294 mae:0.04152596741914749 r2:0.9680221080780029\n",
      "mse:0.0044627063907682896 rmse:0.06680349260568619 mae:0.04544460400938988 r2:0.9634565711021423\n",
      "epoch:25\n",
      "train mse:0.0034435398411005735 rmse:0.058681681752204895 mae:0.0394635871052742 r2:0.9718359708786011\n",
      "mse:0.0033204902429133654 rmse:0.05762369558215141 mae:0.03875467926263809 r2:0.9728097319602966\n",
      "epoch:26\n",
      "train mse:0.004043768160045147 rmse:0.0635906234383583 mae:0.04392968490719795 r2:0.9668734073638916\n",
      "mse:0.006107693072408438 rmse:0.07815173268318176 mae:0.05320019647479057 r2:0.9499863386154175\n",
      "epoch:27\n",
      "train mse:0.003021739423274994 rmse:0.05497035011649132 mae:0.03628547862172127 r2:0.9752938151359558\n",
      "mse:0.0030410299077630043 rmse:0.05514553189277649 mae:0.036964431405067444 r2:0.9750981330871582\n",
      "epoch:28\n",
      "train mse:0.004140525124967098 rmse:0.06434690952301025 mae:0.04425780102610588 r2:0.9660988450050354\n",
      "mse:0.0036133904941380024 rmse:0.060111481696367264 mae:0.041691854596138 r2:0.9704113006591797\n",
      "epoch:29\n",
      "train mse:0.003451532917097211 rmse:0.05874974653124809 mae:0.040004827082157135 r2:0.9717358946800232\n",
      "mse:0.0035168251488357782 rmse:0.05930282548069954 mae:0.0393591970205307 r2:0.97120201587677\n",
      "epoch:30\n",
      "train mse:0.0036806545685976744 rmse:0.060668397694826126 mae:0.04126105457544327 r2:0.9698532819747925\n",
      "mse:0.003917229827493429 rmse:0.06258777529001236 mae:0.043462686240673065 r2:0.9679232835769653\n",
      "epoch:31\n",
      "train mse:0.002573027042672038 rmse:0.05072501301765442 mae:0.03622492030262947 r2:0.9789950251579285\n",
      "mse:0.002812491962686181 rmse:0.05303293094038963 mae:0.03489801287651062 r2:0.9769695401191711\n",
      "epoch:32\n",
      "train mse:0.003375402884557843 rmse:0.058098215609788895 mae:0.03887369856238365 r2:0.9723252058029175\n",
      "mse:0.003653507912531495 rmse:0.06044425442814827 mae:0.04204750061035156 r2:0.9700827598571777\n",
      "epoch:33\n",
      "train mse:0.00273548299446702 rmse:0.05230184644460678 mae:0.034657612442970276 r2:0.9775896072387695\n",
      "mse:0.0029598670080304146 rmse:0.05440466105937958 mae:0.03670179843902588 r2:0.9757627248764038\n",
      "epoch:34\n",
      "train mse:0.0031191371381282806 rmse:0.055849235504865646 mae:0.03681157901883125 r2:0.9744027853012085\n",
      "mse:0.002744639292359352 rmse:0.05238930508494377 mae:0.035030242055654526 r2:0.9775251746177673\n",
      "epoch:35\n",
      "train mse:0.0026304624043405056 rmse:0.05128803476691246 mae:0.03531228005886078 r2:0.9784650206565857\n",
      "mse:0.0027463461738079786 rmse:0.05240559205412865 mae:0.03478918969631195 r2:0.9775111675262451\n",
      "epoch:36\n",
      "train mse:0.00262587727047503 rmse:0.051243312656879425 mae:0.034566447138786316 r2:0.9784891605377197\n",
      "mse:0.0025951277930289507 rmse:0.05094239488244057 mae:0.03377915918827057 r2:0.9787494540214539\n",
      "epoch:37\n",
      "train mse:0.0027016785461455584 rmse:0.051977671682834625 mae:0.03560127317905426 r2:0.9778612852096558\n",
      "mse:0.0035160654224455357 rmse:0.059296417981386185 mae:0.04046235978603363 r2:0.9712082147598267\n",
      "epoch:38\n",
      "train mse:0.0028880916070193052 rmse:0.0537409670650959 mae:0.03650355339050293 r2:0.9764196872711182\n",
      "mse:0.003332708263769746 rmse:0.05772961303591728 mae:0.039091773331165314 r2:0.9727096557617188\n",
      "epoch:39\n",
      "train mse:0.002824821276590228 rmse:0.05314904823899269 mae:0.034298915416002274 r2:0.976893424987793\n",
      "mse:0.0033014779910445213 rmse:0.05745849013328552 mae:0.038078855723142624 r2:0.97296541929245\n",
      "epoch:40\n",
      "train mse:0.002982386155053973 rmse:0.05461122840642929 mae:0.03842735290527344 r2:0.9755986928939819\n",
      "mse:0.002864434150978923 rmse:0.053520407527685165 mae:0.03525080531835556 r2:0.976544201374054\n",
      "epoch:41\n",
      "train mse:0.003013362642377615 rmse:0.0548941045999527 mae:0.0365288071334362 r2:0.9753199815750122\n",
      "mse:0.0030880447011440992 rmse:0.05557017773389816 mae:0.03882197290658951 r2:0.974713146686554\n",
      "epoch:42\n",
      "train mse:0.0030689819250255823 rmse:0.05539839342236519 mae:0.035578928887844086 r2:0.9748034477233887\n",
      "mse:0.0030722871888428926 rmse:0.055428218096494675 mae:0.03737926110625267 r2:0.9748421907424927\n",
      "epoch:43\n",
      "train mse:0.002330504124984145 rmse:0.04827529564499855 mae:0.03253345564007759 r2:0.9809527397155762\n",
      "mse:0.0027265073731541634 rmse:0.05221596732735634 mae:0.034848425537347794 r2:0.9776736497879028\n",
      "epoch:44\n",
      "train mse:0.0023459268268197775 rmse:0.04843476787209511 mae:0.03293894603848457 r2:0.9807749390602112\n",
      "mse:0.0023607825860381126 rmse:0.04858788475394249 mae:0.03232986852526665 r2:0.9806684255599976\n",
      "epoch:45\n",
      "train mse:0.0034814903046935797 rmse:0.059004154056310654 mae:0.04097240790724754 r2:0.9714828729629517\n",
      "mse:0.0039932928048074245 rmse:0.06319250166416168 mae:0.041831523180007935 r2:0.9673004150390625\n",
      "epoch:46\n",
      "train mse:0.0035949035082012415 rmse:0.059957511723041534 mae:0.04179748147726059 r2:0.9705770611763\n",
      "mse:0.0030469130724668503 rmse:0.055198848247528076 mae:0.037930410355329514 r2:0.9750499725341797\n",
      "epoch:47\n",
      "train mse:0.0036630695685744286 rmse:0.060523297637701035 mae:0.04124969244003296 r2:0.9700263142585754\n",
      "mse:0.002728824969381094 rmse:0.05223815515637398 mae:0.03553522750735283 r2:0.9776546359062195\n",
      "epoch:48\n",
      "train mse:0.003162937005981803 rmse:0.05623999610543251 mae:0.03654574602842331 r2:0.9741020202636719\n",
      "mse:0.0026393828447908163 rmse:0.051374923437833786 mae:0.03397727757692337 r2:0.9783870577812195\n",
      "epoch:49\n",
      "train mse:0.0031361011788249016 rmse:0.056000903248786926 mae:0.038341544568538666 r2:0.9742749333381653\n",
      "mse:0.0037652626633644104 rmse:0.061361733824014664 mae:0.04200547933578491 r2:0.9691676497459412\n",
      "epoch:50\n",
      "train mse:0.0020123273134231567 rmse:0.04485897347331047 mae:0.03068455494940281 r2:0.9835296273231506\n",
      "mse:0.0024889048654586077 rmse:0.04988892376422882 mae:0.03388737142086029 r2:0.9796192646026611\n",
      "epoch:51\n",
      "train mse:0.002263493835926056 rmse:0.047576189041137695 mae:0.03165026754140854 r2:0.9814861416816711\n",
      "mse:0.0022315022069960833 rmse:0.047238778322935104 mae:0.031514402478933334 r2:0.9817270636558533\n",
      "epoch:52\n",
      "train mse:0.002484336029738188 rmse:0.049843113869428635 mae:0.034133195877075195 r2:0.9796751141548157\n",
      "mse:0.0027541446033865213 rmse:0.05247994512319565 mae:0.03535006567835808 r2:0.9774473309516907\n",
      "epoch:53\n",
      "train mse:0.0025251973420381546 rmse:0.05025134235620499 mae:0.033886395394802094 r2:0.9793660044670105\n",
      "mse:0.002562658628448844 rmse:0.05062270909547806 mae:0.03451358526945114 r2:0.9790153503417969\n",
      "epoch:54\n",
      "train mse:0.0023064089473336935 rmse:0.048025086522102356 mae:0.03325062617659569 r2:0.9811186790466309\n",
      "mse:0.0024613584391772747 rmse:0.049612078815698624 mae:0.03365679830312729 r2:0.979844868183136\n",
      "epoch:55\n",
      "train mse:0.002101447433233261 rmse:0.04584154859185219 mae:0.030999651178717613 r2:0.9828019738197327\n",
      "mse:0.0023208092898130417 rmse:0.04817477613687515 mae:0.03194707632064819 r2:0.980995774269104\n",
      "epoch:56\n",
      "train mse:0.0021941799204796553 rmse:0.046842075884342194 mae:0.03176792711019516 r2:0.9820541143417358\n",
      "mse:0.0025037815794348717 rmse:0.05003780126571655 mae:0.033485401421785355 r2:0.9794974327087402\n",
      "epoch:57\n",
      "train mse:0.0025357394479215145 rmse:0.050356123596429825 mae:0.03416512534022331 r2:0.979256808757782\n",
      "mse:0.0025169651489704847 rmse:0.050169363617897034 mae:0.034275345504283905 r2:0.979389488697052\n",
      "epoch:58\n",
      "train mse:0.002311017131432891 rmse:0.04807303845882416 mae:0.033257368952035904 r2:0.9810854196548462\n",
      "mse:0.0024228766560554504 rmse:0.04922272637486458 mae:0.03314670920372009 r2:0.9801599383354187\n",
      "epoch:59\n",
      "train mse:0.0022814858239144087 rmse:0.04776490107178688 mae:0.033401232212781906 r2:0.9812822937965393\n",
      "mse:0.003207871923223138 rmse:0.05663807690143585 mae:0.038826510310173035 r2:0.9737319350242615\n",
      "epoch:60\n",
      "train mse:0.002082104329019785 rmse:0.04563008248806 mae:0.031083816662430763 r2:0.98297119140625\n",
      "mse:0.0024508268106728792 rmse:0.04950582608580589 mae:0.03278503194451332 r2:0.979931116104126\n",
      "epoch:61\n",
      "train mse:0.0029356456361711025 rmse:0.054181598126888275 mae:0.03739656135439873 r2:0.9759598970413208\n",
      "mse:0.0023631292860955 rmse:0.04861202836036682 mae:0.03287392854690552 r2:0.9806492328643799\n",
      "epoch:62\n",
      "train mse:0.0023119808174669743 rmse:0.04808306321501732 mae:0.03236740827560425 r2:0.9810907244682312\n",
      "mse:0.0028891738038510084 rmse:0.053751036524772644 mae:0.03671198710799217 r2:0.9763416051864624\n",
      "epoch:63\n",
      "train mse:0.0031790502835065126 rmse:0.05638306587934494 mae:0.03789626806974411 r2:0.9739949107170105\n",
      "mse:0.0025076118763536215 rmse:0.05007605999708176 mae:0.03396438807249069 r2:0.9794660806655884\n",
      "epoch:64\n",
      "train mse:0.0017961625708267093 rmse:0.042381156235933304 mae:0.02844538912177086 r2:0.9852988719940186\n",
      "mse:0.0018812604248523712 rmse:0.04337349906563759 mae:0.02822735533118248 r2:0.9845950603485107\n",
      "epoch:65\n",
      "train mse:0.002385548083111644 rmse:0.04884207248687744 mae:0.03252733498811722 r2:0.9805037975311279\n",
      "mse:0.004123346414417028 rmse:0.06421329081058502 mae:0.045748524367809296 r2:0.9662354588508606\n",
      "epoch:66\n",
      "train mse:0.003251686692237854 rmse:0.057023562490940094 mae:0.03941844776272774 r2:0.9733226895332336\n",
      "mse:0.003091640304774046 rmse:0.05560252070426941 mae:0.03716594725847244 r2:0.9746837019920349\n",
      "epoch:67\n",
      "train mse:0.0028885062783956528 rmse:0.05374482646584511 mae:0.035897765308618546 r2:0.9763423800468445\n",
      "mse:0.0024407075252383947 rmse:0.04940351843833923 mae:0.034055519849061966 r2:0.9800139665603638\n",
      "epoch:68\n",
      "train mse:0.0019247354939579964 rmse:0.04387180507183075 mae:0.029793651774525642 r2:0.9842605590820312\n",
      "mse:0.0018429755000397563 rmse:0.042929891496896744 mae:0.028769610449671745 r2:0.9849085807800293\n",
      "epoch:69\n",
      "train mse:0.002141623990610242 rmse:0.04627768322825432 mae:0.030779117718338966 r2:0.9824786186218262\n",
      "mse:0.0018919621361419559 rmse:0.043496690690517426 mae:0.028653299435973167 r2:0.9845074415206909\n",
      "epoch:70\n",
      "train mse:0.0018754748161882162 rmse:0.043306753039360046 mae:0.029886776581406593 r2:0.984664797782898\n",
      "mse:0.002074321499094367 rmse:0.04554472118616104 mae:0.030431952327489853 r2:0.9830141663551331\n",
      "epoch:71\n",
      "train mse:0.0022503123618662357 rmse:0.047437459230422974 mae:0.03270230442285538 r2:0.9815940260887146\n",
      "mse:0.0022190117742866278 rmse:0.047106388956308365 mae:0.03202103078365326 r2:0.9818293452262878\n",
      "epoch:72\n",
      "train mse:0.0024193101562559605 rmse:0.04918648302555084 mae:0.033684637397527695 r2:0.9801886677742004\n",
      "mse:0.0025905475486069918 rmse:0.050897423177957535 mae:0.03395574167370796 r2:0.9787869453430176\n",
      "epoch:73\n",
      "train mse:0.0023864845279604197 rmse:0.048851657658815384 mae:0.03362146019935608 r2:0.9803528189659119\n",
      "mse:0.002546629635617137 rmse:0.050464142113924026 mae:0.03411027789115906 r2:0.9791465997695923\n",
      "epoch:74\n",
      "train mse:0.0017959069227799773 rmse:0.04237814247608185 mae:0.028545696288347244 r2:0.9852807521820068\n",
      "mse:0.0019308587070554495 rmse:0.04394153505563736 mae:0.029253724962472916 r2:0.9841889142990112\n",
      "epoch:75\n",
      "train mse:0.0019526721443980932 rmse:0.044189050793647766 mae:0.0292215533554554 r2:0.9840199947357178\n",
      "mse:0.0020738118328154087 rmse:0.04553912580013275 mae:0.030929898843169212 r2:0.9830183386802673\n",
      "epoch:76\n",
      "train mse:0.0025510054547339678 rmse:0.050507478415966034 mae:0.03128930181264877 r2:0.9790827631950378\n",
      "mse:0.002176523907110095 rmse:0.046653229743242264 mae:0.03189288079738617 r2:0.9821772575378418\n",
      "epoch:77\n",
      "train mse:0.001930679427459836 rmse:0.0439394973218441 mae:0.029173709452152252 r2:0.9841840863227844\n",
      "mse:0.0019943758379667997 rmse:0.04465843364596367 mae:0.029932528734207153 r2:0.9836688041687012\n",
      "epoch:78\n",
      "train mse:0.0023047274444252253 rmse:0.048007577657699585 mae:0.03344140201807022 r2:0.9811416864395142\n",
      "mse:0.0027034920640289783 rmse:0.051995113492012024 mae:0.03412037715315819 r2:0.9778621196746826\n",
      "epoch:79\n",
      "train mse:0.0019383615581318736 rmse:0.044026825577020645 mae:0.03077382594347 r2:0.9841567873954773\n",
      "mse:0.0020476493518799543 rmse:0.04525095969438553 mae:0.030119001865386963 r2:0.9832325577735901\n",
      "epoch:80\n",
      "train mse:0.002224708441644907 rmse:0.047166816890239716 mae:0.03203459829092026 r2:0.981779158115387\n",
      "mse:0.0025018309243023396 rmse:0.0500183068215847 mae:0.03379829600453377 r2:0.97951340675354\n",
      "epoch:81\n",
      "train mse:0.0017185058677569032 rmse:0.04145486652851105 mae:0.027255099266767502 r2:0.9859291315078735\n",
      "mse:0.0016142376698553562 rmse:0.04017757624387741 mae:0.02622918412089348 r2:0.9867815971374512\n",
      "epoch:82\n",
      "train mse:0.0028348639607429504 rmse:0.05324343964457512 mae:0.037105266004800797 r2:0.9767762422561646\n",
      "mse:0.0031759142875671387 rmse:0.056355249136686325 mae:0.03904630243778229 r2:0.9739935994148254\n",
      "epoch:83\n",
      "train mse:0.002501095412299037 rmse:0.05001095309853554 mae:0.033334411680698395 r2:0.9794726371765137\n",
      "mse:0.002340140286833048 rmse:0.04837499558925629 mae:0.03315288573503494 r2:0.9808374643325806\n",
      "epoch:84\n",
      "train mse:0.0017887118738144636 rmse:0.04229316487908363 mae:0.028853852301836014 r2:0.9853556156158447\n",
      "mse:0.0020114006474614143 rmse:0.044848643243312836 mae:0.03033342957496643 r2:0.9835293889045715\n",
      "epoch:85\n",
      "train mse:0.002599206520244479 rmse:0.050982411950826645 mae:0.034580159932374954 r2:0.9787564277648926\n",
      "mse:0.005093573592603207 rmse:0.07136927545070648 mae:0.047976285219192505 r2:0.9582906365394592\n",
      "epoch:86\n",
      "train mse:0.0024073291569948196 rmse:0.04906454309821129 mae:0.03229435533285141 r2:0.9802797436714172\n",
      "mse:0.0023145144805312157 rmse:0.048109401017427444 mae:0.03173883259296417 r2:0.9810472726821899\n",
      "epoch:87\n",
      "train mse:0.002335986355319619 rmse:0.04833204299211502 mae:0.03178941458463669 r2:0.9808483123779297\n",
      "mse:0.002810757141560316 rmse:0.05301657319068909 mae:0.03693125769495964 r2:0.9769837260246277\n",
      "epoch:88\n",
      "train mse:0.0029303848277777433 rmse:0.05413302779197693 mae:0.03660384565591812 r2:0.9759936928749084\n",
      "mse:0.0023822588846087456 rmse:0.04880838841199875 mae:0.03180775046348572 r2:0.9804925918579102\n",
      "epoch:89\n",
      "train mse:0.0016453457064926624 rmse:0.040562860667705536 mae:0.028043070808053017 r2:0.9865230321884155\n",
      "mse:0.002133161760866642 rmse:0.046186164021492004 mae:0.030927056446671486 r2:0.9825323224067688\n",
      "epoch:90\n",
      "train mse:0.001797625795006752 rmse:0.04239841550588608 mae:0.029296495020389557 r2:0.9853059649467468\n",
      "mse:0.0019294536905363202 rmse:0.04392554610967636 mae:0.030230246484279633 r2:0.9842004179954529\n",
      "epoch:91\n",
      "train mse:0.0023595041129738092 rmse:0.04857472702860832 mae:0.033645566552877426 r2:0.980701208114624\n",
      "mse:0.002135577844455838 rmse:0.04621231183409691 mae:0.03240544721484184 r2:0.9825125336647034\n",
      "epoch:92\n",
      "train mse:0.0023541597183793783 rmse:0.04851968213915825 mae:0.03340322896838188 r2:0.9807121157646179\n",
      "mse:0.002136895200237632 rmse:0.046226561069488525 mae:0.031687844544649124 r2:0.982501745223999\n",
      "epoch:93\n",
      "train mse:0.0018077277345582843 rmse:0.04251737892627716 mae:0.030144864693284035 r2:0.9852153658866882\n",
      "mse:0.0018205777741968632 rmse:0.04266822710633278 mae:0.028848569840192795 r2:0.9850919842720032\n",
      "epoch:94\n",
      "train mse:0.002300210762768984 rmse:0.04796051234006882 mae:0.032637495547533035 r2:0.9811703562736511\n",
      "mse:0.003046451834961772 rmse:0.0551946721971035 mae:0.03744194284081459 r2:0.9750537276268005\n",
      "epoch:95\n",
      "train mse:0.0021463120356202126 rmse:0.04632830619812012 mae:0.0322529673576355 r2:0.9824137687683105\n",
      "mse:0.0024605602957308292 rmse:0.049604035913944244 mae:0.03414814919233322 r2:0.9798513650894165\n",
      "epoch:96\n",
      "train mse:0.0018625606317073107 rmse:0.043157391250133514 mae:0.028730232268571854 r2:0.9847546219825745\n",
      "mse:0.002011409727856517 rmse:0.044848743826150894 mae:0.02916962467133999 r2:0.9835293292999268\n",
      "epoch:97\n",
      "train mse:0.0015483190072700381 rmse:0.03934868425130844 mae:0.026644671335816383 r2:0.987296462059021\n",
      "mse:0.0016679685795679688 rmse:0.04084077104926109 mae:0.027022946625947952 r2:0.9863415956497192\n",
      "epoch:98\n",
      "train mse:0.002001525368541479 rmse:0.04473840817809105 mae:0.031197866424918175 r2:0.9836307168006897\n",
      "mse:0.002075485186651349 rmse:0.04555749148130417 mae:0.03092115931212902 r2:0.983004629611969\n",
      "epoch:99\n",
      "train mse:0.0017998215043917298 rmse:0.04242430254817009 mae:0.03020101971924305 r2:0.9852699041366577\n",
      "mse:0.0022896036971360445 rmse:0.047849804162979126 mae:0.03161630779504776 r2:0.9812512993812561\n",
      "epoch:100\n",
      "train mse:0.0017118043033406138 rmse:0.04137395694851875 mae:0.028189342468976974 r2:0.9859693646430969\n",
      "mse:0.0019037285819649696 rmse:0.04363173618912697 mae:0.029138250276446342 r2:0.9844110608100891\n",
      "epoch:101\n",
      "train mse:0.0027726469561457634 rmse:0.05265592783689499 mae:0.03458751365542412 r2:0.9772292971611023\n",
      "mse:0.0036186182405799627 rmse:0.060154952108860016 mae:0.042428698390722275 r2:0.970368504524231\n",
      "epoch:102\n",
      "train mse:0.0021346681751310825 rmse:0.04620246961712837 mae:0.031063443049788475 r2:0.9825160503387451\n",
      "mse:0.001932513783685863 rmse:0.043960366398096085 mae:0.029308542609214783 r2:0.9841753840446472\n",
      "epoch:103\n",
      "train mse:0.0026377197355031967 rmse:0.05135873705148697 mae:0.03589985519647598 r2:0.9784209132194519\n",
      "mse:0.0022897187154740095 rmse:0.04785100370645523 mae:0.032397422939538956 r2:0.9812503457069397\n",
      "epoch:104\n",
      "train mse:0.0014506877632811666 rmse:0.03808789327740669 mae:0.026478361338377 r2:0.9881114363670349\n",
      "mse:0.0016263709403574467 rmse:0.040328290313482285 mae:0.026155220344662666 r2:0.9866822361946106\n",
      "epoch:105\n",
      "train mse:0.001641182927414775 rmse:0.040511514991521835 mae:0.02820376679301262 r2:0.9865847229957581\n",
      "mse:0.0018587767845019698 rmse:0.04311353340744972 mae:0.028885558247566223 r2:0.9847791790962219\n",
      "epoch:106\n",
      "train mse:0.0019551939330995083 rmse:0.04421757534146309 mae:0.029155032709240913 r2:0.9840099811553955\n",
      "mse:0.0017055727075785398 rmse:0.041298579424619675 mae:0.027052562683820724 r2:0.9860336780548096\n",
      "epoch:107\n",
      "train mse:0.0023550959303975105 rmse:0.048529330641031265 mae:0.03335747495293617 r2:0.9807169437408447\n",
      "mse:0.0026540192775428295 rmse:0.05151717737317085 mae:0.034444648772478104 r2:0.9782671928405762\n",
      "epoch:108\n",
      "train mse:0.0016087447293102741 rmse:0.04010916128754616 mae:0.027209356427192688 r2:0.9868238568305969\n",
      "mse:0.002423711819574237 rmse:0.049231208860874176 mae:0.032716527581214905 r2:0.9801531434059143\n",
      "epoch:109\n",
      "train mse:0.00205513765104115 rmse:0.0453336238861084 mae:0.030445504933595657 r2:0.9831568002700806\n",
      "mse:0.0019738052505999804 rmse:0.044427528977394104 mae:0.0297484640032053 r2:0.9838372468948364\n",
      "epoch:110\n",
      "train mse:0.0021258906926959753 rmse:0.04610738158226013 mae:0.03167890012264252 r2:0.9826337099075317\n",
      "mse:0.002131394576281309 rmse:0.046167027205228806 mae:0.031154492869973183 r2:0.9825468063354492\n",
      "epoch:111\n",
      "train mse:0.0015258132480084896 rmse:0.03906165808439255 mae:0.025743816047906876 r2:0.9875193238258362\n",
      "mse:0.0017639778088778257 rmse:0.041999734938144684 mae:0.02828146517276764 r2:0.9855554103851318\n",
      "epoch:112\n",
      "train mse:0.002047841204330325 rmse:0.04525307938456535 mae:0.0313263013958931 r2:0.983220636844635\n",
      "mse:0.0024744959082454443 rmse:0.04974430426955223 mae:0.03369125351309776 r2:0.9797372817993164\n",
      "epoch:113\n",
      "train mse:0.0019173715263605118 rmse:0.04378779977560043 mae:0.02861085534095764 r2:0.9842706918716431\n",
      "mse:0.0026232583913952112 rmse:0.05121775344014168 mae:0.0346241258084774 r2:0.978519082069397\n",
      "epoch:114\n",
      "train mse:0.0015905267791822553 rmse:0.03988140821456909 mae:0.026298534125089645 r2:0.9869502782821655\n",
      "mse:0.0016167268622666597 rmse:0.040208540856838226 mae:0.02736981399357319 r2:0.986761212348938\n",
      "epoch:115\n",
      "train mse:0.0024084369651973248 rmse:0.04907583072781563 mae:0.03404698520898819 r2:0.980303943157196\n",
      "mse:0.002198566449806094 rmse:0.04688887298107147 mae:0.03136610984802246 r2:0.9819967746734619\n",
      "epoch:116\n",
      "train mse:0.001761510968208313 rmse:0.04197035729885101 mae:0.02985367178916931 r2:0.9855716228485107\n",
      "mse:0.002233185339719057 rmse:0.04725658893585205 mae:0.031194426119327545 r2:0.9817132949829102\n",
      "epoch:117\n",
      "train mse:0.002294037025421858 rmse:0.04789610579609871 mae:0.032308001071214676 r2:0.9812197685241699\n",
      "mse:0.0017442951211705804 rmse:0.0417647585272789 mae:0.02806769870221615 r2:0.9857166409492493\n",
      "epoch:118\n",
      "train mse:0.0016147785354405642 rmse:0.04018430784344673 mae:0.0265542883425951 r2:0.9867838025093079\n",
      "mse:0.0016997831407934427 rmse:0.04122842475771904 mae:0.027375349774956703 r2:0.9860811233520508\n",
      "epoch:119\n",
      "train mse:0.0014397080522030592 rmse:0.037943486124277115 mae:0.026279479265213013 r2:0.9882230162620544\n",
      "mse:0.0017955971416085958 rmse:0.04237448796629906 mae:0.028485847637057304 r2:0.9852965474128723\n",
      "epoch:120\n",
      "train mse:0.0014822729863226414 rmse:0.03850029781460762 mae:0.027804048731923103 r2:0.9878979325294495\n",
      "mse:0.0018245269311591983 rmse:0.04271448031067848 mae:0.02876158058643341 r2:0.9850596189498901\n",
      "epoch:121\n",
      "train mse:0.0011706670047715306 rmse:0.034215010702610016 mae:0.02351725660264492 r2:0.9904359579086304\n",
      "mse:0.0014740729238837957 rmse:0.03839365765452385 mae:0.02550571970641613 r2:0.9879293441772461\n",
      "epoch:122\n",
      "train mse:0.0014681294560432434 rmse:0.03831617534160614 mae:0.025225630030035973 r2:0.9879568815231323\n",
      "mse:0.0015831089112907648 rmse:0.03978830203413963 mae:0.02674461156129837 r2:0.9870365262031555\n",
      "epoch:123\n",
      "train mse:0.001655407017096877 rmse:0.04068669304251671 mae:0.027891283854842186 r2:0.986471951007843\n",
      "mse:0.0017759330803528428 rmse:0.04214182123541832 mae:0.028349827975034714 r2:0.9854575395584106\n",
      "epoch:124\n",
      "train mse:0.0016351599479094148 rmse:0.040437109768390656 mae:0.027564628049731255 r2:0.9866266250610352\n",
      "mse:0.0017225694609805942 rmse:0.041503846645355225 mae:0.027462124824523926 r2:0.985894501209259\n",
      "epoch:125\n",
      "train mse:0.001760203274898231 rmse:0.04195477440953255 mae:0.028137419372797012 r2:0.9855984449386597\n",
      "mse:0.0020315581932663918 rmse:0.04507280886173248 mae:0.031045321375131607 r2:0.9833643436431885\n",
      "epoch:126\n",
      "train mse:0.0015390729531645775 rmse:0.03923102095723152 mae:0.02644910290837288 r2:0.9874070286750793\n",
      "mse:0.0017180765280500054 rmse:0.04144968464970589 mae:0.027141662314534187 r2:0.9859313368797302\n",
      "epoch:127\n",
      "train mse:0.002281565684825182 rmse:0.047765739262104034 mae:0.032664597034454346 r2:0.981296181678772\n",
      "mse:0.002332493895664811 rmse:0.04829590022563934 mae:0.03323397412896156 r2:0.9809000492095947\n",
      "epoch:128\n",
      "train mse:0.0016447862144559622 rmse:0.04055596515536308 mae:0.026260077953338623 r2:0.986539900302887\n",
      "mse:0.0020245390478521585 rmse:0.04499487578868866 mae:0.030334629118442535 r2:0.983421802520752\n",
      "epoch:129\n",
      "train mse:0.0015850105555728078 rmse:0.03981219232082367 mae:0.025533663108944893 r2:0.9869980216026306\n",
      "mse:0.0015349822351709008 rmse:0.03917884826660156 mae:0.02601412869989872 r2:0.9874305725097656\n",
      "epoch:130\n",
      "train mse:0.0014638144057244062 rmse:0.038259826600551605 mae:0.026300039142370224 r2:0.9880415797233582\n",
      "mse:0.0018556307768449187 rmse:0.04307703301310539 mae:0.02923400327563286 r2:0.9848049283027649\n",
      "epoch:131\n",
      "train mse:0.001946136704646051 rmse:0.04411504045128822 mae:0.029929889366030693 r2:0.9840533137321472\n",
      "mse:0.0016643323469907045 rmse:0.040796227753162384 mae:0.02781083434820175 r2:0.9863713979721069\n",
      "epoch:132\n",
      "train mse:0.002743668854236603 rmse:0.0523800402879715 mae:0.036037806421518326 r2:0.9775398373603821\n",
      "mse:0.002035001525655389 rmse:0.04511099308729172 mae:0.029766270890831947 r2:0.9833361506462097\n",
      "epoch:133\n",
      "train mse:0.0018922606250271201 rmse:0.04350012168288231 mae:0.029656190425157547 r2:0.9845197200775146\n",
      "mse:0.0018791088368743658 rmse:0.04334868863224983 mae:0.02881658636033535 r2:0.9846127033233643\n",
      "epoch:134\n",
      "train mse:0.0015154255088418722 rmse:0.03892846778035164 mae:0.02639598399400711 r2:0.9875996708869934\n",
      "mse:0.002144505036994815 rmse:0.04630880057811737 mae:0.031182173639535904 r2:0.9824394583702087\n",
      "epoch:135\n",
      "train mse:0.0014985023299232125 rmse:0.038710493594408035 mae:0.025937430560588837 r2:0.9877021908760071\n",
      "mse:0.0017260960303246975 rmse:0.04154631122946739 mae:0.027482395991683006 r2:0.9858656525611877\n",
      "epoch:136\n",
      "train mse:0.0016003202181309462 rmse:0.040004003793001175 mae:0.026958124712109566 r2:0.9869096279144287\n",
      "mse:0.0017862782115116715 rmse:0.04226438328623772 mae:0.028308115899562836 r2:0.9853728413581848\n",
      "epoch:137\n",
      "train mse:0.0017299342434853315 rmse:0.04159247875213623 mae:0.027997545897960663 r2:0.985845148563385\n",
      "mse:0.002049331786110997 rmse:0.045269545167684555 mae:0.030873391777276993 r2:0.983218789100647\n",
      "epoch:138\n",
      "train mse:0.0019064098596572876 rmse:0.043662454932928085 mae:0.02976345457136631 r2:0.9844290614128113\n",
      "mse:0.002512031700462103 rmse:0.050120171159505844 mae:0.034930430352687836 r2:0.9794299006462097\n",
      "epoch:139\n",
      "train mse:0.0016614057822152972 rmse:0.0407603457570076 mae:0.027365388348698616 r2:0.9864133596420288\n",
      "mse:0.0015789262251928449 rmse:0.03973570466041565 mae:0.02673269994556904 r2:0.9870707392692566\n",
      "epoch:140\n",
      "train mse:0.0017920138780027628 rmse:0.042332183569669724 mae:0.027997249737381935 r2:0.9853276014328003\n",
      "mse:0.00174077064730227 rmse:0.04172254353761673 mae:0.027792135253548622 r2:0.9857454895973206\n",
      "epoch:141\n",
      "train mse:0.002109722001478076 rmse:0.04593170806765556 mae:0.03138303756713867 r2:0.9827389717102051\n",
      "mse:0.0027002363931387663 rmse:0.05196379870176315 mae:0.03375460207462311 r2:0.9778887629508972\n",
      "epoch:142\n",
      "train mse:0.0017158586997538805 rmse:0.041422925889492035 mae:0.028277061879634857 r2:0.9859461784362793\n",
      "mse:0.0016872413689270616 rmse:0.041076041758060455 mae:0.027489233762025833 r2:0.9861838221549988\n",
      "epoch:143\n",
      "train mse:0.002999141113832593 rmse:0.054764412343502045 mae:0.038348108530044556 r2:0.975459098815918\n",
      "mse:0.002226353157311678 rmse:0.04718424752354622 mae:0.03171626850962639 r2:0.9817692041397095\n",
      "epoch:144\n",
      "train mse:0.0019220218528062105 rmse:0.04384087026119232 mae:0.028733208775520325 r2:0.9842494130134583\n",
      "mse:0.0015647612744942307 rmse:0.03955706208944321 mae:0.026838816702365875 r2:0.9871867299079895\n",
      "epoch:145\n",
      "train mse:0.0018919432768598199 rmse:0.043496474623680115 mae:0.027955548837780952 r2:0.9845089316368103\n",
      "mse:0.0018146338406950235 rmse:0.04259851947426796 mae:0.02901967242360115 r2:0.9851406216621399\n",
      "epoch:146\n",
      "train mse:0.001540962839499116 rmse:0.03925509750843048 mae:0.02685548923909664 r2:0.9874019622802734\n",
      "mse:0.001666413969360292 rmse:0.04082173481583595 mae:0.02783529832959175 r2:0.9863543510437012\n",
      "epoch:147\n",
      "train mse:0.0021142764016985893 rmse:0.045981261879205704 mae:0.031645603477954865 r2:0.982704222202301\n",
      "mse:0.002307126298546791 rmse:0.04803255572915077 mae:0.0317687951028347 r2:0.981107771396637\n",
      "epoch:148\n",
      "train mse:0.001953781582415104 rmse:0.044201601296663284 mae:0.029786299914121628 r2:0.9839836359024048\n",
      "mse:0.0019066259264945984 rmse:0.043664928525686264 mae:0.028774725273251534 r2:0.9843873381614685\n",
      "epoch:149\n",
      "train mse:0.002007779199630022 rmse:0.04480824992060661 mae:0.029175909236073494 r2:0.9835538268089294\n",
      "mse:0.0030807899311184883 rmse:0.05550486221909523 mae:0.0374990850687027 r2:0.974772572517395\n"
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    for X, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            tr_mse = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        tr_rmse = tf.sqrt(tr_mse)\n",
    "        tr_mae = tf.reduce_mean(tf.abs(y_pred - y))\n",
    "        tr_r2 = 1 - tf.reduce_sum(tf.square(y_pred - y)) / tf.reduce_sum(tf.square(y - tf.reduce_mean(y)))\n",
    "        grads = tape.gradient(tr_mse, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    print(\"epoch:{}\".format(i))\n",
    "    print(\"train mse:{} rmse:{} mae:{} r2:{}\".format(tr_mse, tr_rmse, tr_mae, tr_r2))\n",
    "    valiAll(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
