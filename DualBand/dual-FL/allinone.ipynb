{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712e2fa-92bb-499b-801e-950dfcda6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 22:14:58.360302: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-26 22:14:58.833831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b51fc08-787d-42bb-bf42-65eacb5ce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "l_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a8dd19-5960-44cc-90c6-e2c42c86b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=8)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        output = self.dense5(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f65bda-5f20-4df4-b12e-e0fdf99c5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valiAll(index_epoch):\n",
    "    y_v_p = model(X_v)\n",
    "    va_mse = tf.reduce_mean(tf.square(y_v_p - y_v))\n",
    "    va_rmse = tf.sqrt(va_mse)\n",
    "    va_mae = tf.reduce_mean(tf.abs(y_v_p - y_v))\n",
    "    va_r2 = 1 - tf.reduce_sum(tf.square(y_v_p - y_v)) / tf.reduce_sum(tf.square(y_v - tf.reduce_mean(y_v)))\n",
    "    print(\"mse:{} rmse:{} mae:{} r2:{}\".format(va_mse, va_rmse, va_mae, va_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b543212c-3492-45eb-ad62-77930b2d4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"testset.csv\", encoding='utf-8').sample(frac=1).reset_index(drop=True)\n",
    "X_v = test_dataset.loc[:,'freq':'L2'].to_numpy(dtype = np.float32)\n",
    "y_v = test_dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eef8846-dcb9-4aca-bdb0-cfddcec2ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 22:15:00.720059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9604 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2023-09-26 22:15:00.720587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9621 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('./20-24Trainset.csv', encoding='utf-8')\n",
    "dataset2 = pd.read_csv('./50-54Trainset.csv', encoding='utf-8')\n",
    "dataset = pd.concat([dataset1, dataset2], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "X = dataset.loc[:,'freq':'L2'].to_numpy(dtype = np.float32)\n",
    "y = dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=X.shape[0])\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1327167a-9b84-45dd-a8c1-edf35b22e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624eb49b-e8c8-4d45-bc8b-b020e9398908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 22:15:01.766820: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d4483d81b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-26 22:15:01.766840: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-26 22:15:01.766844: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-26 22:15:01.769880: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-26 22:15:01.874650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-26 22:15:01.982119: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f82d03391f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f82d03391f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch:0\n",
      "train mse:0.04797578975558281 rmse:0.21903376281261444 mae:0.17104573547840118 r2:0.6074907779693604\n",
      "mse:0.05394343286752701 rmse:0.23225724697113037 mae:0.18157859146595 r2:0.55818772315979\n",
      "epoch:1\n",
      "train mse:0.040751758962869644 rmse:0.2018706500530243 mae:0.15815585851669312 r2:0.6667999029159546\n",
      "mse:0.04484250396490097 rmse:0.2117604911327362 mae:0.16445299983024597 r2:0.6327269673347473\n",
      "epoch:2\n",
      "train mse:0.0385105274617672 rmse:0.19624099135398865 mae:0.15314337611198425 r2:0.6832287311553955\n",
      "mse:0.03858768939971924 rmse:0.19643749296665192 mae:0.151967853307724 r2:0.6839556694030762\n",
      "epoch:3\n",
      "train mse:0.040450114756822586 rmse:0.20112213492393494 mae:0.15411455929279327 r2:0.6688069105148315\n",
      "mse:0.04238121211528778 rmse:0.2058669775724411 mae:0.15830989181995392 r2:0.6528856754302979\n",
      "epoch:4\n",
      "train mse:0.030904388055205345 rmse:0.17579643428325653 mae:0.13156168162822723 r2:0.7444779872894287\n",
      "mse:0.034121349453926086 rmse:0.18471965193748474 mae:0.1395758092403412 r2:0.7205363512039185\n",
      "epoch:5\n",
      "train mse:0.03401792421936989 rmse:0.18443948030471802 mae:0.1387338787317276 r2:0.720721960067749\n",
      "mse:0.031174417585134506 rmse:0.17656278610229492 mae:0.13201718032360077 r2:0.7446725368499756\n",
      "epoch:6\n",
      "train mse:0.02760223299264908 rmse:0.16613920032978058 mae:0.1207723394036293 r2:0.7741466164588928\n",
      "mse:0.030716845765709877 rmse:0.1752622127532959 mae:0.12986572086811066 r2:0.7484201788902283\n",
      "epoch:7\n",
      "train mse:0.021739371120929718 rmse:0.1474427729845047 mae:0.11183691024780273 r2:0.8222670555114746\n",
      "mse:0.02417926676571369 rmse:0.15549683570861816 mae:0.11357668787240982 r2:0.8019648194313049\n",
      "epoch:8\n",
      "train mse:0.018681496381759644 rmse:0.13668027520179749 mae:0.09845509380102158 r2:0.8468465209007263\n",
      "mse:0.022532528266310692 rmse:0.15010838210582733 mae:0.10650718957185745 r2:0.8154520988464355\n",
      "epoch:9\n",
      "train mse:0.021458422765135765 rmse:0.14648693799972534 mae:0.10601616650819778 r2:0.8245425820350647\n",
      "mse:0.021428897976875305 rmse:0.14638613164424896 mae:0.10354254394769669 r2:0.8244911432266235\n",
      "epoch:10\n",
      "train mse:0.028541574254631996 rmse:0.16894251108169556 mae:0.12622414529323578 r2:0.7659651041030884\n",
      "mse:0.033022284507751465 rmse:0.18172034621238708 mae:0.13191206753253937 r2:0.7295379638671875\n",
      "epoch:11\n",
      "train mse:0.014946874231100082 rmse:0.12225741147994995 mae:0.08666414022445679 r2:0.8774011135101318\n",
      "mse:0.017309630289673805 rmse:0.13156607747077942 mae:0.09195230156183243 r2:0.8582291603088379\n",
      "epoch:12\n",
      "train mse:0.016198279336094856 rmse:0.12727245688438416 mae:0.08871202915906906 r2:0.8673759698867798\n",
      "mse:0.01699904166162014 rmse:0.13038037717342377 mae:0.09246248006820679 r2:0.860772967338562\n",
      "epoch:13\n",
      "train mse:0.024926677346229553 rmse:0.15788184106349945 mae:0.1167886033654213 r2:0.7959887981414795\n",
      "mse:0.02428228221833706 rmse:0.15582773089408875 mae:0.11240354180335999 r2:0.8011211156845093\n",
      "epoch:14\n",
      "train mse:0.016543637961149216 rmse:0.12862206995487213 mae:0.08874112367630005 r2:0.8640955090522766\n",
      "mse:0.018963247537612915 rmse:0.13770709931850433 mae:0.10041457414627075 r2:0.8446855545043945\n",
      "epoch:15\n",
      "train mse:0.019168151542544365 rmse:0.13844908773899078 mae:0.09856876730918884 r2:0.8418596982955933\n",
      "mse:0.017502376809716225 rmse:0.13229654729366302 mae:0.09488065540790558 r2:0.8566504716873169\n",
      "epoch:16\n",
      "train mse:0.01292475312948227 rmse:0.11368708312511444 mae:0.08071382343769073 r2:0.8937448263168335\n",
      "mse:0.012608605436980724 rmse:0.11228804290294647 mae:0.07802408933639526 r2:0.8967319130897522\n",
      "epoch:17\n",
      "train mse:0.013957451097667217 rmse:0.11814165860414505 mae:0.08081481605768204 r2:0.8855705261230469\n",
      "mse:0.011671934276819229 rmse:0.10803672671318054 mae:0.0753193125128746 r2:0.9044035077095032\n",
      "epoch:18\n",
      "train mse:0.010842744261026382 rmse:0.10412850230932236 mae:0.06811528652906418 r2:0.9109475016593933\n",
      "mse:0.012105613946914673 rmse:0.11002551019191742 mae:0.07545726001262665 r2:0.9008515477180481\n",
      "epoch:19\n",
      "train mse:0.00985375139862299 rmse:0.09926606714725494 mae:0.06943420320749283 r2:0.9193348288536072\n",
      "mse:0.01123214140534401 rmse:0.10598179697990417 mae:0.07492253184318542 r2:0.9080055356025696\n",
      "epoch:20\n",
      "train mse:0.00924252811819315 rmse:0.09613806754350662 mae:0.06557520478963852 r2:0.9241505265235901\n",
      "mse:0.010908951982855797 rmse:0.1044459268450737 mae:0.07154364883899689 r2:0.9106525182723999\n",
      "epoch:21\n",
      "train mse:0.008788473904132843 rmse:0.0937468633055687 mae:0.06645273417234421 r2:0.9281054139137268\n",
      "mse:0.010273457504808903 rmse:0.10135806351900101 mae:0.07032787799835205 r2:0.9158574342727661\n",
      "epoch:22\n",
      "train mse:0.007715556770563126 rmse:0.08783823996782303 mae:0.060799624770879745 r2:0.9369485974311829\n",
      "mse:0.011127625592052937 rmse:0.10548756271600723 mae:0.07265879213809967 r2:0.908861517906189\n",
      "epoch:23\n",
      "train mse:0.007204773370176554 rmse:0.08488093316555023 mae:0.05933396890759468 r2:0.9411503672599792\n",
      "mse:0.010251383297145367 rmse:0.10124911367893219 mae:0.07213259488344193 r2:0.9160382151603699\n",
      "epoch:24\n",
      "train mse:0.010152786038815975 rmse:0.1007610335946083 mae:0.06597592681646347 r2:0.9166903495788574\n",
      "mse:0.017847977578639984 rmse:0.13359631597995758 mae:0.0901150181889534 r2:0.8538199067115784\n",
      "epoch:25\n",
      "train mse:0.009244433604180813 rmse:0.09614797681570053 mae:0.06440241634845734 r2:0.9236424565315247\n",
      "mse:0.009907804429531097 rmse:0.09953795373439789 mae:0.06808993965387344 r2:0.9188522100448608\n",
      "epoch:26\n",
      "train mse:0.006005138158798218 rmse:0.07749282568693161 mae:0.053265318274497986 r2:0.9507544040679932\n",
      "mse:0.008305891416966915 rmse:0.09113666415214539 mae:0.0626545399427414 r2:0.9319723844528198\n",
      "epoch:27\n",
      "train mse:0.008526313118636608 rmse:0.09233803302049637 mae:0.06039745733141899 r2:0.9298256039619446\n",
      "mse:0.01268432941287756 rmse:0.11262472718954086 mae:0.0778886079788208 r2:0.8961116671562195\n",
      "epoch:28\n",
      "train mse:0.0130277955904603 rmse:0.1141393706202507 mae:0.07824888080358505 r2:0.8933032751083374\n",
      "mse:0.010519460774958134 rmse:0.10256442427635193 mae:0.07269233465194702 r2:0.9138425588607788\n",
      "epoch:29\n",
      "train mse:0.013919662684202194 rmse:0.11798162013292313 mae:0.08296363800764084 r2:0.8853070139884949\n",
      "mse:0.013866116292774677 rmse:0.11775447428226471 mae:0.08661933243274689 r2:0.8864325284957886\n",
      "epoch:30\n",
      "train mse:0.007421950343996286 rmse:0.08615074306726456 mae:0.05996057763695717 r2:0.9393474459648132\n",
      "mse:0.008797414600849152 rmse:0.09379453212022781 mae:0.06567488610744476 r2:0.9279466271400452\n",
      "epoch:31\n",
      "train mse:0.006181671749800444 rmse:0.07862360775470734 mae:0.050306376069784164 r2:0.9492415189743042\n",
      "mse:0.007040877360850573 rmse:0.08390993624925613 mae:0.057580236345529556 r2:0.9423331618309021\n",
      "epoch:32\n",
      "train mse:0.00748490309342742 rmse:0.08651533722877502 mae:0.0569293387234211 r2:0.9385814666748047\n",
      "mse:0.010111474432051182 rmse:0.10055582970380783 mae:0.069645456969738 r2:0.9171841144561768\n",
      "epoch:33\n",
      "train mse:0.006144365761429071 rmse:0.07838600128889084 mae:0.05604708194732666 r2:0.9495863914489746\n",
      "mse:0.013743417337536812 rmse:0.1172323226928711 mae:0.07888127863407135 r2:0.8874374628067017\n",
      "epoch:34\n",
      "train mse:0.007090559229254723 rmse:0.08420545607805252 mae:0.05434059724211693 r2:0.9417415261268616\n",
      "mse:0.007657479960471392 rmse:0.08750702440738678 mae:0.06087234988808632 r2:0.9372830390930176\n",
      "epoch:35\n",
      "train mse:0.006535206455737352 rmse:0.08084062486886978 mae:0.058454662561416626 r2:0.9465814828872681\n",
      "mse:0.008013634942471981 rmse:0.08951890468597412 mae:0.061663538217544556 r2:0.9343660473823547\n",
      "epoch:36\n",
      "train mse:0.004176931455731392 rmse:0.06462918221950531 mae:0.04489611089229584 r2:0.9658218622207642\n",
      "mse:0.006161849945783615 rmse:0.07849745452404022 mae:0.05258292332291603 r2:0.949532687664032\n",
      "epoch:37\n",
      "train mse:0.006743923295289278 rmse:0.08212139457464218 mae:0.05777670070528984 r2:0.9447247982025146\n",
      "mse:0.008335988968610764 rmse:0.09130163490772247 mae:0.06309685111045837 r2:0.9317258596420288\n",
      "epoch:38\n",
      "train mse:0.005620123818516731 rmse:0.07496748119592667 mae:0.05306459963321686 r2:0.9540369510650635\n",
      "mse:0.007463420741260052 rmse:0.08639109134674072 mae:0.05824810639023781 r2:0.9388724565505981\n",
      "epoch:39\n",
      "train mse:0.00451917527243495 rmse:0.06722480803728104 mae:0.04547348618507385 r2:0.9630535840988159\n",
      "mse:0.006192879285663366 rmse:0.07869485020637512 mae:0.05445556715130806 r2:0.9492785334587097\n",
      "epoch:40\n",
      "train mse:0.004934155382215977 rmse:0.07024354487657547 mae:0.04805857315659523 r2:0.9595714211463928\n",
      "mse:0.006764260120689869 rmse:0.0822451189160347 mae:0.056001048535108566 r2:0.9445987343788147\n",
      "epoch:41\n",
      "train mse:0.006724028382450342 rmse:0.08200017362833023 mae:0.05615224316716194 r2:0.944961667060852\n",
      "mse:0.013977311551570892 rmse:0.11822567880153656 mae:0.08035478740930557 r2:0.8855217695236206\n",
      "epoch:42\n",
      "train mse:0.006692484021186829 rmse:0.08180760592222214 mae:0.05501945689320564 r2:0.945114016532898\n",
      "mse:0.006471975706517696 rmse:0.08044859021902084 mae:0.05538878217339516 r2:0.9469926357269287\n",
      "epoch:43\n",
      "train mse:0.004449017345905304 rmse:0.06670095771551132 mae:0.0509876124560833 r2:0.9634738564491272\n",
      "mse:0.007944226264953613 rmse:0.08913038671016693 mae:0.06129910796880722 r2:0.9349344968795776\n",
      "epoch:44\n",
      "train mse:0.005086038261651993 rmse:0.07131646573543549 mae:0.04641636088490486 r2:0.9583732485771179\n",
      "mse:0.0067613739520311356 rmse:0.08222757279872894 mae:0.05682257190346718 r2:0.9446223974227905\n",
      "epoch:45\n",
      "train mse:0.005984960589557886 rmse:0.07736252248287201 mae:0.05436640977859497 r2:0.9508950114250183\n",
      "mse:0.007860048674046993 rmse:0.08865690976381302 mae:0.062335167080163956 r2:0.9356239438056946\n",
      "epoch:46\n",
      "train mse:0.005679188761860132 rmse:0.07536038756370544 mae:0.051846589893102646 r2:0.9534761905670166\n",
      "mse:0.006242748349905014 rmse:0.07901106029748917 mae:0.054152991622686386 r2:0.948870062828064\n",
      "epoch:47\n",
      "train mse:0.006509900093078613 rmse:0.08068395406007767 mae:0.05798858404159546 r2:0.9467961192131042\n",
      "mse:0.007707302924245596 rmse:0.08779124170541763 mae:0.06111590564250946 r2:0.9368749856948853\n",
      "epoch:48\n",
      "train mse:0.004274091683328152 rmse:0.06537653505802155 mae:0.045911166816949844 r2:0.9649609327316284\n",
      "mse:0.0054668462835252285 rmse:0.07393812388181686 mae:0.05018239840865135 r2:0.9552249312400818\n",
      "epoch:49\n",
      "train mse:0.004002473317086697 rmse:0.06326510012149811 mae:0.044160395860672 r2:0.9672988057136536\n",
      "mse:0.006082642823457718 rmse:0.0779912993311882 mae:0.053835444152355194 r2:0.9501814246177673\n",
      "epoch:50\n",
      "train mse:0.004261696245521307 rmse:0.06528166681528091 mae:0.04856347665190697 r2:0.9650567770004272\n",
      "mse:0.006059529259800911 rmse:0.07784298062324524 mae:0.0530434250831604 r2:0.950370728969574\n",
      "epoch:51\n",
      "train mse:0.004812282044440508 rmse:0.06937061250209808 mae:0.04602919518947601 r2:0.9604392647743225\n",
      "mse:0.006281623616814613 rmse:0.07925669848918915 mae:0.05242044851183891 r2:0.9485517144203186\n",
      "epoch:52\n",
      "train mse:0.0037758194375783205 rmse:0.06144769489765167 mae:0.04278978705406189 r2:0.969109058380127\n",
      "mse:0.0059163994155824184 rmse:0.07691813260316849 mae:0.05203291028738022 r2:0.9515429735183716\n",
      "epoch:53\n",
      "train mse:0.0048772539012134075 rmse:0.06983733922243118 mae:0.04723261296749115 r2:0.9598039984703064\n",
      "mse:0.006067821756005287 rmse:0.07789622247219086 mae:0.054248008877038956 r2:0.95030277967453\n",
      "epoch:54\n",
      "train mse:0.00487398449331522 rmse:0.06981392949819565 mae:0.04916135594248772 r2:0.9600851535797119\n",
      "mse:0.006500654388219118 rmse:0.08062663674354553 mae:0.05440526828169823 r2:0.9467577934265137\n",
      "epoch:55\n",
      "train mse:0.0035660855937749147 rmse:0.059716712683439255 mae:0.04077882319688797 r2:0.9708559513092041\n",
      "mse:0.004736312199383974 rmse:0.06882087141275406 mae:0.04584577679634094 r2:0.9612082242965698\n",
      "epoch:56\n",
      "train mse:0.004951414652168751 rmse:0.0703662857413292 mae:0.05049991235136986 r2:0.9593737721443176\n",
      "mse:0.00490298168733716 rmse:0.07002129405736923 mae:0.047630827873945236 r2:0.9598431587219238\n",
      "epoch:57\n",
      "train mse:0.007588167209178209 rmse:0.08711008727550507 mae:0.05262644588947296 r2:0.9375240206718445\n",
      "mse:0.007979897782206535 rmse:0.0893302708864212 mae:0.06143287196755409 r2:0.9346423149108887\n",
      "epoch:58\n",
      "train mse:0.004572717007249594 rmse:0.0676218643784523 mae:0.04433589056134224 r2:0.9624209403991699\n",
      "mse:0.006015566177666187 rmse:0.07756008207798004 mae:0.05115097761154175 r2:0.9507308006286621\n",
      "epoch:59\n",
      "train mse:0.004066155292093754 rmse:0.06376641243696213 mae:0.04294270649552345 r2:0.9667724370956421\n",
      "mse:0.0048029604367911816 rmse:0.06930339336395264 mae:0.046498507261276245 r2:0.9606623649597168\n",
      "epoch:60\n",
      "train mse:0.003204369219020009 rmse:0.05660714954137802 mae:0.03956177085638046 r2:0.973779022693634\n",
      "mse:0.0055228834971785545 rmse:0.07431610673666 mae:0.05164400488138199 r2:0.9547659754753113\n",
      "epoch:61\n",
      "train mse:0.005611461121588945 rmse:0.07490968704223633 mae:0.04827024042606354 r2:0.9539915323257446\n",
      "mse:0.007685712538659573 rmse:0.08766819536685944 mae:0.05817871168255806 r2:0.9370517730712891\n",
      "epoch:62\n",
      "train mse:0.006441627163439989 rmse:0.08025974780321121 mae:0.05543245002627373 r2:0.947359025478363\n",
      "mse:0.007825769484043121 rmse:0.08846337348222733 mae:0.05970750004053116 r2:0.9359046816825867\n",
      "epoch:63\n",
      "train mse:0.005078524816781282 rmse:0.07126376777887344 mae:0.05050171539187431 r2:0.9584203362464905\n",
      "mse:0.006630972493439913 rmse:0.08143078535795212 mae:0.05444719269871712 r2:0.9456904530525208\n",
      "epoch:64\n",
      "train mse:0.004001046065241098 rmse:0.06325381994247437 mae:0.04187647998332977 r2:0.9671616554260254\n",
      "mse:0.005372751038521528 rmse:0.07329905033111572 mae:0.05012643709778786 r2:0.9559956192970276\n",
      "epoch:65\n",
      "train mse:0.00683538569137454 rmse:0.08267639577388763 mae:0.0606204979121685 r2:0.9440171718597412\n",
      "mse:0.008838925510644913 rmse:0.09401556104421616 mae:0.06418722122907639 r2:0.9276066422462463\n",
      "epoch:66\n",
      "train mse:0.004715952556580305 rmse:0.0686727911233902 mae:0.04678916931152344 r2:0.9615373015403748\n",
      "mse:0.0069129616022109985 rmse:0.08314421772956848 mae:0.054345402866601944 r2:0.9433808326721191\n",
      "epoch:67\n",
      "train mse:0.005210538860410452 rmse:0.07218406349420547 mae:0.05016162991523743 r2:0.957318902015686\n",
      "mse:0.0076789711602032185 rmse:0.08762973546981812 mae:0.05821707099676132 r2:0.9371070265769958\n",
      "epoch:68\n",
      "train mse:0.003717903746291995 rmse:0.060974616557359695 mae:0.04223218932747841 r2:0.9696133136749268\n",
      "mse:0.004915263969451189 rmse:0.07010894268751144 mae:0.045740339905023575 r2:0.959742546081543\n",
      "epoch:69\n",
      "train mse:0.002882180968299508 rmse:0.05368594452738762 mae:0.03811810538172722 r2:0.9763663411140442\n",
      "mse:0.00534040154889226 rmse:0.07307805120944977 mae:0.04887087270617485 r2:0.9562605619430542\n",
      "epoch:70\n",
      "train mse:0.002452861052006483 rmse:0.04952636733651161 mae:0.03424472734332085 r2:0.9798891544342041\n",
      "mse:0.00535858329385519 rmse:0.07320234179496765 mae:0.048021938651800156 r2:0.9561116695404053\n",
      "epoch:71\n",
      "train mse:0.0027514630928635597 rmse:0.0524543896317482 mae:0.03707457333803177 r2:0.9774691462516785\n",
      "mse:0.004661710932850838 rmse:0.06827671825885773 mae:0.04497743397951126 r2:0.9618192315101624\n",
      "epoch:72\n",
      "train mse:0.004419825505465269 rmse:0.06648176908493042 mae:0.0433088056743145 r2:0.9638006091117859\n",
      "mse:0.004844223614782095 rmse:0.06960045546293259 mae:0.047311436384916306 r2:0.9603244066238403\n",
      "epoch:73\n",
      "train mse:0.0028037764132022858 rmse:0.052950698882341385 mae:0.037056904286146164 r2:0.9770194888114929\n",
      "mse:0.006015833001583815 rmse:0.07756180316209793 mae:0.053019214421510696 r2:0.9507285952568054\n",
      "epoch:74\n",
      "train mse:0.0033811803441494703 rmse:0.058147918432950974 mae:0.038289401680231094 r2:0.9722419381141663\n",
      "mse:0.004425393883138895 rmse:0.06652363389730453 mae:0.04423151910305023 r2:0.9637547731399536\n",
      "epoch:75\n",
      "train mse:0.005633951630443335 rmse:0.07505965232849121 mae:0.05453295633196831 r2:0.9535638689994812\n",
      "mse:0.005770027171820402 rmse:0.07596069574356079 mae:0.05135251581668854 r2:0.952741801738739\n",
      "epoch:76\n",
      "train mse:0.006287385243922472 rmse:0.07929303497076035 mae:0.048835620284080505 r2:0.9482642412185669\n",
      "mse:0.007125902455300093 rmse:0.08441505581140518 mae:0.05772871524095535 r2:0.9416368007659912\n",
      "epoch:77\n",
      "train mse:0.007989500649273396 rmse:0.08938400447368622 mae:0.06341619044542313 r2:0.9346451759338379\n",
      "mse:0.005730439908802509 rmse:0.07569967210292816 mae:0.05277304723858833 r2:0.9530660510063171\n",
      "epoch:78\n",
      "train mse:0.0035735603887587786 rmse:0.05977926403284073 mae:0.03936513885855675 r2:0.9707494974136353\n",
      "mse:0.004527553450316191 rmse:0.06728709489107132 mae:0.04472466558218002 r2:0.9629180431365967\n",
      "epoch:79\n",
      "train mse:0.003985536750406027 rmse:0.06313110888004303 mae:0.04070734232664108 r2:0.967409610748291\n",
      "mse:0.004109888803213835 rmse:0.06410840898752213 mae:0.042615342885255814 r2:0.9663388133049011\n",
      "epoch:80\n",
      "train mse:0.002832254394888878 rmse:0.05321892723441124 mae:0.03447858616709709 r2:0.9768036007881165\n",
      "mse:0.004033180419355631 rmse:0.0635073259472847 mae:0.04182228818535805 r2:0.9669671058654785\n",
      "epoch:81\n",
      "train mse:0.0050255791284143925 rmse:0.07089132070541382 mae:0.04704901576042175 r2:0.9586137533187866\n",
      "mse:0.009097780101001263 rmse:0.09538228064775467 mae:0.06527746468782425 r2:0.9254865646362305\n",
      "epoch:82\n",
      "train mse:0.005531383212655783 rmse:0.0743732675909996 mae:0.05180820822715759 r2:0.9547552466392517\n",
      "mse:0.008868301287293434 rmse:0.09417165815830231 mae:0.06353837996721268 r2:0.9273660778999329\n",
      "epoch:83\n",
      "train mse:0.0025938996113836765 rmse:0.050930339843034744 mae:0.03493731841444969 r2:0.978629469871521\n",
      "mse:0.004159943200647831 rmse:0.06449761986732483 mae:0.043252501636743546 r2:0.965928852558136\n",
      "epoch:84\n",
      "train mse:0.003212579293176532 rmse:0.056679617613554 mae:0.03493286296725273 r2:0.9735419750213623\n",
      "mse:0.0046008070930838585 rmse:0.06782925128936768 mae:0.045644599944353104 r2:0.9623180627822876\n",
      "epoch:85\n",
      "train mse:0.004918042104691267 rmse:0.07012875378131866 mae:0.04249770939350128 r2:0.95964515209198\n",
      "mse:0.008301254361867905 rmse:0.09111122041940689 mae:0.06084521487355232 r2:0.9320103526115417\n",
      "epoch:86\n",
      "train mse:0.00389029155485332 rmse:0.062372200191020966 mae:0.04283920302987099 r2:0.9681416749954224\n",
      "mse:0.0056265112943947315 rmse:0.07501007616519928 mae:0.05094730481505394 r2:0.9539172649383545\n",
      "epoch:87\n",
      "train mse:0.0023944955319166183 rmse:0.04893358051776886 mae:0.03332749381661415 r2:0.9803454279899597\n",
      "mse:0.003447651630267501 rmse:0.058716706931591034 mae:0.038272447884082794 r2:0.9717627167701721\n",
      "epoch:88\n",
      "train mse:0.005458254367113113 rmse:0.07388000190258026 mae:0.04997655376791954 r2:0.9552276134490967\n",
      "mse:0.007274635136127472 rmse:0.08529146760702133 mae:0.056264106184244156 r2:0.9404186606407166\n",
      "epoch:89\n",
      "train mse:0.003058460308238864 rmse:0.05530334636569023 mae:0.036217398941516876 r2:0.9748340845108032\n",
      "mse:0.004365433007478714 rmse:0.06607142090797424 mae:0.04315841197967529 r2:0.9642458558082581\n",
      "epoch:90\n",
      "train mse:0.002112612361088395 rmse:0.04596316069364548 mae:0.031589359045028687 r2:0.9827569127082825\n",
      "mse:0.005014318041503429 rmse:0.07081184536218643 mae:0.048459332436323166 r2:0.9589312672615051\n",
      "epoch:91\n",
      "train mse:0.0034999807830899954 rmse:0.059160634875297546 mae:0.036042992025613785 r2:0.9713121652603149\n",
      "mse:0.00445545744150877 rmse:0.0667492151260376 mae:0.04471762105822563 r2:0.9635085463523865\n",
      "epoch:92\n",
      "train mse:0.0025081888306885958 rmse:0.05008181929588318 mae:0.03476734831929207 r2:0.9795264601707458\n",
      "mse:0.0037281012628227472 rmse:0.061058178544044495 mae:0.039856113493442535 r2:0.9694657921791077\n",
      "epoch:93\n",
      "train mse:0.003551323665305972 rmse:0.05959298089146614 mae:0.03960869088768959 r2:0.9708161354064941\n",
      "mse:0.0037497372832149267 rmse:0.06123510003089905 mae:0.04053027182817459 r2:0.9692885875701904\n",
      "epoch:94\n",
      "train mse:0.002235117368400097 rmse:0.04727702587842941 mae:0.033543605357408524 r2:0.9817786812782288\n",
      "mse:0.0034626570995897055 rmse:0.058844346553087234 mae:0.03912625089287758 r2:0.9716398119926453\n",
      "epoch:95\n",
      "train mse:0.004267125856131315 rmse:0.06532324105501175 mae:0.045884568244218826 r2:0.9648113250732422\n",
      "mse:0.006376622710376978 rmse:0.07985375821590424 mae:0.054530590772628784 r2:0.9477736353874207\n",
      "epoch:96\n",
      "train mse:0.0027590428944677114 rmse:0.05252659320831299 mae:0.03762734308838844 r2:0.9772724509239197\n",
      "mse:0.004257749766111374 rmse:0.0652514323592186 mae:0.04281533136963844 r2:0.9651278257369995\n",
      "epoch:97\n",
      "train mse:0.004778873175382614 rmse:0.06912939250469208 mae:0.04692612960934639 r2:0.9606611132621765\n",
      "mse:0.01231745257973671 rmse:0.11098401993513107 mae:0.07823454588651657 r2:0.8991165161132812\n",
      "epoch:98\n",
      "train mse:0.0023601793218404055 rmse:0.048581674695014954 mae:0.03641347214579582 r2:0.980747401714325\n",
      "mse:0.004113692790269852 rmse:0.06413807719945908 mae:0.043905504047870636 r2:0.9663076400756836\n",
      "epoch:99\n",
      "train mse:0.0035369780380278826 rmse:0.059472497552633286 mae:0.03771338239312172 r2:0.970856249332428\n",
      "mse:0.006244313903152943 rmse:0.07902096956968307 mae:0.052392274141311646 r2:0.9488572478294373\n",
      "epoch:100\n",
      "train mse:0.0024601027835160494 rmse:0.04959942400455475 mae:0.033929578959941864 r2:0.9798691868782043\n",
      "mse:0.004622336011379957 rmse:0.06798776239156723 mae:0.045194562524557114 r2:0.962141752243042\n",
      "epoch:101\n",
      "train mse:0.003686195472255349 rmse:0.06071404740214348 mae:0.04419796168804169 r2:0.9698998928070068\n",
      "mse:0.00445331959053874 rmse:0.06673319637775421 mae:0.044259730726480484 r2:0.9635260105133057\n",
      "epoch:102\n",
      "train mse:0.0024284052196890116 rmse:0.049278851598501205 mae:0.03531170263886452 r2:0.9801320433616638\n",
      "mse:0.003968323580920696 rmse:0.06299462914466858 mae:0.041647497564554214 r2:0.9674983024597168\n",
      "epoch:103\n",
      "train mse:0.0036678367760032415 rmse:0.06056266650557518 mae:0.037516284734010696 r2:0.9698705673217773\n",
      "mse:0.004696691874414682 rmse:0.06853241473436356 mae:0.04464591667056084 r2:0.961532711982727\n",
      "epoch:104\n",
      "train mse:0.0028128474950790405 rmse:0.05303628370165825 mae:0.03755656257271767 r2:0.976974368095398\n",
      "mse:0.004377319943159819 rmse:0.06616131961345673 mae:0.04282210022211075 r2:0.9641484618186951\n",
      "epoch:105\n",
      "train mse:0.0028925782535225153 rmse:0.05378269404172897 mae:0.03534076735377312 r2:0.9762536287307739\n",
      "mse:0.004290134180337191 rmse:0.06549911201000214 mae:0.04309200868010521 r2:0.964862585067749\n",
      "epoch:106\n",
      "train mse:0.0025733185466378927 rmse:0.05072788521647453 mae:0.03265790268778801 r2:0.9788798689842224\n",
      "mse:0.004080135375261307 rmse:0.06387593597173691 mae:0.04234011843800545 r2:0.9665825366973877\n",
      "epoch:107\n",
      "train mse:0.004347402602434158 rmse:0.06593483686447144 mae:0.04201202467083931 r2:0.9644193649291992\n",
      "mse:0.007494503166526556 rmse:0.08657079935073853 mae:0.055186085402965546 r2:0.9386178851127625\n",
      "epoch:108\n",
      "train mse:0.0029317208100110292 rmse:0.054145365953445435 mae:0.039871592074632645 r2:0.9760053157806396\n",
      "mse:0.003937246277928352 rmse:0.06274747848510742 mae:0.04165343567728996 r2:0.9677528142929077\n",
      "epoch:109\n",
      "train mse:0.0030625630170106888 rmse:0.05534042790532112 mae:0.03780020400881767 r2:0.9749614596366882\n",
      "mse:0.005319107323884964 rmse:0.07293220609426498 mae:0.04823077470064163 r2:0.956434965133667\n",
      "epoch:110\n",
      "train mse:0.0037806762848049402 rmse:0.06148720532655716 mae:0.041724834591150284 r2:0.9691029787063599\n",
      "mse:0.004403516184538603 rmse:0.06635899096727371 mae:0.04224260896444321 r2:0.9639339447021484\n",
      "epoch:111\n",
      "train mse:0.00506979413330555 rmse:0.07120248675346375 mae:0.0498519241809845 r2:0.9585528373718262\n",
      "mse:0.010526788420975208 rmse:0.10260013490915298 mae:0.06824450939893723 r2:0.9137825965881348\n",
      "epoch:112\n",
      "train mse:0.003591587534174323 rmse:0.05992985516786575 mae:0.04008049890398979 r2:0.9705378413200378\n",
      "mse:0.0042074983939528465 rmse:0.06486523151397705 mae:0.04191164672374725 r2:0.9655393958091736\n",
      "epoch:113\n",
      "train mse:0.006175442133098841 rmse:0.0785839781165123 mae:0.04941573366522789 r2:0.949181854724884\n",
      "mse:0.012203465215861797 rmse:0.11046929657459259 mae:0.07312186062335968 r2:0.9000501036643982\n",
      "epoch:114\n",
      "train mse:0.0038862095680087805 rmse:0.06233946979045868 mae:0.04041676223278046 r2:0.9681457281112671\n",
      "mse:0.005871538072824478 rmse:0.07662595808506012 mae:0.04782932624220848 r2:0.9519104361534119\n",
      "epoch:115\n",
      "train mse:0.0026831028517335653 rmse:0.05179867520928383 mae:0.03648709878325462 r2:0.9780969619750977\n",
      "mse:0.0036697883624583483 rmse:0.06057877838611603 mae:0.040782757103443146 r2:0.9699434041976929\n",
      "epoch:116\n",
      "train mse:0.003166493261232972 rmse:0.05627160146832466 mae:0.03899897262454033 r2:0.9741161465644836\n",
      "mse:0.004398454912006855 rmse:0.06632084399461746 mae:0.04609304666519165 r2:0.9639753699302673\n",
      "epoch:117\n",
      "train mse:0.0022676975931972265 rmse:0.04762034863233566 mae:0.033452071249485016 r2:0.9812946915626526\n",
      "mse:0.003927873447537422 rmse:0.06267274916172028 mae:0.0421781912446022 r2:0.9678295850753784\n",
      "epoch:118\n",
      "train mse:0.002488755388185382 rmse:0.04988742619752884 mae:0.03512856364250183 r2:0.9794898629188538\n",
      "mse:0.003422409761697054 rmse:0.058501362800598145 mae:0.03904738649725914 r2:0.971969485282898\n",
      "epoch:119\n",
      "train mse:0.003561534686014056 rmse:0.05967859551310539 mae:0.042387958616018295 r2:0.9708789587020874\n",
      "mse:0.005605462472885847 rmse:0.07486964017152786 mae:0.04801458492875099 r2:0.9540896415710449\n",
      "epoch:120\n",
      "train mse:0.004059928935021162 rmse:0.0637175664305687 mae:0.04240472614765167 r2:0.9667559266090393\n",
      "mse:0.009592109359800816 rmse:0.09793931245803833 mae:0.06594535708427429 r2:0.9214378595352173\n",
      "epoch:121\n",
      "train mse:0.003964239731431007 rmse:0.06296220421791077 mae:0.044953037053346634 r2:0.9675949811935425\n",
      "mse:0.006484943442046642 rmse:0.08052914589643478 mae:0.05387425422668457 r2:0.946886420249939\n",
      "epoch:122\n",
      "train mse:0.003180589759722352 rmse:0.056396715342998505 mae:0.041893333196640015 r2:0.9739990234375\n",
      "mse:0.004187269136309624 rmse:0.06470911204814911 mae:0.04494868591427803 r2:0.9657050371170044\n",
      "epoch:123\n",
      "train mse:0.0056438203901052475 rmse:0.07512535899877548 mae:0.050479788333177567 r2:0.9536479711532593\n",
      "mse:0.0083830077201128 rmse:0.09155876934528351 mae:0.06302794069051743 r2:0.931340754032135\n",
      "epoch:124\n",
      "train mse:0.00255165109410882 rmse:0.050513871014118195 mae:0.03541366010904312 r2:0.9789913892745972\n",
      "mse:0.004768160171806812 rmse:0.06905186176300049 mae:0.04734856262803078 r2:0.9609473943710327\n",
      "epoch:125\n",
      "train mse:0.003244578605517745 rmse:0.05696120113134384 mae:0.03791489452123642 r2:0.9734355211257935\n",
      "mse:0.003821036545559764 rmse:0.061814531683921814 mae:0.04101398587226868 r2:0.9687045812606812\n",
      "epoch:126\n",
      "train mse:0.00264664925634861 rmse:0.05144559592008591 mae:0.03567500039935112 r2:0.9781604409217834\n",
      "mse:0.00608464190736413 rmse:0.07800411432981491 mae:0.050428569316864014 r2:0.9501650333404541\n",
      "epoch:127\n",
      "train mse:0.004093059804290533 rmse:0.06397702544927597 mae:0.043042611330747604 r2:0.9665201306343079\n",
      "mse:0.006728433072566986 rmse:0.08202702552080154 mae:0.05630962550640106 r2:0.944892168045044\n",
      "epoch:128\n",
      "train mse:0.003032142296433449 rmse:0.055064890533685684 mae:0.035066377371549606 r2:0.9751325249671936\n",
      "mse:0.0034165573306381702 rmse:0.058451324701309204 mae:0.03845706954598427 r2:0.9720174074172974\n",
      "epoch:129\n",
      "train mse:0.002837156178429723 rmse:0.053264960646629333 mae:0.03600229695439339 r2:0.9768276214599609\n",
      "mse:0.003416102146729827 rmse:0.05844742804765701 mae:0.03854905441403389 r2:0.9720211625099182\n",
      "epoch:130\n",
      "train mse:0.002417720854282379 rmse:0.049170322716236115 mae:0.03539116680622101 r2:0.9802553653717041\n",
      "mse:0.0036341773811727762 rmse:0.06028413772583008 mae:0.04034636542201042 r2:0.9702350497245789\n",
      "epoch:131\n",
      "train mse:0.004007879178971052 rmse:0.06330781430006027 mae:0.04186316952109337 r2:0.9671803116798401\n",
      "mse:0.007118557579815388 rmse:0.08437154442071915 mae:0.05764615535736084 r2:0.9416969418525696\n",
      "epoch:132\n",
      "train mse:0.0031111526768654585 rmse:0.05577770620584488 mae:0.03474552556872368 r2:0.9745245575904846\n",
      "mse:0.0034288314636796713 rmse:0.05855622515082359 mae:0.039119549095630646 r2:0.9719168543815613\n",
      "epoch:133\n",
      "train mse:0.004693761933594942 rmse:0.06851103156805038 mae:0.046488016843795776 r2:0.9612626433372498\n",
      "mse:0.005241121631115675 rmse:0.07239559292793274 mae:0.04910653829574585 r2:0.9570736885070801\n",
      "epoch:134\n",
      "train mse:0.003658846253529191 rmse:0.06048839911818504 mae:0.04097038507461548 r2:0.9701116681098938\n",
      "mse:0.0050084288232028484 rmse:0.0707702487707138 mae:0.04746794328093529 r2:0.9589795470237732\n",
      "epoch:135\n",
      "train mse:0.004937638994306326 rmse:0.07026833295822144 mae:0.044649869203567505 r2:0.9594274163246155\n",
      "mse:0.013587823137640953 rmse:0.11656681448221207 mae:0.07507231831550598 r2:0.8887118101119995\n",
      "epoch:136\n",
      "train mse:0.0035038497298955917 rmse:0.05919332429766655 mae:0.03821789473295212 r2:0.9712585806846619\n",
      "mse:0.0040948642417788506 rmse:0.06399112194776535 mae:0.04418051615357399 r2:0.9664618968963623\n",
      "epoch:137\n",
      "train mse:0.0034913141280412674 rmse:0.05908734351396561 mae:0.038048047572374344 r2:0.971375048160553\n",
      "mse:0.005411397200077772 rmse:0.07356219738721848 mae:0.0485614575445652 r2:0.9556791186332703\n",
      "epoch:138\n",
      "train mse:0.002684199484065175 rmse:0.05180926248431206 mae:0.030666140839457512 r2:0.9780547022819519\n",
      "mse:0.0038457736372947693 rmse:0.062014300376176834 mae:0.04169134423136711 r2:0.9685019850730896\n",
      "epoch:139\n",
      "train mse:0.004180277232080698 rmse:0.06465506553649902 mae:0.0394989550113678 r2:0.9655249118804932\n",
      "mse:0.0033148834481835365 rmse:0.05757502466440201 mae:0.03784996271133423 r2:0.9728501439094543\n",
      "epoch:140\n",
      "train mse:0.004796263296157122 rmse:0.06925506144762039 mae:0.04355334863066673 r2:0.9606631994247437\n",
      "mse:0.012185017578303814 rmse:0.11038576811552048 mae:0.07341951876878738 r2:0.9002012014389038\n",
      "epoch:141\n",
      "train mse:0.0034138949122279882 rmse:0.05842854455113411 mae:0.036784544587135315 r2:0.9720318913459778\n",
      "mse:0.0034995591267943382 rmse:0.05915707349777222 mae:0.039818260818719864 r2:0.971337616443634\n",
      "epoch:142\n",
      "train mse:0.0023315800353884697 rmse:0.04828643798828125 mae:0.03540324792265892 r2:0.980928361415863\n",
      "mse:0.0038812155835330486 rmse:0.06229940056800842 mae:0.04075407609343529 r2:0.9682117104530334\n",
      "epoch:143\n",
      "train mse:0.002765027806162834 rmse:0.05258353054523468 mae:0.03377487137913704 r2:0.9773663878440857\n",
      "mse:0.003678596578538418 rmse:0.06065143644809723 mae:0.03918556123971939 r2:0.9698712229728699\n",
      "epoch:144\n",
      "train mse:0.0030367672443389893 rmse:0.05510687083005905 mae:0.036198098212480545 r2:0.9751543998718262\n",
      "mse:0.005182807333767414 rmse:0.07199171930551529 mae:0.047591473907232285 r2:0.9575513005256653\n",
      "epoch:145\n",
      "train mse:0.0024948075879365206 rmse:0.04994804784655571 mae:0.03230256214737892 r2:0.9794226884841919\n",
      "mse:0.0036845626309514046 rmse:0.06070059910416603 mae:0.03985496982932091 r2:0.969822347164154\n",
      "epoch:146\n",
      "train mse:0.0020839639473706484 rmse:0.04565045237541199 mae:0.031272560358047485 r2:0.9829788208007812\n",
      "mse:0.002686370862647891 rmse:0.05183020979166031 mae:0.03394452854990959 r2:0.9779978394508362\n",
      "epoch:147\n",
      "train mse:0.002339312806725502 rmse:0.04836644232273102 mae:0.03189139440655708 r2:0.9806790947914124\n",
      "mse:0.003902196418493986 rmse:0.06246756389737129 mae:0.03895844891667366 r2:0.968039870262146\n",
      "epoch:148\n",
      "train mse:0.002157103968784213 rmse:0.04644463211297989 mae:0.03196953609585762 r2:0.9823367595672607\n",
      "mse:0.0029798389878124 rmse:0.05458790063858032 mae:0.0354347825050354 r2:0.9755942821502686\n",
      "epoch:149\n",
      "train mse:0.0017285194480791688 rmse:0.041575465351343155 mae:0.029935471713542938 r2:0.9858690500259399\n",
      "mse:0.0035518405493348837 rmse:0.05959731712937355 mae:0.03895755112171173 r2:0.9709094166755676\n"
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    for X, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            tr_mse = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        tr_rmse = tf.sqrt(tr_mse)\n",
    "        tr_mae = tf.reduce_mean(tf.abs(y_pred - y))\n",
    "        tr_r2 = 1 - tf.reduce_sum(tf.square(y_pred - y)) / tf.reduce_sum(tf.square(y - tf.reduce_mean(y)))\n",
    "        grads = tape.gradient(tr_mse, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    print(\"epoch:{}\".format(i))\n",
    "    print(\"train mse:{} rmse:{} mae:{} r2:{}\".format(tr_mse, tr_rmse, tr_mae, tr_r2))\n",
    "    valiAll(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
