{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e712e2fa-92bb-499b-801e-950dfcda6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 18:29:40.488885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 18:29:40.959708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b51fc08-787d-42bb-bf42-65eacb5ce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "l_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a8dd19-5960-44cc-90c6-e2c42c86b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=128, activation=tf.nn.leaky_relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=8)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        output = self.dense5(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f65bda-5f20-4df4-b12e-e0fdf99c5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valiAll(index_epoch):\n",
    "    y_v_p = model(X_v)\n",
    "    va_mse = tf.reduce_mean(tf.square(y_v_p - y_v))\n",
    "    va_rmse = tf.sqrt(va_mse)\n",
    "    va_mae = tf.reduce_mean(tf.abs(y_v_p - y_v))\n",
    "    va_r2 = 1 - tf.reduce_sum(tf.square(y_v_p - y_v)) / tf.reduce_sum(tf.square(y_v - tf.reduce_mean(y_v)))\n",
    "    print(\"mse:{} rmse:{} mae:{} r2:{}\".format(va_mse, va_rmse, va_mae, va_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b543212c-3492-45eb-ad62-77930b2d4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"testset.csv\", encoding='utf-8').sample(frac=1).reset_index(drop=True)\n",
    "X_v = test_dataset.loc[:,'freq':'L2'].to_numpy(dtype = np.float32)\n",
    "y_v = test_dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eef8846-dcb9-4aca-bdb0-cfddcec2ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 18:29:42.813437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9604 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2023-09-18 18:29:42.813889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9621 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('./20-24Trainset.csv', encoding='utf-8')\n",
    "dataset2 = pd.read_csv('./50-54Trainset.csv', encoding='utf-8')\n",
    "dataset = pd.concat([dataset1, dataset2], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "# dataset = dataset[(dataset['freq'] >= 1.0) & (dataset['freq'] <= 2.6)]\n",
    "X = dataset.loc[:,'freq':'L2'].to_numpy(dtype = np.float32)\n",
    "y = dataset.loc[:,'S11r':'S41i'].to_numpy(dtype = np.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset_train = dataset_train.shuffle(buffer_size=X.shape[0])\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1327167a-9b84-45dd-a8c1-edf35b22e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624eb49b-e8c8-4d45-bc8b-b020e9398908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 18:29:43.842354: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562ad64b47a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-18 18:29:43.842380: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-18 18:29:43.842384: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-18 18:29:43.845775: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-18 18:29:43.949538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-18 18:29:44.058727: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f0181570dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f0181570dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch:0\n",
      "train mse:0.05951593443751335 rmse:0.24395887553691864 mae:0.19133226573467255 r2:0.512557864189148\n",
      "mse:0.060445789247751236 rmse:0.24585725367069244 mae:0.19305388629436493 r2:0.5049315690994263\n",
      "epoch:1\n",
      "train mse:0.05100033059716225 rmse:0.2258325219154358 mae:0.1750820428133011 r2:0.5823920965194702\n",
      "mse:0.05047406628727913 rmse:0.2246643453836441 mae:0.17570842802524567 r2:0.5866028070449829\n",
      "epoch:2\n",
      "train mse:0.047532662749290466 rmse:0.21801985800266266 mae:0.17075784504413605 r2:0.6105225086212158\n",
      "mse:0.0457192063331604 rmse:0.21382050216197968 mae:0.1662796437740326 r2:0.6255465745925903\n",
      "epoch:3\n",
      "train mse:0.041642460972070694 rmse:0.20406484603881836 mae:0.15785786509513855 r2:0.6589142084121704\n",
      "mse:0.041213568300008774 rmse:0.20301124453544617 mae:0.15768875181674957 r2:0.6624490022659302\n",
      "epoch:4\n",
      "train mse:0.03894885629415512 rmse:0.19735464453697205 mae:0.1525951623916626 r2:0.6812013387680054\n",
      "mse:0.03833235427737236 rmse:0.1957864910364151 mae:0.15069057047367096 r2:0.6860469579696655\n",
      "epoch:5\n",
      "train mse:0.034787826240062714 rmse:0.1865149438381195 mae:0.14418943226337433 r2:0.715416669845581\n",
      "mse:0.035667892545461655 rmse:0.18885944783687592 mae:0.14443472027778625 r2:0.7078696489334106\n",
      "epoch:6\n",
      "train mse:0.0327138788998127 rmse:0.18086978793144226 mae:0.13742129504680634 r2:0.7323630452156067\n",
      "mse:0.03392887860536575 rmse:0.18419793248176575 mae:0.14036749303340912 r2:0.7221127152442932\n",
      "epoch:7\n",
      "train mse:0.03788001090288162 rmse:0.19462788105010986 mae:0.14792755246162415 r2:0.6895318031311035\n",
      "mse:0.03379932418465614 rmse:0.18384592235088348 mae:0.14046533405780792 r2:0.7231737971305847\n",
      "epoch:8\n",
      "train mse:0.04416642710566521 rmse:0.2101580947637558 mae:0.16222074627876282 r2:0.6377422213554382\n",
      "mse:0.03459350764751434 rmse:0.18599329888820648 mae:0.14059413969516754 r2:0.7166692018508911\n",
      "epoch:9\n",
      "train mse:0.03592551499605179 rmse:0.18954026699066162 mae:0.14446090161800385 r2:0.7060015201568604\n",
      "mse:0.04206810146570206 rmse:0.20510509610176086 mae:0.15739575028419495 r2:0.6554501056671143\n",
      "epoch:10\n",
      "train mse:0.02727513760328293 rmse:0.16515186429023743 mae:0.12372876703739166 r2:0.7766226530075073\n",
      "mse:0.026933465152978897 rmse:0.16411417722702026 mae:0.12158903479576111 r2:0.7794071435928345\n",
      "epoch:11\n",
      "train mse:0.024271948263049126 rmse:0.15579457581043243 mae:0.11659454554319382 r2:0.8016201853752136\n",
      "mse:0.02980305813252926 rmse:0.17263561487197876 mae:0.1299288272857666 r2:0.7559043765068054\n",
      "epoch:12\n",
      "train mse:0.02477589249610901 rmse:0.1574036031961441 mae:0.11612336337566376 r2:0.7971429824829102\n",
      "mse:0.024733472615480423 rmse:0.15726879239082336 mae:0.11498962342739105 r2:0.7974257469177246\n",
      "epoch:13\n",
      "train mse:0.02383691444993019 rmse:0.1543920785188675 mae:0.11233477294445038 r2:0.8048872351646423\n",
      "mse:0.024822091683745384 rmse:0.15755029022693634 mae:0.11602194607257843 r2:0.7966998815536499\n",
      "epoch:14\n",
      "train mse:0.022584760561585426 rmse:0.15028226375579834 mae:0.10751914978027344 r2:0.8152652382850647\n",
      "mse:0.022502632811665535 rmse:0.15000876784324646 mae:0.10833396017551422 r2:0.8156969547271729\n",
      "epoch:15\n",
      "train mse:0.025530770421028137 rmse:0.1597835123538971 mae:0.11702374368906021 r2:0.790708065032959\n",
      "mse:0.02315494604408741 rmse:0.15216749906539917 mae:0.110530786216259 r2:0.8103542923927307\n",
      "epoch:16\n",
      "train mse:0.02181597799062729 rmse:0.14770232141017914 mae:0.10667947679758072 r2:0.8210522532463074\n",
      "mse:0.024049047380685806 rmse:0.15507754683494568 mae:0.11381811648607254 r2:0.8030313849449158\n",
      "epoch:17\n",
      "train mse:0.022693684324622154 rmse:0.1506442278623581 mae:0.10956504940986633 r2:0.8138819336891174\n",
      "mse:0.025784701108932495 rmse:0.16057614982128143 mae:0.11937817186117172 r2:0.7888158559799194\n",
      "epoch:18\n",
      "train mse:0.019885946065187454 rmse:0.14101754128932953 mae:0.10191534459590912 r2:0.8370454907417297\n",
      "mse:0.01958209089934826 rmse:0.13993601500988007 mae:0.09935005754232407 r2:0.8396170139312744\n",
      "epoch:19\n",
      "train mse:0.01841551437973976 rmse:0.13570377230644226 mae:0.09736114740371704 r2:0.8492485284805298\n",
      "mse:0.01908700168132782 rmse:0.1381557136774063 mae:0.09860646724700928 r2:0.843671977519989\n",
      "epoch:20\n",
      "train mse:0.01867070235311985 rmse:0.1366407722234726 mae:0.09888827800750732 r2:0.8469467759132385\n",
      "mse:0.01743170991539955 rmse:0.1320292055606842 mae:0.09292171895503998 r2:0.8572292327880859\n",
      "epoch:21\n",
      "train mse:0.01692814566195011 rmse:0.13010820746421814 mae:0.09116994589567184 r2:0.8611451983451843\n",
      "mse:0.019993318244814873 rmse:0.14139772951602936 mae:0.10283117741346359 r2:0.8362489938735962\n",
      "epoch:22\n",
      "train mse:0.015476991422474384 rmse:0.12440655380487442 mae:0.08661441504955292 r2:0.8733628392219543\n",
      "mse:0.017480725422501564 rmse:0.13221469521522522 mae:0.09290430694818497 r2:0.8568278551101685\n",
      "epoch:23\n",
      "train mse:0.02118367701768875 rmse:0.14554613828659058 mae:0.105622299015522 r2:0.8264448642730713\n",
      "mse:0.016604995355010033 rmse:0.12886036932468414 mae:0.09032367914915085 r2:0.8640003204345703\n",
      "epoch:24\n",
      "train mse:0.01720535010099411 rmse:0.1311691701412201 mae:0.09382545202970505 r2:0.8591430187225342\n",
      "mse:0.01613384485244751 rmse:0.12701906263828278 mae:0.09019359201192856 r2:0.8678591251373291\n",
      "epoch:25\n",
      "train mse:0.01268814131617546 rmse:0.11264164745807648 mae:0.07753006368875504 r2:0.8961668014526367\n",
      "mse:0.014132632873952389 rmse:0.1188807487487793 mae:0.08144034445285797 r2:0.8842496871948242\n",
      "epoch:26\n",
      "train mse:0.015640152618288994 rmse:0.12506058812141418 mae:0.08742058277130127 r2:0.8717809915542603\n",
      "mse:0.016715995967388153 rmse:0.1292903572320938 mae:0.09216180443763733 r2:0.8630911707878113\n",
      "epoch:27\n",
      "train mse:0.015416628681123257 rmse:0.12416371703147888 mae:0.08473634719848633 r2:0.8737118244171143\n",
      "mse:0.014665171504020691 rmse:0.12109983712434769 mae:0.08601608127355576 r2:0.8798879981040955\n",
      "epoch:28\n",
      "train mse:0.011968628503382206 rmse:0.10940122604370117 mae:0.0738493800163269 r2:0.9020074009895325\n",
      "mse:0.01232266053557396 rmse:0.11100747436285019 mae:0.07625656574964523 r2:0.8990738391876221\n",
      "epoch:29\n",
      "train mse:0.015161871910095215 rmse:0.12313355505466461 mae:0.08790931850671768 r2:0.8760504722595215\n",
      "mse:0.014797689393162727 rmse:0.12164574861526489 mae:0.08727964013814926 r2:0.8788026571273804\n",
      "epoch:30\n",
      "train mse:0.012843620963394642 rmse:0.1133297011256218 mae:0.08056390285491943 r2:0.8948579430580139\n",
      "mse:0.012913794256746769 rmse:0.11363887786865234 mae:0.08086051046848297 r2:0.8942322731018066\n",
      "epoch:31\n",
      "train mse:0.010354110039770603 rmse:0.10175514966249466 mae:0.06900621950626373 r2:0.9152669906616211\n",
      "mse:0.01081673614680767 rmse:0.10400354117155075 mae:0.06992567330598831 r2:0.9114078283309937\n",
      "epoch:32\n",
      "train mse:0.012715335935354233 rmse:0.11276229470968246 mae:0.07850868999958038 r2:0.8958383202552795\n",
      "mse:0.011439631693065166 rmse:0.10695621371269226 mae:0.07340750098228455 r2:0.9063061475753784\n",
      "epoch:33\n",
      "train mse:0.016736110672354698 rmse:0.1293681114912033 mae:0.09300993382930756 r2:0.8629510998725891\n",
      "mse:0.01444598101079464 rmse:0.12019143253564835 mae:0.08338987827301025 r2:0.8816832304000854\n",
      "epoch:34\n",
      "train mse:0.009455607272684574 rmse:0.09723994880914688 mae:0.06601766496896744 r2:0.9225614070892334\n",
      "mse:0.009868566878139973 rmse:0.09934066236019135 mae:0.06750678271055222 r2:0.9191735982894897\n",
      "epoch:35\n",
      "train mse:0.009289426729083061 rmse:0.09638167172670364 mae:0.06455518305301666 r2:0.9239058494567871\n",
      "mse:0.009133436717092991 rmse:0.09556901454925537 mae:0.06409522145986557 r2:0.925194501876831\n",
      "epoch:36\n",
      "train mse:0.010398393496870995 rmse:0.10197251290082932 mae:0.07152460515499115 r2:0.9147642850875854\n",
      "mse:0.012517762370407581 rmse:0.11188280582427979 mae:0.07806376367807388 r2:0.8974758982658386\n",
      "epoch:37\n",
      "train mse:0.009645096026360989 rmse:0.098209448158741 mae:0.06844836473464966 r2:0.9209561347961426\n",
      "mse:0.011079289950430393 rmse:0.10525820404291153 mae:0.07474172115325928 r2:0.9092574119567871\n",
      "epoch:38\n",
      "train mse:0.008619537577033043 rmse:0.09284146130084991 mae:0.0630498081445694 r2:0.9294368028640747\n",
      "mse:0.008536799810826778 rmse:0.09239480644464493 mae:0.06165507808327675 r2:0.9300811290740967\n",
      "epoch:39\n",
      "train mse:0.00942338164895773 rmse:0.09707409888505936 mae:0.06729859858751297 r2:0.92282634973526\n",
      "mse:0.008790886960923672 rmse:0.09375973045825958 mae:0.06392934173345566 r2:0.9280000925064087\n",
      "epoch:40\n",
      "train mse:0.008778613060712814 rmse:0.09369425475597382 mae:0.06621525436639786 r2:0.9280220866203308\n",
      "mse:0.009802698157727718 rmse:0.09900857508182526 mae:0.06844377517700195 r2:0.9197130799293518\n",
      "epoch:41\n",
      "train mse:0.008318453095853329 rmse:0.09120555222034454 mae:0.0626322478055954 r2:0.9318679571151733\n",
      "mse:0.011273402720689774 rmse:0.10617627948522568 mae:0.0725308358669281 r2:0.9076675772666931\n",
      "epoch:42\n",
      "train mse:0.012456005439162254 rmse:0.11160647124052048 mae:0.07952943444252014 r2:0.8980410695075989\n",
      "mse:0.00958982389420271 rmse:0.09792764484882355 mae:0.06696391105651855 r2:0.9214565753936768\n",
      "epoch:43\n",
      "train mse:0.008361637592315674 rmse:0.09144198894500732 mae:0.06266776472330093 r2:0.9314202666282654\n",
      "mse:0.008344308473169804 rmse:0.09134718775749207 mae:0.06265556067228317 r2:0.9316577315330505\n",
      "epoch:44\n",
      "train mse:0.00795005913823843 rmse:0.08916310220956802 mae:0.05945948138833046 r2:0.9348479509353638\n",
      "mse:0.008117029443383217 rmse:0.09009455889463425 mae:0.06157396361231804 r2:0.933519184589386\n",
      "epoch:45\n",
      "train mse:0.008843978866934776 rmse:0.09404242783784866 mae:0.0632576122879982 r2:0.9275290966033936\n",
      "mse:0.010176751762628555 rmse:0.10087988525629044 mae:0.07103965431451797 r2:0.9166494607925415\n",
      "epoch:46\n",
      "train mse:0.008174377493560314 rmse:0.09041226655244827 mae:0.06211766600608826 r2:0.9329848885536194\n",
      "mse:0.009152859449386597 rmse:0.09567058086395264 mae:0.06535053998231888 r2:0.9250354170799255\n",
      "epoch:47\n",
      "train mse:0.00823576282709837 rmse:0.09075110405683517 mae:0.06237397715449333 r2:0.9325900673866272\n",
      "mse:0.008066566661000252 rmse:0.08981406688690186 mae:0.061053819954395294 r2:0.9339324831962585\n",
      "epoch:48\n",
      "train mse:0.007053141947835684 rmse:0.08398298174142838 mae:0.05573013424873352 r2:0.9422841668128967\n",
      "mse:0.00785121787339449 rmse:0.08860709518194199 mae:0.06104670464992523 r2:0.9356962442398071\n",
      "epoch:49\n",
      "train mse:0.009075770154595375 rmse:0.09526683390140533 mae:0.06616151332855225 r2:0.9256129860877991\n",
      "mse:0.010673338547348976 rmse:0.10331185162067413 mae:0.07156245410442352 r2:0.912582278251648\n",
      "epoch:50\n",
      "train mse:0.008624229580163956 rmse:0.09286673367023468 mae:0.06365194171667099 r2:0.9292650818824768\n",
      "mse:0.009158612228929996 rmse:0.09570063650608063 mae:0.06758932024240494 r2:0.924988329410553\n",
      "epoch:51\n",
      "train mse:0.007510276511311531 rmse:0.08666185289621353 mae:0.05937674269080162 r2:0.9383484125137329\n",
      "mse:0.0069301254115998745 rmse:0.08324737101793289 mae:0.055029455572366714 r2:0.9432402849197388\n",
      "epoch:52\n",
      "train mse:0.01024628710001707 rmse:0.10122394561767578 mae:0.07068774104118347 r2:0.9161185622215271\n",
      "mse:0.008581499569118023 rmse:0.09263638406991959 mae:0.06507005542516708 r2:0.9297150373458862\n",
      "epoch:53\n",
      "train mse:0.0081002376973629 rmse:0.09000132232904434 mae:0.06236893683671951 r2:0.9336768388748169\n",
      "mse:0.00832222681492567 rmse:0.0912262350320816 mae:0.06273715943098068 r2:0.9318385720252991\n",
      "epoch:54\n",
      "train mse:0.011224375106394291 rmse:0.1059451475739479 mae:0.07517262548208237 r2:0.9079194068908691\n",
      "mse:0.011253481730818748 rmse:0.10608242452144623 mae:0.07457863539457321 r2:0.9078307151794434\n",
      "epoch:55\n",
      "train mse:0.009512889198958874 rmse:0.09753403812646866 mae:0.06741974502801895 r2:0.9221233129501343\n",
      "mse:0.008950711227953434 rmse:0.09460819512605667 mae:0.06581490486860275 r2:0.9266911149024963\n",
      "epoch:56\n",
      "train mse:0.006697452161461115 rmse:0.081837959587574 mae:0.054052699357271194 r2:0.9451594948768616\n",
      "mse:0.006888790987432003 rmse:0.08299873769283295 mae:0.05563114583492279 r2:0.943578839302063\n",
      "epoch:57\n",
      "train mse:0.0070127337239682674 rmse:0.08374206721782684 mae:0.05723758414387703 r2:0.9425413608551025\n",
      "mse:0.006634559016674757 rmse:0.08145280182361603 mae:0.05491429567337036 r2:0.9456610679626465\n",
      "epoch:58\n",
      "train mse:0.007146187126636505 rmse:0.08453512191772461 mae:0.057280607521533966 r2:0.9414361119270325\n",
      "mse:0.007234519813209772 rmse:0.08505597710609436 mae:0.05644597113132477 r2:0.9407472014427185\n",
      "epoch:59\n",
      "train mse:0.0072110965847969055 rmse:0.08491817116737366 mae:0.05811339616775513 r2:0.9409855008125305\n",
      "mse:0.007910076528787613 rmse:0.08893860876560211 mae:0.058679644018411636 r2:0.9352141618728638\n",
      "epoch:60\n",
      "train mse:0.006819296162575483 rmse:0.08257903158664703 mae:0.05753583833575249 r2:0.9440652132034302\n",
      "mse:0.006594877690076828 rmse:0.08120884746313095 mae:0.054665978997945786 r2:0.9459860324859619\n",
      "epoch:61\n",
      "train mse:0.01161271333694458 rmse:0.10776229947805405 mae:0.07722397893667221 r2:0.9049111008644104\n",
      "mse:0.009254141710698605 rmse:0.09619844704866409 mae:0.06532809138298035 r2:0.9242058992385864\n",
      "epoch:62\n",
      "train mse:0.0062050363048911095 rmse:0.07877205312252045 mae:0.051782917231321335 r2:0.9491589665412903\n",
      "mse:0.007041440811008215 rmse:0.08391328901052475 mae:0.05923904478549957 r2:0.9423285722732544\n",
      "epoch:63\n",
      "train mse:0.006660042330622673 rmse:0.0816090777516365 mae:0.05697723850607872 r2:0.9454311728477478\n",
      "mse:0.006671978160738945 rmse:0.08168217539787292 mae:0.054996564984321594 r2:0.9453545808792114\n",
      "epoch:64\n",
      "train mse:0.00933233741670847 rmse:0.09660402685403824 mae:0.06873085349798203 r2:0.9235985279083252\n",
      "mse:0.012213138863444328 rmse:0.11051306873559952 mae:0.07832184433937073 r2:0.8999708890914917\n",
      "epoch:65\n",
      "train mse:0.006751117762178183 rmse:0.0821651816368103 mae:0.0543283112347126 r2:0.9446938633918762\n",
      "mse:0.0070823621936142445 rmse:0.08415676653385162 mae:0.05767166614532471 r2:0.9419934153556824\n",
      "epoch:66\n",
      "train mse:0.0063099064864218235 rmse:0.07943491637706757 mae:0.055696338415145874 r2:0.9483143091201782\n",
      "mse:0.005802396219223738 rmse:0.07617346197366714 mae:0.051013700664043427 r2:0.9524766802787781\n",
      "epoch:67\n",
      "train mse:0.008945103734731674 rmse:0.09457855671644211 mae:0.06465326994657516 r2:0.9266963601112366\n",
      "mse:0.009190441109240055 rmse:0.09586678445339203 mae:0.06755615025758743 r2:0.9247276186943054\n",
      "epoch:68\n",
      "train mse:0.006312593352049589 rmse:0.07945182919502258 mae:0.05284319072961807 r2:0.9482563138008118\n",
      "mse:0.0070197489112615585 rmse:0.08378393948078156 mae:0.056901879608631134 r2:0.9425062537193298\n",
      "epoch:69\n",
      "train mse:0.008676588535308838 rmse:0.09314820170402527 mae:0.06491636484861374 r2:0.9289562106132507\n",
      "mse:0.0073213423602283 rmse:0.08556484431028366 mae:0.058694127947092056 r2:0.9400361180305481\n",
      "epoch:70\n",
      "train mse:0.004520928952842951 rmse:0.06723785400390625 mae:0.04509115219116211 r2:0.9629620909690857\n",
      "mse:0.005207256879657507 rmse:0.07216132432222366 mae:0.0478040985763073 r2:0.9573510885238647\n",
      "epoch:71\n",
      "train mse:0.006084218621253967 rmse:0.07800140231847763 mae:0.0525803305208683 r2:0.9501495361328125\n",
      "mse:0.0060351393185555935 rmse:0.07768616080284119 mae:0.05156124755740166 r2:0.9505704641342163\n",
      "epoch:72\n",
      "train mse:0.006453224457800388 rmse:0.0803319662809372 mae:0.05350898578763008 r2:0.947144091129303\n",
      "mse:0.006479221396148205 rmse:0.08049360662698746 mae:0.05461740866303444 r2:0.9469333291053772\n",
      "epoch:73\n",
      "train mse:0.005722466390579939 rmse:0.07564698159694672 mae:0.05153065547347069 r2:0.9531416296958923\n",
      "mse:0.005366836674511433 rmse:0.07325869798660278 mae:0.04892631620168686 r2:0.95604407787323\n",
      "epoch:74\n",
      "train mse:0.006661090534180403 rmse:0.08161550015211105 mae:0.054177265614271164 r2:0.9454612135887146\n",
      "mse:0.006427523214370012 rmse:0.08017183095216751 mae:0.05513598024845123 r2:0.9473567605018616\n",
      "epoch:75\n",
      "train mse:0.004469116218388081 rmse:0.06685144454240799 mae:0.04432731866836548 r2:0.9634361863136292\n",
      "mse:0.005217382218688726 rmse:0.07223144918680191 mae:0.04714085906744003 r2:0.9572681188583374\n",
      "epoch:76\n",
      "train mse:0.0065420144237577915 rmse:0.0808827206492424 mae:0.05605914816260338 r2:0.9464728832244873\n",
      "mse:0.0077398088760674 rmse:0.08797618001699448 mae:0.060880351811647415 r2:0.9366087317466736\n",
      "epoch:77\n",
      "train mse:0.005096594803035259 rmse:0.07139043509960175 mae:0.046450115740299225 r2:0.9581665992736816\n",
      "mse:0.0052644857205450535 rmse:0.07255677133798599 mae:0.04813878610730171 r2:0.9568823575973511\n",
      "epoch:78\n",
      "train mse:0.0076931435614824295 rmse:0.08771056681871414 mae:0.059840425848960876 r2:0.9369614124298096\n",
      "mse:0.010777747258543968 rmse:0.10381592810153961 mae:0.07005102932453156 r2:0.9117271304130554\n",
      "epoch:79\n",
      "train mse:0.005330181680619717 rmse:0.07300809025764465 mae:0.048101186752319336 r2:0.9563284516334534\n",
      "mse:0.00486616650596261 rmse:0.06975791603326797 mae:0.04577191174030304 r2:0.9601446986198425\n",
      "epoch:80\n",
      "train mse:0.004950813949108124 rmse:0.07036201655864716 mae:0.046157337725162506 r2:0.9594108462333679\n",
      "mse:0.005228762049227953 rmse:0.0723101794719696 mae:0.04795365035533905 r2:0.9571749567985535\n",
      "epoch:81\n",
      "train mse:0.005566732957959175 rmse:0.07461053878068924 mae:0.05106550455093384 r2:0.9543536305427551\n",
      "mse:0.006680936552584171 rmse:0.08173699676990509 mae:0.05630521848797798 r2:0.9452812075614929\n",
      "epoch:82\n",
      "train mse:0.006204480770975351 rmse:0.0787685289978981 mae:0.055039215832948685 r2:0.9491201639175415\n",
      "mse:0.005851016379892826 rmse:0.07649193704128265 mae:0.051408253610134125 r2:0.9520784616470337\n",
      "epoch:83\n",
      "train mse:0.005509165581315756 rmse:0.07422374933958054 mae:0.05075307562947273 r2:0.9549117088317871\n",
      "mse:0.005201778374612331 rmse:0.07212335616350174 mae:0.048454754054546356 r2:0.9573959112167358\n",
      "epoch:84\n",
      "train mse:0.004748782142996788 rmse:0.06891140341758728 mae:0.044526100158691406 r2:0.9611312747001648\n",
      "mse:0.005205291789025068 rmse:0.07214770466089249 mae:0.04804542288184166 r2:0.9573671817779541\n",
      "epoch:85\n",
      "train mse:0.004641619976609945 rmse:0.06812943518161774 mae:0.04454198107123375 r2:0.961957573890686\n",
      "mse:0.004769609775394201 rmse:0.06906235963106155 mae:0.045826349407434464 r2:0.9609355330467224\n",
      "epoch:86\n",
      "train mse:0.0064790574833750725 rmse:0.08049259334802628 mae:0.056265946477651596 r2:0.9468902945518494\n",
      "mse:0.0062105972319841385 rmse:0.07880734652280807 mae:0.05347498878836632 r2:0.9491333961486816\n",
      "epoch:87\n",
      "train mse:0.004755640868097544 rmse:0.06896115094423294 mae:0.045922521501779556 r2:0.9610604047775269\n",
      "mse:0.0048105367459356785 rmse:0.06935802847146988 mae:0.045862775295972824 r2:0.9606003165245056\n",
      "epoch:88\n",
      "train mse:0.004452920984476805 rmse:0.06673020869493484 mae:0.04428550973534584 r2:0.9635189771652222\n",
      "mse:0.005337097216397524 rmse:0.0730554386973381 mae:0.04917832463979721 r2:0.9562876224517822\n",
      "epoch:89\n",
      "train mse:0.0051599168218672276 rmse:0.0718325600028038 mae:0.04942706599831581 r2:0.9577846527099609\n",
      "mse:0.005317638628184795 rmse:0.07292214035987854 mae:0.0490804985165596 r2:0.9564470052719116\n",
      "epoch:90\n",
      "train mse:0.004007752984762192 rmse:0.06330681592226028 mae:0.040274374186992645 r2:0.9671759009361267\n",
      "mse:0.004263752605766058 rmse:0.06529741734266281 mae:0.04195510596036911 r2:0.9650786519050598\n",
      "epoch:91\n",
      "train mse:0.0047996630892157555 rmse:0.06927959620952606 mae:0.04655776545405388 r2:0.960766077041626\n",
      "mse:0.004796484485268593 rmse:0.06925665587186813 mae:0.04564353823661804 r2:0.9607154130935669\n",
      "epoch:92\n",
      "train mse:0.006028329022228718 rmse:0.07764231413602829 mae:0.05263426527380943 r2:0.9506195783615112\n",
      "mse:0.005946487188339233 rmse:0.07711347192525864 mae:0.051053401082754135 r2:0.9512965679168701\n",
      "epoch:93\n",
      "train mse:0.005142875015735626 rmse:0.07171384245157242 mae:0.04925978556275368 r2:0.9578601717948914\n",
      "mse:0.005131986923515797 rmse:0.07163788378238678 mae:0.04864511266350746 r2:0.9579675197601318\n",
      "epoch:94\n",
      "train mse:0.004969903267920017 rmse:0.0704975426197052 mae:0.04705323278903961 r2:0.959288477897644\n",
      "mse:0.004701548721641302 rmse:0.06856783479452133 mae:0.04519706591963768 r2:0.9614929556846619\n",
      "epoch:95\n",
      "train mse:0.004325008485466242 rmse:0.06576479971408844 mae:0.04353529214859009 r2:0.9645967483520508\n",
      "mse:0.004381832666695118 rmse:0.06619541347026825 mae:0.04376760497689247 r2:0.9641115069389343\n",
      "epoch:96\n",
      "train mse:0.004934005904942751 rmse:0.07024247944355011 mae:0.04646098613739014 r2:0.9595842361450195\n",
      "mse:0.005437623709440231 rmse:0.07374024391174316 mae:0.05012263357639313 r2:0.9554643034934998\n",
      "epoch:97\n",
      "train mse:0.004486667457967997 rmse:0.06698258966207504 mae:0.045328881591558456 r2:0.9632629156112671\n",
      "mse:0.004543105140328407 rmse:0.06740255653858185 mae:0.04442404955625534 r2:0.9627906680107117\n",
      "epoch:98\n",
      "train mse:0.004796909634023905 rmse:0.06925972551107407 mae:0.0478547178208828 r2:0.9607184529304504\n",
      "mse:0.005410154815763235 rmse:0.07355375587940216 mae:0.04838642105460167 r2:0.9556892514228821\n",
      "epoch:99\n",
      "train mse:0.007896571420133114 rmse:0.08886265754699707 mae:0.05956810712814331 r2:0.935185968875885\n",
      "mse:0.007649601902812719 rmse:0.08746200054883957 mae:0.05941111966967583 r2:0.9373475313186646\n",
      "epoch:100\n",
      "train mse:0.0046940459869802 rmse:0.06851311028003693 mae:0.045594193041324615 r2:0.9615045189857483\n",
      "mse:0.004331561271101236 rmse:0.06581459939479828 mae:0.04371490329504013 r2:0.9645232558250427\n",
      "epoch:101\n",
      "train mse:0.004582302179187536 rmse:0.06769270449876785 mae:0.04617254063487053 r2:0.9624524116516113\n",
      "mse:0.004476673435419798 rmse:0.06690794229507446 mae:0.04401729255914688 r2:0.9633347392082214\n",
      "epoch:102\n",
      "train mse:0.007623595651239157 rmse:0.0873132050037384 mae:0.06100236251950264 r2:0.9374909400939941\n",
      "mse:0.0075807031244039536 rmse:0.08706723153591156 mae:0.061945799738168716 r2:0.937911868095398\n",
      "epoch:103\n",
      "train mse:0.003672631224617362 rmse:0.06060223653912544 mae:0.03843778744339943 r2:0.9698927998542786\n",
      "mse:0.003800018224865198 rmse:0.061644285917282104 mae:0.039545852690935135 r2:0.9688767790794373\n",
      "epoch:104\n",
      "train mse:0.004631305579096079 rmse:0.06805370002985 mae:0.045544106513261795 r2:0.9620991945266724\n",
      "mse:0.004480433650314808 rmse:0.06693603843450546 mae:0.04382713511586189 r2:0.9633039832115173\n",
      "epoch:105\n",
      "train mse:0.003667166456580162 rmse:0.060557134449481964 mae:0.03941653296351433 r2:0.9699816107749939\n",
      "mse:0.004031014628708363 rmse:0.06349027156829834 mae:0.04129360616207123 r2:0.9669848084449768\n",
      "epoch:106\n",
      "train mse:0.004463042598217726 rmse:0.06680600345134735 mae:0.043369606137275696 r2:0.9633631110191345\n",
      "mse:0.004615746904164553 rmse:0.06793928891420364 mae:0.045026276260614395 r2:0.9621956944465637\n",
      "epoch:107\n",
      "train mse:0.004350198898464441 rmse:0.06595603376626968 mae:0.04285242035984993 r2:0.9644020795822144\n",
      "mse:0.004820290021598339 rmse:0.06942830979824066 mae:0.04544137418270111 r2:0.9605204463005066\n",
      "epoch:108\n",
      "train mse:0.003799174912273884 rmse:0.06163744628429413 mae:0.041541650891304016 r2:0.9689432382583618\n",
      "mse:0.004280955996364355 rmse:0.06542900949716568 mae:0.043134819716215134 r2:0.9649377465248108\n",
      "epoch:109\n",
      "train mse:0.004885365720838308 rmse:0.06989538669586182 mae:0.04722552001476288 r2:0.9599548578262329\n",
      "mse:0.006103942170739174 rmse:0.07812772691249847 mae:0.05366428196430206 r2:0.9500069618225098\n",
      "epoch:110\n",
      "train mse:0.0042511955834925175 rmse:0.06520119309425354 mae:0.0431971400976181 r2:0.965202271938324\n",
      "mse:0.004491082858294249 rmse:0.06701554358005524 mae:0.04391884058713913 r2:0.9632167220115662\n",
      "epoch:111\n",
      "train mse:0.0040372093208134174 rmse:0.0635390356183052 mae:0.042305707931518555 r2:0.9669371247291565\n",
      "mse:0.004438567906618118 rmse:0.06662257760763168 mae:0.04421091079711914 r2:0.9636468291282654\n",
      "epoch:112\n",
      "train mse:0.003819873556494713 rmse:0.0618051253259182 mae:0.04120304808020592 r2:0.9687069654464722\n",
      "mse:0.004092299845069647 rmse:0.06397108733654022 mae:0.04320930317044258 r2:0.9664828777313232\n",
      "epoch:113\n",
      "train mse:0.003913155756890774 rmse:0.0625552162528038 mae:0.04166550561785698 r2:0.9679415225982666\n",
      "mse:0.004848543554544449 rmse:0.06963147968053818 mae:0.0461001880466938 r2:0.9602890014648438\n",
      "epoch:114\n",
      "train mse:0.00520740682259202 rmse:0.07216235995292664 mae:0.04779547452926636 r2:0.9573386311531067\n",
      "mse:0.004838554188609123 rmse:0.06955971568822861 mae:0.04653644189238548 r2:0.9603708386421204\n",
      "epoch:115\n",
      "train mse:0.0043183728121221066 rmse:0.06571432948112488 mae:0.0432213619351387 r2:0.9646367430686951\n",
      "mse:0.004711948335170746 rmse:0.06864362955093384 mae:0.04668261855840683 r2:0.9614077806472778\n",
      "epoch:116\n",
      "train mse:0.004212079104036093 rmse:0.06490053236484528 mae:0.041637104004621506 r2:0.9654517769813538\n",
      "mse:0.003979773726314306 rmse:0.06308544427156448 mae:0.04194324463605881 r2:0.9674044847488403\n",
      "epoch:117\n",
      "train mse:0.0035172917414456606 rmse:0.059306759387254715 mae:0.03930089250206947 r2:0.971181333065033\n",
      "mse:0.0036896392703056335 rmse:0.06074240058660507 mae:0.03947946056723595 r2:0.9697808027267456\n",
      "epoch:118\n",
      "train mse:0.004700557328760624 rmse:0.06856060773134232 mae:0.04373140260577202 r2:0.9614818096160889\n",
      "mse:0.004915738943964243 rmse:0.07011233270168304 mae:0.04721752554178238 r2:0.9597386717796326\n",
      "epoch:119\n",
      "train mse:0.003414156846702099 rmse:0.058430787175893784 mae:0.03847101703286171 r2:0.9720283150672913\n",
      "mse:0.004069114103913307 rmse:0.06378960609436035 mae:0.04311753064393997 r2:0.9666727781295776\n",
      "epoch:120\n",
      "train mse:0.0038129400927573442 rmse:0.06174900755286217 mae:0.04020538181066513 r2:0.9687972068786621\n",
      "mse:0.00432410417124629 rmse:0.06575792282819748 mae:0.04383768513798714 r2:0.9645843505859375\n",
      "epoch:121\n",
      "train mse:0.00340650905855 rmse:0.05836530774831772 mae:0.038061659783124924 r2:0.9720968008041382\n",
      "mse:0.0038250735960900784 rmse:0.061847180128097534 mae:0.04040757194161415 r2:0.9686715602874756\n",
      "epoch:122\n",
      "train mse:0.004439516458660364 rmse:0.06662969291210175 mae:0.04398692399263382 r2:0.9636400938034058\n",
      "mse:0.006377399433404207 rmse:0.07985862344503403 mae:0.05451355129480362 r2:0.9477672576904297\n",
      "epoch:123\n",
      "train mse:0.004431306850165129 rmse:0.06656806170940399 mae:0.04428105428814888 r2:0.9637287259101868\n",
      "mse:0.004959709011018276 rmse:0.07042519748210907 mae:0.0467604324221611 r2:0.9593785405158997\n",
      "epoch:124\n",
      "train mse:0.003228032262995839 rmse:0.056815776973962784 mae:0.036779146641492844 r2:0.9735498428344727\n",
      "mse:0.003922825679183006 rmse:0.06263246387243271 mae:0.04118833690881729 r2:0.9678709506988525\n",
      "epoch:125\n",
      "train mse:0.003462227527052164 rmse:0.05884069576859474 mae:0.038013167679309845 r2:0.9716543555259705\n",
      "mse:0.0037363930605351925 rmse:0.061126042157411575 mae:0.04000236093997955 r2:0.9693978428840637\n",
      "epoch:126\n",
      "train mse:0.0037632405292242765 rmse:0.06134525686502457 mae:0.0405096672475338 r2:0.9691240787506104\n",
      "mse:0.0036064982414245605 rmse:0.060054127126932144 mae:0.0397602841258049 r2:0.9704617261886597\n",
      "epoch:127\n",
      "train mse:0.004311798140406609 rmse:0.06566428393125534 mae:0.0432109571993351 r2:0.9646634459495544\n",
      "mse:0.004434410948306322 rmse:0.06659137457609177 mae:0.04488116502761841 r2:0.9636809229850769\n",
      "epoch:128\n",
      "train mse:0.008694414980709553 rmse:0.09324384480714798 mae:0.06518448144197464 r2:0.9287330508232117\n",
      "mse:0.0075241075828671455 rmse:0.0867416113615036 mae:0.060811847448349 r2:0.9383754134178162\n",
      "epoch:129\n",
      "train mse:0.0039596580900251865 rmse:0.06292581558227539 mae:0.041762448847293854 r2:0.9675058126449585\n",
      "mse:0.004652338568121195 rmse:0.06820805370807648 mae:0.04626379907131195 r2:0.9618960022926331\n",
      "epoch:130\n",
      "train mse:0.0036445059813559055 rmse:0.060369741171598434 mae:0.04043039679527283 r2:0.9701066613197327\n",
      "mse:0.0038409617263823748 rmse:0.061975494027137756 mae:0.04061193764209747 r2:0.9685414433479309\n",
      "epoch:131\n",
      "train mse:0.004868534859269857 rmse:0.06977488845586777 mae:0.04844735562801361 r2:0.9601742625236511\n",
      "mse:0.004469309002161026 rmse:0.06685288995504379 mae:0.04506104812026024 r2:0.9633950591087341\n",
      "epoch:132\n",
      "train mse:0.0034634810872375965 rmse:0.058851346373558044 mae:0.038525015115737915 r2:0.9716088771820068\n",
      "mse:0.0036752810701727867 rmse:0.06062409654259682 mae:0.04016667231917381 r2:0.9698984026908875\n",
      "epoch:133\n",
      "train mse:0.004438250325620174 rmse:0.06662019342184067 mae:0.04535560682415962 r2:0.963687002658844\n",
      "mse:0.004675672855228186 rmse:0.06837888807058334 mae:0.04735714569687843 r2:0.9617049098014832\n",
      "epoch:134\n",
      "train mse:0.0033507368061691523 rmse:0.0578855499625206 mae:0.03844815492630005 r2:0.9725539088249207\n",
      "mse:0.003986107185482979 rmse:0.06313562393188477 mae:0.04143049940466881 r2:0.9673526287078857\n",
      "epoch:135\n",
      "train mse:0.005458502098917961 rmse:0.07388167828321457 mae:0.05071614310145378 r2:0.9552851319313049\n",
      "mse:0.006196791771799326 rmse:0.07871970534324646 mae:0.0525687076151371 r2:0.9492464661598206\n",
      "epoch:136\n",
      "train mse:0.003516086144372821 rmse:0.05929659307003021 mae:0.037935394793748856 r2:0.9711741805076599\n",
      "mse:0.0044279685243964195 rmse:0.06654298305511475 mae:0.04528879374265671 r2:0.9637336730957031\n",
      "epoch:137\n",
      "train mse:0.00308766751550138 rmse:0.055566783994436264 mae:0.036419548094272614 r2:0.9747263789176941\n",
      "mse:0.0034862025640904903 rmse:0.05904407054185867 mae:0.03871827572584152 r2:0.9714469909667969\n",
      "epoch:138\n",
      "train mse:0.005839972756803036 rmse:0.07641971111297607 mae:0.052583400160074234 r2:0.9521711468696594\n",
      "mse:0.005654582753777504 rmse:0.07519695907831192 mae:0.049986205995082855 r2:0.953687310218811\n",
      "epoch:139\n",
      "train mse:0.003886967897415161 rmse:0.06234555318951607 mae:0.04191577062010765 r2:0.9681919813156128\n",
      "mse:0.003690370125696063 rmse:0.06074841693043709 mae:0.03932960331439972 r2:0.9697747826576233\n",
      "epoch:140\n",
      "train mse:0.003156066872179508 rmse:0.05617888271808624 mae:0.037175048142671585 r2:0.974159836769104\n",
      "mse:0.0032074684277176857 rmse:0.05663451552391052 mae:0.03603464737534523 r2:0.9737299084663391\n",
      "epoch:141\n",
      "train mse:0.004753900226205587 rmse:0.06894852966070175 mae:0.04601088911294937 r2:0.9610432386398315\n",
      "mse:0.004093596711754799 rmse:0.06398122012615204 mae:0.04287460446357727 r2:0.9664722681045532\n",
      "epoch:142\n",
      "train mse:0.0032969831954687834 rmse:0.05741936340928078 mae:0.03697095811367035 r2:0.9730193614959717\n",
      "mse:0.003471305361017585 rmse:0.058917783200740814 mae:0.03844282031059265 r2:0.9715690016746521\n",
      "epoch:143\n",
      "train mse:0.004354265984147787 rmse:0.06598686426877975 mae:0.044303447008132935 r2:0.9643605947494507\n",
      "mse:0.004422014579176903 rmse:0.06649822741746902 mae:0.04437316209077835 r2:0.9637824296951294\n",
      "epoch:144\n",
      "train mse:0.006298021879047155 rmse:0.07936007529497147 mae:0.054061368107795715 r2:0.9484456181526184\n",
      "mse:0.004908046685159206 rmse:0.0700574517250061 mae:0.04661877453327179 r2:0.9598016738891602\n",
      "epoch:145\n",
      "train mse:0.004909819457679987 rmse:0.07007010281085968 mae:0.04773014783859253 r2:0.9597920775413513\n",
      "mse:0.007398572750389576 rmse:0.08601495623588562 mae:0.05803864076733589 r2:0.9394035339355469\n",
      "epoch:146\n",
      "train mse:0.0031711794435977936 rmse:0.056313224136829376 mae:0.03699547424912453 r2:0.9740018844604492\n",
      "mse:0.0033789267763495445 rmse:0.058128535747528076 mae:0.03743954375386238 r2:0.9723256230354309\n",
      "epoch:147\n",
      "train mse:0.00682142423465848 rmse:0.08259191364049911 mae:0.056014884263277054 r2:0.9441629648208618\n",
      "mse:0.005267589818686247 rmse:0.07257816195487976 mae:0.04981149360537529 r2:0.956856906414032\n",
      "epoch:148\n",
      "train mse:0.0030109737999737263 rmse:0.05487234145402908 mae:0.035329561680555344 r2:0.9753106832504272\n",
      "mse:0.003186579095199704 rmse:0.05644979327917099 mae:0.03641701117157936 r2:0.9739009737968445\n",
      "epoch:149\n",
      "train mse:0.003784717759117484 rmse:0.061520058661699295 mae:0.04113142937421799 r2:0.969013512134552\n",
      "mse:0.004389597102999687 rmse:0.06625403463840485 mae:0.044963542371988297 r2:0.964047908782959\n"
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    for X, y in dataset_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            tr_mse = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        tr_rmse = tf.sqrt(tr_mse)\n",
    "        tr_mae = tf.reduce_mean(tf.abs(y_pred - y))\n",
    "        tr_r2 = 1 - tf.reduce_sum(tf.square(y_pred - y)) / tf.reduce_sum(tf.square(y - tf.reduce_mean(y)))\n",
    "        grads = tape.gradient(tr_mse, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    print(\"epoch:{}\".format(i))\n",
    "    print(\"train mse:{} rmse:{} mae:{} r2:{}\".format(tr_mse, tr_rmse, tr_mae, tr_r2))\n",
    "    valiAll(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
