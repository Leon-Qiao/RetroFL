{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f008c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3776b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(device_type = 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87ed31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        AS_dataset = pd.read_csv('./Arbitrary_Single_band_Coupler_Phase_Shift.csv', encoding='utf-8')\n",
    "        self.X = AS_dataset.loc[:,'freq':'L4'].to_numpy()\n",
    "        self.y = AS_dataset.loc[:,'S11r':'S41i'].to_numpy()\n",
    "#         self.mmX = MinMaxScaler()\n",
    "#         self.X[:,1:] = self.mmX.fit_transform(self.X[:,1:])\n",
    "#         self.X[:,0] = self.X[:,0] / 10\n",
    "#         self.X, _, self.y, _ = train_test_split(self.X, self.y, test_size=0.75, random_state=0)\n",
    "        self.X_train, self.X_vali, self.y_train, self.y_vali = train_test_split(self.X, self.y, test_size=0.1, random_state=0)\n",
    "        self.num_train = self.X_train.shape[0]\n",
    "    def get_batch(self, batch_size=0, mode='train'):\n",
    "        if mode == 'train':\n",
    "            index = np.random.randint(0, self.num_train, batch_size)\n",
    "            return self.X_train[index], self.y_train[index]\n",
    "        if mode == 'validate':\n",
    "            return self.X_vali, self.y_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d306be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=512, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1024, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=512, activation=tf.nn.leaky_relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=256, activation=tf.nn.leaky_relu)\n",
    "        self.dense5 = tf.keras.layers.Dense(units=8)\n",
    "    \n",
    "#     @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        output = self.dense5(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2213fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 1024\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282c7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = DataLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "X_v, y_v = data_loader.get_batch(mode='validate')\n",
    "def train():\n",
    "    num_batch = data_loader.num_train // batch_size\n",
    "    for epoch_index in range(num_epochs):\n",
    "        for batch in range(num_batch):\n",
    "            X, y = data_loader.get_batch(batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(X)\n",
    "                tr_mse = tf.reduce_mean(tf.square(y_pred - y))\n",
    "            grads = tape.gradient(tr_mse, model.variables)\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        if epoch_index % 10 == 0 or epoch_index == num_epochs - 1:\n",
    "            tr_rmse = tf.sqrt(tr_mse)\n",
    "            tr_mae = tf.reduce_mean(tf.abs(y_pred - y))\n",
    "            tr_r2 = 1 - tf.reduce_sum(tf.square(y_pred - y)) / tf.reduce_sum(tf.square(y - tf.cast(tf.reduce_mean(y), dtype=tf.float32)))\n",
    "            print(\"epoch:{}\".format(epoch_index))\n",
    "            print(\"train mse:{} rmse:{} mae:{} r2:{}\".format(tr_mse, tr_rmse, tr_mae, tr_r2))\n",
    "            y_v_p = model(X_v)\n",
    "            va_mse = tf.reduce_mean(tf.square(y_v_p - y_v))\n",
    "            va_rmse = tf.sqrt(va_mse)\n",
    "            va_mae = tf.reduce_mean(tf.abs(y_v_p - y_v))\n",
    "            va_r2 = 1 - tf.reduce_sum(tf.square(y_v_p - y_v)) / tf.reduce_sum(tf.square(y_v - tf.cast(tf.reduce_mean(y_v), dtype=tf.float32)))\n",
    "            print(\"vali mse:{} rmse:{} mae:{} r2:{}\".format(va_mse, va_rmse, va_mae, va_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0897444",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "train mse:0.0952557697892189 rmse:0.30863532423973083 mae:0.25238871574401855 r2:0.21212852001190186\n",
      "vali mse:0.09371825307607651 rmse:0.3061343729496002 mae:0.24976204335689545 r2:0.22309726476669312\n",
      "epoch:10\n",
      "train mse:0.07130761444568634 rmse:0.2670348584651947 mae:0.21519824862480164 r2:0.40854591131210327\n",
      "vali mse:0.07203582674264908 rmse:0.26839491724967957 mae:0.21656323969364166 r2:0.4028396010398865\n",
      "epoch:20\n",
      "train mse:0.06365450471639633 rmse:0.2522984445095062 mae:0.20191535353660583 r2:0.47458916902542114\n",
      "vali mse:0.06593257933855057 rmse:0.2567733824253082 mae:0.2049683779478073 r2:0.4534341096878052\n",
      "epoch:30\n",
      "train mse:0.061959683895111084 rmse:0.24891701340675354 mae:0.19569209218025208 r2:0.4868156909942627\n",
      "vali mse:0.05900618061423302 rmse:0.24291187524795532 mae:0.19354911148548126 r2:0.5108523368835449\n",
      "epoch:40\n",
      "train mse:0.05336909368634224 rmse:0.23101751506328583 mae:0.1824861466884613 r2:0.5586906671524048\n",
      "vali mse:0.05311031639575958 rmse:0.23045675456523895 mae:0.18116815388202667 r2:0.5597277283668518\n",
      "epoch:50\n",
      "train mse:0.04586613178253174 rmse:0.21416379511356354 mae:0.16814455389976501 r2:0.6204397082328796\n",
      "vali mse:0.046283621340990067 rmse:0.21513628959655762 mae:0.16979657113552094 r2:0.6163194179534912\n",
      "epoch:60\n",
      "train mse:0.041858404874801636 rmse:0.20459327101707458 mae:0.16010242700576782 r2:0.653602659702301\n",
      "vali mse:0.04241219162940979 rmse:0.20594221353530884 mae:0.16273878514766693 r2:0.6484127044677734\n",
      "epoch:70\n",
      "train mse:0.041226260364055634 rmse:0.20304250717163086 mae:0.157755047082901 r2:0.6574360132217407\n",
      "vali mse:0.04030068591237068 rmse:0.20075030624866486 mae:0.15804699063301086 r2:0.6659165620803833\n",
      "epoch:80\n",
      "train mse:0.03974876552820206 rmse:0.19937092065811157 mae:0.15642209351062775 r2:0.6701278686523438\n",
      "vali mse:0.0384436696767807 rmse:0.1960705667734146 mae:0.1523059755563736 r2:0.6813108325004578\n",
      "epoch:90\n",
      "train mse:0.034879766404628754 rmse:0.1867612600326538 mae:0.14436118304729462 r2:0.711640477180481\n",
      "vali mse:0.03781593218445778 rmse:0.19446319341659546 mae:0.15237879753112793 r2:0.6865146160125732\n",
      "epoch:100\n",
      "train mse:0.03455635532736778 rmse:0.18589340150356293 mae:0.14423778653144836 r2:0.7146183252334595\n",
      "vali mse:0.03475450724363327 rmse:0.1864255964756012 mae:0.1453680396080017 r2:0.7118931412696838\n",
      "epoch:110\n",
      "train mse:0.035857006907463074 rmse:0.18935945630073547 mae:0.14598578214645386 r2:0.7034059762954712\n",
      "vali mse:0.03563176840543747 rmse:0.1887637972831726 mae:0.14720530807971954 r2:0.7046208381652832\n",
      "epoch:120\n",
      "train mse:0.03450543060898781 rmse:0.1857563704252243 mae:0.14209115505218506 r2:0.7148398756980896\n",
      "vali mse:0.0337701253592968 rmse:0.18376649916172028 mae:0.1422642022371292 r2:0.7200534343719482\n",
      "epoch:130\n",
      "train mse:0.029244475066661835 rmse:0.17101015150547028 mae:0.13162733614444733 r2:0.7583921551704407\n",
      "vali mse:0.029899567365646362 rmse:0.17291490733623505 mae:0.1339017152786255 r2:0.7521394491195679\n",
      "epoch:140\n",
      "train mse:0.028892245143651962 rmse:0.16997718811035156 mae:0.130789116024971 r2:0.7611491680145264\n",
      "vali mse:0.028690548613667488 rmse:0.16938284039497375 mae:0.12984143197536469 r2:0.7621619701385498\n",
      "epoch:150\n",
      "train mse:0.024707838892936707 rmse:0.15718726813793182 mae:0.1209574043750763 r2:0.7955916523933411\n",
      "vali mse:0.027335496619343758 rmse:0.16533450782299042 mae:0.12537334859371185 r2:0.7733950614929199\n",
      "epoch:160\n",
      "train mse:0.025729432702064514 rmse:0.16040396690368652 mae:0.12134552001953125 r2:0.7864571809768677\n",
      "vali mse:0.02586814947426319 rmse:0.16083578765392303 mae:0.12164393812417984 r2:0.7855589985847473\n",
      "epoch:170\n",
      "train mse:0.019430186599493027 rmse:0.1393921971321106 mae:0.10476270318031311 r2:0.8385096788406372\n",
      "vali mse:0.022265244275331497 rmse:0.149215430021286 mae:0.11328653991222382 r2:0.815426230430603\n",
      "epoch:180\n",
      "train mse:0.019362371414899826 rmse:0.1391487419605255 mae:0.10479827225208282 r2:0.8394378423690796\n",
      "vali mse:0.022043291479349136 rmse:0.14846983551979065 mae:0.11089559644460678 r2:0.8172662258148193\n",
      "epoch:190\n",
      "train mse:0.022514168173074722 rmse:0.1500472128391266 mae:0.11229294538497925 r2:0.8134972453117371\n",
      "vali mse:0.02281823195517063 rmse:0.15105704963207245 mae:0.1138250008225441 r2:0.8108420968055725\n",
      "epoch:199\n",
      "train mse:0.01407872885465622 rmse:0.11865381896495819 mae:0.08949249982833862 r2:0.883340060710907\n",
      "vali mse:0.01652388833463192 rmse:0.12854528427124023 mae:0.09633903205394745 r2:0.8630207777023315\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0279a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, './models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b733364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f4c0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func(s_para):\n",
    "    E = tf.square(s_para)\n",
    "    E11 = E[:,0] + E[:,1]\n",
    "    E21 = E[:,2] + E[:,3]\n",
    "    E31 = E[:,4] + E[:,5]\n",
    "    E41 = E[:,6] + E[:,7]\n",
    "    l1 = E11 - E21 - E31 + E41\n",
    "    l2 = tf.square(E21 / (E31 + E21) - 2 / 3)\n",
    "    l3 = tf.square(tf.math.atan2(s_para[:,5], s_para[:,4]) - tf.math.atan2(s_para[:,3], s_para[:,2]) - np.pi / 4)\n",
    "    l4 = tf.square(tf.reduce_sum(E, axis=1) - 1)\n",
    "    loss = l1 + l2 + l3 + l4\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66b3fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 5000\n",
    "num_node_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "781053c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "\n",
    "mmin = np.min(data_loader.X[: , 1: ], axis=0)\n",
    "mmax = np.max(data_loader.X[: , 1: ], axis=0)\n",
    "\n",
    "# structure.append(tf.Variable(np.random.uniform(0, 1, (num_nodes, 10)), dtype=tf.float32))\n",
    "structure = tf.Variable(np.random.uniform(mmin, mmax, (num_nodes, 12)), dtype=tf.float32)\n",
    "\n",
    "freq1 = tf.ones([num_nodes, 1]) * 2.4\n",
    "freq2 = tf.ones([num_nodes, 1]) * 2.5\n",
    "freq3 = tf.ones([num_nodes, 1]) * 2.6\n",
    "\n",
    "minLoss = 0\n",
    "minIndex = 0\n",
    "minS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72925434",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLoss = 10\n",
    "bestStructure = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbc721db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(structure):\n",
    "    inva_place1 = tf.where(tf.logical_or(structure[:,:8] < 1, structure[:,:8] > 4))\n",
    "    structure = tf.tensor_scatter_nd_update(structure, [inva_place1], [np.random.uniform(mmin[inva_place1[:,1]], mmax[inva_place1[:,1]], (inva_place1.shape[0]))])\n",
    "    \n",
    "    inva_place2 = tf.where(tf.logical_or(structure[:,8:] < 4, structure[:,8:] > 100)) + [0, 8]\n",
    "    structure = tf.tensor_scatter_nd_update(structure, [inva_place2], [np.random.uniform(mmin[inva_place2[:,1]], mmax[inva_place2[:,1]], (inva_place2.shape[0]))])\n",
    "    \n",
    "    return tf.Variable(structure)\n",
    "    \n",
    "    # structure[j] = tf.Variable(tf.tensor_scatter_nd_update(structure[j], [nega_place], [np.random.uniform(0, 1, (nega_place.shape[0]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aeaa40d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2230\n",
      "0 -1.9859238\n",
      "[ 4.5723305  2.991286   2.854786   4.19759    4.4924245  1.4546661\n",
      "  1.377169   2.9322083 98.22275   36.799522  10.596915  78.14789  ]\n",
      "\n",
      "2230\n",
      "1 -2.0379958\n",
      "[ 2.0177522  2.981289   2.8447857  2.9113207  3.4707549  1.444666\n",
      "  1.3671688  2.9222083 98.212746  36.80952   10.586916  78.137886 ]\n",
      "\n",
      "2230\n",
      "2 -2.0502336\n",
      "[ 2.0251935  2.98873    2.8373444  2.9038794  3.478196   1.4372249\n",
      "  1.3597274  2.9296496 98.220184  36.816963  10.594357  78.13045  ]\n",
      "\n",
      "2230\n",
      "3 -2.0606916\n",
      "[ 2.0315814  2.9951177  2.8309562  2.8974915  3.4845839  1.4308369\n",
      "  1.3533393  2.9360375 98.22657   36.823353  10.600744  78.12406  ]\n",
      "\n",
      "2230\n",
      "4 -2.070194\n",
      "[ 2.0373926  3.0009286  2.825145   2.8916802  3.4903948  1.4250258\n",
      "  1.347528   2.9418485 98.23238   36.829163  10.606555  78.11825  ]\n",
      "\n",
      "2230\n",
      "5 -2.0792317\n",
      "[ 2.0428474  3.0063832  2.8196902  2.8862255  3.4958496  1.419571\n",
      "  1.3420731  2.9473033 98.23784   36.834618  10.612009  78.11279  ]\n",
      "\n",
      "2230\n",
      "6 -2.0878568\n",
      "[ 2.0480685  3.011604   2.814469   2.8810043  3.5010705  1.41435\n",
      "  1.336852   2.9525242 98.24306   36.83984   10.617229  78.107574 ]\n",
      "\n",
      "2230\n",
      "7 -2.0962143\n",
      "[ 2.053132   3.0166676  2.8094053  2.8759406  3.506134   1.4092864\n",
      "  1.3317882  2.9575877 98.24812   36.844902  10.6222925 78.10251  ]\n",
      "\n",
      "2230\n",
      "8 -2.1039166\n",
      "[ 2.0580895  3.0216248  2.804448   2.8709831  3.5110915  1.404329\n",
      "  1.3268306  2.9625452 98.25308   36.84986   10.62725   78.09755  ]\n",
      "\n",
      "2230\n",
      "9 -2.1113236\n",
      "[ 2.0629768  3.0265121  2.7995603  2.8660958  3.5159788  1.3994416\n",
      "  1.3219432  2.9674325 98.25797   36.854748  10.632137  78.09266  ]\n",
      "\n",
      "2230\n",
      "10 -2.118487\n",
      "[ 2.067821   3.031356   2.7947161  2.8612516  3.520823   1.3945975\n",
      "  1.3170989  2.9722767 98.26282   36.859592  10.636981  78.087814 ]\n",
      "\n",
      "2230\n",
      "11 -2.1256268\n",
      "[ 2.0726423  3.0361772  2.7898948  2.8564303  3.525644   1.3897765\n",
      "  1.3122776  2.9770977 98.26764   36.864414  10.641802  78.08299  ]\n",
      "\n",
      "2230\n",
      "12 -2.1327434\n",
      "[ 2.0774565  3.0409913  2.7850807  2.8516161  3.5304582  1.3849624\n",
      "  1.3074633  2.981912  98.27245   36.86923   10.646616  78.07818  ]\n",
      "\n",
      "2230\n",
      "13 -2.139864\n",
      "[ 2.0822763  3.045811   2.7802608  2.8467963  3.5352778  1.3801427\n",
      "  1.3026434  2.9867315 98.277275  36.874046  10.651436  78.07336  ]\n",
      "\n",
      "2230\n",
      "14 -2.147118\n",
      "[ 2.087112   3.0506465  2.775425   2.8419607  3.5401134  1.3753071\n",
      "  1.2978077  2.9915671 98.28211   36.878883  10.656271  78.06852  ]\n",
      "\n",
      "2230\n",
      "15 -2.1545687\n",
      "[ 2.0919719  3.0555065  2.7705648  2.8371007  3.5449734  1.3704472\n",
      "  1.2929476  2.996427  98.28697   36.883743  10.661131  78.06366  ]\n",
      "\n",
      "2230\n",
      "16 -2.1620574\n",
      "[ 2.0968633  3.0603976  2.7656734  2.8322093  3.5498645  1.3655559\n",
      "  1.2880563  3.0013182 98.29186   36.888634  10.666022  78.05877  ]\n",
      "\n",
      "2230\n",
      "17 -2.16957\n",
      "[ 2.1017919  3.065326   2.7607448  2.8272808  3.554793   1.3606274\n",
      "  1.2831277  3.0062466 98.29679   36.893562  10.670951  78.05384  ]\n",
      "\n",
      "2230\n",
      "18 -2.1771204\n",
      "[ 2.1067624  3.0702965  2.755774   2.8223102  3.5597634  1.3556569\n",
      "  1.278157   3.011217  98.301765  36.898533  10.675921  78.048874 ]\n",
      "\n",
      "2230\n",
      "19 -2.1844172\n",
      "[ 2.1117795  3.0753133  2.750757   2.8172932  3.5647802  1.3506399\n",
      "  1.27314    3.016234  98.306786  36.90355   10.680938  78.04386  ]\n",
      "\n",
      "2230\n",
      "20 -2.1916738\n",
      "[ 2.1168463  3.08038    2.74569    2.8122263  3.569847   1.3455732\n",
      "  1.268073   3.0213008 98.31185   36.908615  10.686005  78.038795 ]\n",
      "\n",
      "2230\n",
      "21 -2.19895\n",
      "[ 2.1219661  3.0854995  2.7405703  2.8071065  3.574967   1.3404534\n",
      "  1.262953   3.0264206 98.31697   36.913734  10.691124  78.033676 ]\n",
      "\n",
      "2230\n",
      "22 -2.2060204\n",
      "[ 2.1271415  3.0906744  2.7353947  2.8019311  3.5801423  1.335278\n",
      "  1.2577775  3.031596  98.32214   36.91891   10.6963    78.0285   ]\n",
      "\n",
      "2230\n",
      "23 -2.2130282\n",
      "[ 2.1323748  3.095907   2.7301614  2.7966979  3.5853753  1.3300449\n",
      "  1.252544   3.0368292 98.32738   36.924145  10.701532  78.02327  ]\n",
      "\n",
      "2230\n",
      "24 -2.2201123\n",
      "[ 2.1376677  3.1011992  2.7248683  2.7914047  3.5906682  1.324752\n",
      "  1.2472509  3.0421221 98.33267   36.92944   10.706825  78.017975 ]\n",
      "\n",
      "2230\n",
      "25 -2.2272692\n",
      "[ 2.143022   3.1065528  2.719514   2.7860503  3.5960226  1.3193977\n",
      "  1.2418964  3.0474765 98.33803   36.934795  10.712179  78.01262  ]\n",
      "\n",
      "2230\n",
      "26 -2.2344089\n",
      "[ 2.1484392  3.1119692  2.7140965  2.7806332  3.6014397  1.3139806\n",
      "  1.236479   3.0528936 98.343445  36.940212  10.717596  78.0072   ]\n",
      "\n",
      "2230\n",
      "27 -2.2415884\n",
      "[ 2.1539202  3.1174488  2.7086153  2.775152   3.6069207  1.3084996\n",
      "  1.2309978  3.0583746 98.34892   36.945694  10.723077  78.001724 ]\n",
      "\n",
      "2230\n",
      "28 -2.2488518\n",
      "[ 2.1594663  3.1229935  2.7030692  2.7696059  3.6124666  1.3029536\n",
      "  1.2254516  3.0639205 98.35447   36.95124   10.728622  77.99618  ]\n",
      "\n",
      "2230\n",
      "29 -2.256124\n",
      "[ 2.1650782  3.1286042  2.6974573  2.763994   3.6180782  1.2973418\n",
      "  1.2198396  3.0695322 98.360085  36.956852  10.734234  77.99057  ]\n",
      "\n",
      "2230\n",
      "30 -2.263301\n",
      "[ 2.1707566  3.1342814  2.691779   2.7583156  3.6237564  1.2916636\n",
      "  1.2141612  3.0752103 98.36576   36.962532  10.739912  77.98489  ]\n",
      "\n",
      "2230\n",
      "31 -2.2704513\n",
      "[ 2.176502   3.1400254  2.6860335  2.7525702  3.6295013  1.2859186\n",
      "  1.2084157  3.0809555 98.371506  36.968277  10.745657  77.97915  ]\n",
      "\n",
      "2230\n",
      "32 -2.2775884\n",
      "[ 2.1823146  3.1458366  2.6802206  2.7467575  3.6353137  1.2801061\n",
      "  1.2026029  3.0867682 98.37732   36.97409   10.75147   77.973335 ]\n",
      "\n",
      "2230\n",
      "33 -2.2848043\n",
      "[ 2.1881952  3.1517155  2.67434    2.740877   3.6411939  1.2742258\n",
      "  1.1967223  3.0926485 98.3832    36.97997   10.75735   77.96745  ]\n",
      "\n",
      "2230\n",
      "34 -2.2921407\n",
      "[ 2.1941438  3.1576624  2.6683915  2.7349284  3.647142   1.2682776\n",
      "  1.1907737  3.0985968 98.38915   36.985916  10.763298  77.96151  ]\n",
      "\n",
      "2230\n",
      "35 -2.2995982\n",
      "[ 2.2001603  3.1636777  2.6623747  2.7289119  3.6531582  1.2622613\n",
      "  1.184757   3.1046133 98.39517   36.991932  10.769314  77.9555   ]\n",
      "\n",
      "2230\n",
      "36 -2.3071303\n",
      "[ 2.2062452  3.1697612  2.6562898  2.7228272  3.6592426  1.2561767\n",
      "  1.1786721  3.110698  98.40126   36.998016  10.775398  77.94942  ]\n",
      "\n",
      "2230\n",
      "37 -2.3144653\n",
      "[ 2.2123983  3.1759129  2.6501367  2.716674   3.6653953  1.250024\n",
      "  1.172519   3.1168509 98.40741   37.00417   10.78155   77.94327  ]\n",
      "\n",
      "2230\n",
      "38 -2.3216186\n",
      "[ 2.2186196  3.1821332  2.6439154  2.710453   3.6716163  1.243803\n",
      "  1.1662977  3.123072  98.41363   37.01039   10.787771  77.949486 ]\n",
      "\n",
      "2230\n",
      "39 -2.3286955\n",
      "[ 2.2249088  3.1758442  2.6376262  2.7041638  3.6779053  1.237514\n",
      "  1.1600083  3.1293612 98.419914  37.01668   10.79406   77.95577  ]\n",
      "\n",
      "2230\n",
      "40 -2.3358397\n",
      "[ 2.231266   3.1694872  2.631269   2.6978066  3.6842623  1.2311571\n",
      "  1.153651   3.1357183 98.42627   37.023037  10.800417  77.96213  ]\n",
      "\n",
      "2230\n",
      "41 -2.3423605\n",
      "[ 2.237691   3.1759114  2.624844   2.6913817  3.690687   1.2247324\n",
      "  1.147226   3.1421432 98.43269   37.02946   10.806842  77.96855  ]\n",
      "\n",
      "2230\n",
      "42 -2.3496344\n",
      "[ 2.2441833  3.1694193  2.6183517  2.6848893  3.6971793  1.2182403\n",
      "  1.1407335  3.1486356 98.439186  37.035954  10.8133335 77.975044 ]\n",
      "\n",
      "2230\n",
      "43 -2.357119\n",
      "[ 2.250743   3.16286    2.6117918  2.6783297  3.703739   1.2116808\n",
      "  1.1341736  3.1551952 98.44575   37.042515  10.819893  77.981606 ]\n",
      "\n",
      "2230\n",
      "44 -2.3646717\n",
      "[ 2.2573695  3.1562335  2.6051652  2.671703   3.7103655  1.2050544\n",
      "  1.1275469  3.1618218 98.45238   37.04914   10.826519  77.98823  ]\n",
      "\n",
      "2230\n",
      "45 -2.3722847\n",
      "[ 2.2640626  3.1495404  2.5984719  2.6650097  3.7170587  1.1983613\n",
      "  1.1208534  3.1685152 98.45907   37.055836  10.833212  77.99492  ]\n",
      "\n",
      "2230\n",
      "46 -2.379982\n",
      "[ 2.2708223  3.142781   2.5917122  2.65825    3.7238183  1.1916019\n",
      "  1.1140937  3.1752748 98.46583   37.062595  10.839972  78.00168  ]\n",
      "\n",
      "2230\n",
      "47 -2.3877344\n",
      "[ 2.2776477  3.1359556  2.5848866  2.6514244  3.7306437  1.1847764\n",
      "  1.107268   3.1821003 98.47266   37.06942   10.846797  78.0085   ]\n",
      "\n",
      "2230\n",
      "48 -2.3955364\n",
      "[ 2.284539   3.1290646  2.5779953  2.6445332  3.737535   1.1778854\n",
      "  1.1003766  3.1889915 98.479546  37.07631   10.853688  78.01539  ]\n",
      "\n",
      "2230\n",
      "49 -2.4034257\n",
      "[ 2.2914953  3.1221085  2.5710387  2.6375768  3.7444913  1.1709292\n",
      "  1.09342    3.195948  98.4865    37.083267  10.860644  78.02235  ]\n",
      "\n",
      "2230\n",
      "50 -2.4114041\n",
      "[ 2.2985165  3.1150875  2.5640173  2.6305556  3.7515125  1.1639081\n",
      "  1.0863986  3.202969  98.49352   37.090286  10.867665  78.029366 ]\n",
      "\n",
      "2230\n",
      "51 -2.419434\n",
      "[ 2.305602   3.108002   2.5569315  2.6234698  3.758598   1.1568227\n",
      "  1.0793127  3.2100549 98.50061   37.09737   10.874751  78.03645  ]\n",
      "\n",
      "2230\n",
      "52 -2.4267838\n",
      "[ 2.3127518  3.115151   2.5497816  2.6163201  3.7657478  1.1496732\n",
      "  1.0721627  3.2172046 98.50776   37.10452   10.881901  78.0436   ]\n",
      "\n",
      "2230\n",
      "53 -2.4348094\n",
      "[ 2.3199651  3.1079378  2.5425682  2.6091068  3.7729611  1.1424601\n",
      "  1.0649493  3.224418  98.51497   37.111732  10.889113  78.05081  ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2230\n",
      "54 -2.4427204\n",
      "[ 2.3272417  3.1006615  2.5352917  2.6018302  3.7802377  1.1351839\n",
      "  1.0576726  3.2316945 98.52225   37.119007  10.89639   78.05809  ]\n",
      "\n",
      "2230\n",
      "55 -2.4506793\n",
      "[ 2.334581   3.0933225  2.5279522  2.594491   3.787577   1.1278449\n",
      "  1.0503331  3.2390337 98.52959   37.126347  10.9037285 78.06543  ]\n",
      "\n",
      "2230\n",
      "56 -2.4586778\n",
      "[ 2.3419824  3.0859213  2.5205505  2.5870895  3.7949784  1.1204436\n",
      "  1.0429314  3.2464352 98.53699   37.133747  10.91113   78.07283  ]\n",
      "\n",
      "2230\n",
      "57 -2.4667268\n",
      "[ 2.3494458  3.078458   2.5130868  2.579626   3.8024418  1.1129804\n",
      "  1.0354677  3.2538986 98.54445   37.14121   10.918593  78.08029  ]\n",
      "\n",
      "2230\n",
      "58 -2.4748008\n",
      "[ 2.3569705  3.0709333  2.5055618  2.572101   3.8099668  1.1054558\n",
      "  1.0279427  3.2614236 98.55197   37.148735  10.926118  78.087814 ]\n",
      "\n",
      "2230\n",
      "59 -2.482864\n",
      "[ 2.3645563  3.0633476  2.4979758  2.564515   3.8175528  1.0978701\n",
      "  1.0203565  3.2690096 98.559555  37.156322  10.933703  78.0954   ]\n",
      "\n",
      "2230\n",
      "60 -2.490395\n",
      "[ 2.3722029  3.0709937  2.4903293  2.5568686  3.8251994  1.0902239\n",
      "  1.0127097  3.2766562 98.5672    37.163967  10.94135   78.10304  ]\n",
      "\n",
      "2230\n",
      "61 -2.4985697\n",
      "[ 2.3799095  3.0632873  2.4826224  2.549162   3.832906   1.0825175\n",
      "  1.0050029  3.2843628 98.574905  37.171673  10.949057  78.11075  ]\n",
      "\n",
      "2230\n",
      "63 -2.4998639\n",
      "[ 2.3955014  3.0477035  2.46703    2.5335696  3.8484983  1.066926\n",
      "  1.0317936  3.299955  98.5905    37.187263  10.964647  78.126335 ]\n",
      "\n",
      "2230\n",
      "64 -2.5083742\n",
      "[ 2.4033859  3.039829   2.4591453  2.5256848  3.856383   1.0590417\n",
      "  1.0239087  3.3078394 98.59838   37.19515   10.972531  78.13422  ]\n",
      "\n",
      "2230\n",
      "65 -2.5169544\n",
      "[ 2.411329   3.031898   2.4512022  2.5177417  3.8643262  1.051099\n",
      "  1.0159655  3.3157823 98.60632   37.20309   10.9804735 78.14215  ]\n",
      "\n",
      "2230\n",
      "66 -2.525613\n",
      "[ 2.4193301  3.0239089  2.4432008  2.5097406  3.8723273  1.0430982\n",
      "  1.0079641  3.3237834 98.61433   37.21109   10.988474  78.15015  ]\n",
      "\n",
      "3262\n",
      "81 -2.5343347\n",
      "[ 2.7998502  2.3482118  1.3336357  2.869411   3.2206585  3.8894386\n",
      "  3.029276   1.7579385 16.27072    4.4617777 61.968906  59.630535 ]\n",
      "\n",
      "3262\n",
      "82 -2.5485222\n",
      "[ 2.8087265  2.3570883  1.3247591  2.8605347  3.2295346  3.8983147\n",
      "  3.0203993  1.7490923 16.279596   4.4529014 61.960033  59.639412 ]\n",
      "\n",
      "3262\n",
      "83 -2.5625074\n",
      "[ 2.8176544  2.3660164  1.3158311  2.8516068  3.2384622  3.9072425\n",
      "  3.0114713  1.7578913 16.288525   4.4439735 61.951107  59.64834  ]\n",
      "\n",
      "3262\n",
      "84 -2.576668\n",
      "[ 2.8266335  2.3749957  1.3068517  2.8426278  3.247441   3.9162216\n",
      "  3.002492   1.748943  16.297504   4.434994  61.94213   59.65732  ]\n",
      "\n",
      "3262\n",
      "85 -2.5905783\n",
      "[ 2.8356636  2.384026   1.2978214  2.8335977  3.2564707  3.9252515\n",
      "  2.9934616  1.739925  16.306534   4.4259644 61.9331    59.666348 ]\n",
      "\n",
      "3262\n",
      "86 -2.6043775\n",
      "[ 2.8447442  2.393107   1.2887405  2.824517   3.265551   3.9343321\n",
      "  2.9843807  1.7308565 16.315615   4.4168835 61.924023  59.675426 ]\n",
      "\n",
      "3262\n",
      "87 -2.6179488\n",
      "[ 2.8538752  2.4022381  1.2796093  2.815386   3.2746816  3.9434628\n",
      "  2.9752495  1.7217386 16.324745   4.4077525 61.914894  59.68456  ]\n",
      "\n",
      "3262\n",
      "88 -2.6313522\n",
      "[ 2.8630562  2.4114194  1.2704282  2.8062053  3.283862   3.9526436\n",
      "  2.9660683  1.7309078 16.333925   4.3985715 61.905716  59.69374  ]\n",
      "\n",
      "3262\n",
      "89 -2.6447523\n",
      "[ 2.8722868  2.4206502  1.2611973  2.7969747  3.2930923  3.961874\n",
      "  2.9568374  1.7401257 16.343155   4.389341  61.89649   59.702972 ]\n",
      "\n",
      "3262\n",
      "90 -2.6580875\n",
      "[ 2.8815668  2.4299304  1.2519171  2.7876947  3.3023717  3.9711537\n",
      "  2.9475572  1.7493945 16.352434   4.380061  61.88721   59.712254 ]\n",
      "\n",
      "3262\n",
      "91 -2.6714163\n",
      "[ 2.8908956  2.4392598  1.2425879  2.7783659  3.3117003  3.9804826\n",
      "  2.938228   1.7587092 16.361763   4.3707323 61.877884  59.72158  ]\n",
      "\n",
      "3262\n",
      "92 -2.6842532\n",
      "[ 2.9002733  2.4486377  1.23321    2.7689881  3.3210776  3.98986\n",
      "  2.92885    1.7680744 16.37114    4.3613544 61.868507  59.730957 ]\n",
      "\n",
      "3262\n",
      "93 -2.696104\n",
      "[ 2.9096992  2.458064   1.2237836  2.759562   3.3305023  3.9992855\n",
      "  2.9194236  1.7586509 16.380564   4.351928  61.85908   59.740383 ]\n",
      "\n",
      "3262\n",
      "105 -2.705166\n",
      "[ 3.0264916  2.5748582  1.1069896  2.6427734  3.4472926  1.7035043\n",
      "  2.80263    1.8564895 16.49735    4.235137  61.95691   59.857174 ]\n",
      "\n",
      "3262\n",
      "106 -2.7217937\n",
      "[ 3.0365226  2.5848894  1.0969584  2.632743   3.4573233  1.7135353\n",
      "  2.7925987  1.8665199 16.50738    4.2251062 61.966938  59.867203 ]\n",
      "\n",
      "3262\n",
      "107 -2.7379684\n",
      "[ 3.0465984  2.5949652  1.0868825  2.6226678  3.4673986  1.723611\n",
      "  2.782523   1.876595  16.517452   4.2150307 61.977013  59.877277 ]\n",
      "\n",
      "3262\n",
      "108 -2.7535868\n",
      "[ 3.0567186  2.6050856  1.0767621  2.6125484  3.4775183  1.7337312\n",
      "  2.7724025  1.8867145 16.507416   4.2049108 61.98713   59.887398 ]\n",
      "\n",
      "3262\n",
      "109 -2.7692218\n",
      "[ 3.066883   2.61525    1.0665975  2.6023846  3.487682   1.7438955\n",
      "  2.762238   1.8968781 16.497276   4.1947465 61.99729   59.89756  ]\n",
      "\n",
      "3262\n",
      "110 -2.7845833\n",
      "[ 3.0770915  2.6254587  1.0563889  2.592177   3.49789    1.7541039\n",
      "  2.7520294  1.9070859 16.487072   4.184538  62.0075    59.90777  ]\n",
      "\n",
      "3262\n",
      "111 -2.7995994\n",
      "[ 3.0873437  2.6357112  1.0461365  2.5819254  3.5081413  1.764356\n",
      "  2.7417772  1.9173373 16.476828   4.174286  62.01775   59.91802  ]\n",
      "\n",
      "3262\n",
      "112 -2.813957\n",
      "[ 3.0976393  2.646007   1.0358406  2.5716302  3.518436   1.7746516\n",
      "  2.7314813  1.9276323 16.466537   4.16399   62.02804   59.928314 ]\n",
      "\n",
      "3262\n",
      "113 -2.8280387\n",
      "[ 3.1079783  2.6563463  1.0255014  2.5612917  3.5287726  1.7849905\n",
      "  2.721142   1.9379706 16.456202   4.153651  62.038372  59.938652 ]\n",
      "\n",
      "3262\n",
      "114 -2.8417206\n",
      "[ 3.1183605  2.6667285  1.0151191  2.5509102  3.539152   1.7953726\n",
      "  2.7107596  1.948352  16.445822   4.143269  62.04874   59.94903  ]\n",
      "\n",
      "3262\n",
      "115 -2.8549683\n",
      "[ 3.1287854  2.6771536  1.0046939  2.5404859  3.5495665  1.8057975\n",
      "  2.7003345  1.9587761 16.435398   4.132844  62.038326  59.959457 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_node_epochs):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(structure)\n",
    "        y_pred1 = model(tf.concat([freq1, structure], axis=1))\n",
    "        y_pred2 = model(tf.concat([freq2, structure], axis=1))\n",
    "        y_pred3 = model(tf.concat([freq3, structure], axis=1))\n",
    "        loss = obj_func(y_pred1) + obj_func(y_pred2) + obj_func(y_pred3)\n",
    "    minLoss = tf.reduce_min(loss).numpy()\n",
    "    minIndex = tf.argmin(loss).numpy()\n",
    "    minS = structure[minIndex].numpy()\n",
    "    grads = tape.gradient(loss, structure)\n",
    "    opt.apply_gradients(grads_and_vars=zip([grads], [structure]))\n",
    "    structure = check(structure)\n",
    "    if minLoss < bestLoss:\n",
    "        bestLoss = minLoss\n",
    "        bestStructure = minS\n",
    "        # bestStructure = data_loader.mmX.inverse_transform([minS[0]])[0]\n",
    "        print(minIndex)\n",
    "        print(i, bestLoss)\n",
    "        print(bestStructure)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
